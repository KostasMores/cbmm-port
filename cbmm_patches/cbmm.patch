diff --git a/.gitignore b/.gitignore
index 72ef86a5570d..f3b9e5556e42 100644
--- a/.gitignore
+++ b/.gitignore
@@ -146,3 +146,6 @@ x509.genkey
 
 # Clang's compilation database file
 /compile_commands.json
+
+# read-pftrace binaries
+/mm/read-pftrace/target/*
diff --git a/arch/x86/entry/syscalls/syscall_64.tbl b/arch/x86/entry/syscalls/syscall_64.tbl
index c29976eca4a8..9fc1824fd953 100644
--- a/arch/x86/entry/syscalls/syscall_64.tbl
+++ b/arch/x86/entry/syscalls/syscall_64.tbl
@@ -357,6 +357,7 @@
 433	common	fspick			__x64_sys_fspick
 434	common	pidfd_open		__x64_sys_pidfd_open
 435	common	clone3			__x64_sys_clone3/ptregs
+436     64      init_badger_trap        __x64_sys_init_badger_trap
 
 #
 # x32-specific system call numbers start at 512 to avoid cache impact
diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
index ad97dc155195..5650f65b7a6c 100644
--- a/arch/x86/include/asm/pgtable.h
+++ b/arch/x86/include/asm/pgtable.h
@@ -212,6 +212,7 @@ static inline unsigned long pte_pfn(pte_t pte)
 {
 	phys_addr_t pfn = pte_val(pte);
 	pfn ^= protnone_mask(pfn);
+	pfn &= ~_PAGE_RESERVED; // markm: badger trap
 	return (pfn & PTE_PFN_MASK) >> PAGE_SHIFT;
 }
 
@@ -219,6 +220,7 @@ static inline unsigned long pmd_pfn(pmd_t pmd)
 {
 	phys_addr_t pfn = pmd_val(pmd);
 	pfn ^= protnone_mask(pfn);
+	pfn &= ~_PAGE_RESERVED; // markm: badger trap
 	return (pfn & pmd_pfn_mask(pmd)) >> PAGE_SHIFT;
 }
 
@@ -226,6 +228,7 @@ static inline unsigned long pud_pfn(pud_t pud)
 {
 	phys_addr_t pfn = pud_val(pud);
 	pfn ^= protnone_mask(pfn);
+	pfn &= ~_PAGE_RESERVED; // markm: badger trap
 	return (pfn & pud_pfn_mask(pud)) >> PAGE_SHIFT;
 }
 
@@ -791,7 +794,9 @@ static inline int pmd_none(pmd_t pmd)
 
 static inline unsigned long pmd_page_vaddr(pmd_t pmd)
 {
-	return (unsigned long)__va(pmd_val(pmd) & pmd_pfn_mask(pmd));
+	unsigned long raw_val = pmd_val(pmd) & pmd_pfn_mask(pmd);
+	raw_val &= ~_PAGE_RESERVED; // markm: badger trap
+	return (unsigned long)__va(raw_val);
 }
 
 /*
diff --git a/arch/x86/include/asm/pgtable_types.h b/arch/x86/include/asm/pgtable_types.h
index b5e49e6bac63..6d5185e9497f 100644
--- a/arch/x86/include/asm/pgtable_types.h
+++ b/arch/x86/include/asm/pgtable_types.h
@@ -23,6 +23,7 @@
 #define _PAGE_BIT_SOFTW2	10	/* " */
 #define _PAGE_BIT_SOFTW3	11	/* " */
 #define _PAGE_BIT_PAT_LARGE	12	/* On 2MB or 1GB pages */
+#define _PAGE_BIT_RESERVED	51 	/* Reserved bit should be 0, used by BadgerTrap */
 #define _PAGE_BIT_SOFTW4	58	/* available for programmer */
 #define _PAGE_BIT_PKEY_BIT0	59	/* Protection Keys, bit 1/4 */
 #define _PAGE_BIT_PKEY_BIT1	60	/* Protection Keys, bit 2/4 */
@@ -53,6 +54,7 @@
 #define _PAGE_SOFTW3	(_AT(pteval_t, 1) << _PAGE_BIT_SOFTW3)
 #define _PAGE_PAT	(_AT(pteval_t, 1) << _PAGE_BIT_PAT)
 #define _PAGE_PAT_LARGE (_AT(pteval_t, 1) << _PAGE_BIT_PAT_LARGE)
+#define _PAGE_RESERVED  (_AT(pteval_t, 1) << _PAGE_BIT_RESERVED)
 #define _PAGE_SPECIAL	(_AT(pteval_t, 1) << _PAGE_BIT_SPECIAL)
 #define _PAGE_CPA_TEST	(_AT(pteval_t, 1) << _PAGE_BIT_CPA_TEST)
 #ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS
diff --git a/arch/x86/kernel/cpu/aperfmperf.c b/arch/x86/kernel/cpu/aperfmperf.c
index e2f319dc992d..22c929bac71f 100644
--- a/arch/x86/kernel/cpu/aperfmperf.c
+++ b/arch/x86/kernel/cpu/aperfmperf.c
@@ -135,3 +135,4 @@ unsigned int arch_freq_get_on_cpu(int cpu)
 
 	return per_cpu(samples.khz, cpu);
 }
+EXPORT_SYMBOL(arch_freq_get_on_cpu);
diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 61e93a318983..325bec25e1f9 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -917,7 +917,7 @@ unsigned long arch_align_stack(unsigned long sp)
 
 unsigned long arch_randomize_brk(struct mm_struct *mm)
 {
-	return randomize_page(mm->brk, 0x02000000);
+	return randomize_huge_page(mm->brk, 0x01000000);
 }
 
 /*
diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
index 304d31d8cbbc..ce3266874580 100644
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@ -18,6 +18,9 @@
 #include <linux/uaccess.h>		/* faulthandler_disabled()	*/
 #include <linux/efi.h>			/* efi_recover_from_page_fault()*/
 #include <linux/mm_types.h>
+#include <linux/mm_stats.h>
+#include <linux/huge_mm.h>
+#include <linux/mm_econ.h>
 
 #include <asm/cpufeature.h>		/* boot_cpu_has, ...		*/
 #include <asm/traps.h>			/* dotraplinkage, ...		*/
@@ -398,7 +401,7 @@ NOKPROBE_SYMBOL(vmalloc_fault);
 
 #ifdef CONFIG_CPU_SUP_AMD
 static const char errata93_warning[] =
-KERN_ERR 
+KERN_ERR
 "******* Your BIOS seems to not contain a fix for K8 errata #93\n"
 "******* Working around it, but it may cause SEGVs or burn power.\n"
 "******* Please consider a BIOS update.\n"
@@ -627,6 +630,10 @@ show_fault_oops(struct pt_regs *regs, unsigned long error_code, unsigned long ad
 		 (error_code & X86_PF_PK)    ? "protection keys violation" :
 					       "permissions violation");
 
+	if ((error_code & X86_PF_WRITE) && (error_code & X86_PF_PROT)) {
+		pr_alert("value=%lx\n", *(unsigned long *)address);
+	}
+
 	if (!(error_code & X86_PF_USER) && user_mode(regs)) {
 		struct desc_ptr idt, gdt;
 		u16 ldtr, tr;
@@ -671,14 +678,19 @@ pgtable_bad(struct pt_regs *regs, unsigned long error_code,
 	tsk = current;
 	sig = SIGKILL;
 
-	printk(KERN_ALERT "%s: Corrupted page table at address %lx\n",
+	printk(KERN_ALERT "%s: Corrupted page table at fault address %lx\n",
 	       tsk->comm, address);
 	dump_pagetable(address);
 
+	if (error_code & X86_PF_RSVD)
+		printk(KERN_ALERT "%s: reserved bit set\n", tsk->comm);
+
+	/* markm: make it not fatal...
 	if (__die("Bad pagetable", regs, error_code))
 		sig = 0;
 
 	oops_end(flags, regs, sig);
+	*/
 }
 
 static void set_signal_archinfo(unsigned long address,
@@ -1195,8 +1207,10 @@ access_error(unsigned long error_code, struct vm_area_struct *vma)
 	}
 
 	/* read, present: */
-	if (unlikely(error_code & X86_PF_PROT))
-		return 1;
+	if (unlikely(error_code & X86_PF_PROT)) {
+		return !((error_code & X86_PF_RSVD) &&
+				current->mm->badger_trap_was_enabled);
+	}
 
 	/* read, not present: */
 	if (unlikely(!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE))))
@@ -1277,30 +1291,37 @@ do_kern_addr_fault(struct pt_regs *regs, unsigned long hw_error_code,
 }
 NOKPROBE_SYMBOL(do_kern_addr_fault);
 
-/* Handle faults in the user portion of the address space */
+/* Handle faults in the user portion of the address space
+ *
+ * (markm) returns true if the page fault resulted in a huge page allocation.
+ */
 static inline
-void do_user_addr_fault(struct pt_regs *regs,
+bool do_user_addr_fault(struct pt_regs *regs,
 			unsigned long hw_error_code,
-			unsigned long address)
+			unsigned long address,
+			struct mm_stats_pftrace *pftrace)
 {
 	struct vm_area_struct *vma;
 	struct task_struct *tsk;
 	struct mm_struct *mm;
 	vm_fault_t fault, major = 0;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
+	bool is_huge = false;
+	int ret;
 
 	tsk = current;
 	mm = tsk->mm;
 
 	/* kprobes don't want to hook the spurious faults: */
 	if (unlikely(kprobe_page_fault(regs, X86_TRAP_PF)))
-		return;
+		return false;
 
 	/*
 	 * Reserved bits are never expected to be set on
 	 * entries in the user portion of the page tables.
 	 */
-	if (unlikely(hw_error_code & X86_PF_RSVD))
+	if (unlikely(hw_error_code & X86_PF_RSVD) &&
+			!current->mm->badger_trap_was_enabled)
 		pgtable_bad(regs, hw_error_code, address);
 
 	/*
@@ -1315,7 +1336,7 @@ void do_user_addr_fault(struct pt_regs *regs,
 		     !(regs->flags & X86_EFLAGS_AC)))
 	{
 		bad_area_nosemaphore(regs, hw_error_code, address);
-		return;
+		return false;
 	}
 
 	/*
@@ -1324,7 +1345,7 @@ void do_user_addr_fault(struct pt_regs *regs,
 	 */
 	if (unlikely(faulthandler_disabled() || !mm)) {
 		bad_area_nosemaphore(regs, hw_error_code, address);
-		return;
+		return false;
 	}
 
 	/*
@@ -1346,8 +1367,11 @@ void do_user_addr_fault(struct pt_regs *regs,
 
 	if (hw_error_code & X86_PF_WRITE)
 		flags |= FAULT_FLAG_WRITE;
-	if (hw_error_code & X86_PF_INSTR)
+	if (hw_error_code & X86_PF_INSTR) {
 		flags |= FAULT_FLAG_INSTRUCTION;
+		mm_stats_set_flag(pftrace, MM_STATS_PF_EXEC);
+	}
+
 
 #ifdef CONFIG_X86_64
 	/*
@@ -1363,7 +1387,7 @@ void do_user_addr_fault(struct pt_regs *regs,
 	 */
 	if (is_vsyscall_vaddr(address)) {
 		if (emulate_vsyscall(hw_error_code, regs, address))
-			return;
+			return false;
 	}
 #endif
 
@@ -1386,7 +1410,7 @@ void do_user_addr_fault(struct pt_regs *regs,
 			 * which we do not expect faults.
 			 */
 			bad_area_nosemaphore(regs, hw_error_code, address);
-			return;
+			return false;
 		}
 retry:
 		down_read(&mm->mmap_sem);
@@ -1402,17 +1426,17 @@ void do_user_addr_fault(struct pt_regs *regs,
 	vma = find_vma(mm, address);
 	if (unlikely(!vma)) {
 		bad_area(regs, hw_error_code, address);
-		return;
+		return false;
 	}
 	if (likely(vma->vm_start <= address))
 		goto good_area;
 	if (unlikely(!(vma->vm_flags & VM_GROWSDOWN))) {
 		bad_area(regs, hw_error_code, address);
-		return;
+		return false;
 	}
 	if (unlikely(expand_stack(vma, address))) {
 		bad_area(regs, hw_error_code, address);
-		return;
+		return false;
 	}
 
 	/*
@@ -1422,7 +1446,7 @@ void do_user_addr_fault(struct pt_regs *regs,
 good_area:
 	if (unlikely(access_error(hw_error_code, vma))) {
 		bad_area_access_error(regs, hw_error_code, address, vma);
-		return;
+		return false;
 	}
 
 	/*
@@ -1438,9 +1462,15 @@ void do_user_addr_fault(struct pt_regs *regs,
 	 * userland). The return to userland is identified whenever
 	 * FAULT_FLAG_USER|FAULT_FLAG_KILLABLE are both set in flags.
 	 */
-	fault = handle_mm_fault(vma, address, flags);
+	fault = handle_mm_fault(vma, address, flags, pftrace);
 	major |= fault & VM_FAULT_MAJOR;
 
+	is_huge = !(fault & (VM_FAULT_OOM | VM_FAULT_BASE_PAGE));
+
+	if (is_huge) {
+		mm_register_promotion(address & HPAGE_PMD_MASK);
+	}
+
 	/*
 	 * If we need to retry the mmap_sem has already been released,
 	 * and if there is a fatal signal pending there is no guarantee
@@ -1457,17 +1487,25 @@ void do_user_addr_fault(struct pt_regs *regs,
 
 		/* User mode? Just return to handle the fatal exception */
 		if (flags & FAULT_FLAG_USER)
-			return;
+			return is_huge;
 
 		/* Not returning to user mode? Handle exceptions or die: */
 		no_context(regs, hw_error_code, address, SIGBUS, BUS_ADRERR);
-		return;
+		return is_huge;
 	}
 
 	up_read(&mm->mmap_sem);
 	if (unlikely(fault & VM_FAULT_ERROR)) {
 		mm_fault_error(regs, hw_error_code, address, fault);
-		return;
+		return is_huge;
+	}
+
+	// markm: check if we should promote the recently created page.
+	if (!is_huge && huge_addr_enabled(vma, address)) {
+		ret = promote_to_huge(mm, vma, address & HPAGE_PMD_MASK, pftrace);
+		if (ret == SCAN_SUCCEED) {
+			mm_register_promotion(address & HPAGE_PMD_MASK);
+		}
 	}
 
 	/*
@@ -1483,27 +1521,33 @@ void do_user_addr_fault(struct pt_regs *regs,
 	}
 
 	check_v8086_mode(regs, address, tsk);
+
+	return is_huge;
 }
 NOKPROBE_SYMBOL(do_user_addr_fault);
 
 /*
  * Explicitly marked noinline such that the function tracer sees this as the
  * page_fault entry point.
+ *
+ * (markm) returns true if the page fault resulted in a huge page allocation.
  */
-static noinline void
+static noinline bool
 __do_page_fault(struct pt_regs *regs, unsigned long hw_error_code,
-		unsigned long address)
+		unsigned long address, struct mm_stats_pftrace *pftrace)
 {
 	prefetchw(&current->mm->mmap_sem);
 
 	if (unlikely(kmmio_fault(regs, address)))
-		return;
+		return false;
 
 	/* Was the fault on kernel-controlled part of the address space? */
-	if (unlikely(fault_in_kernel_space(address)))
+	if (unlikely(fault_in_kernel_space(address))) {
 		do_kern_addr_fault(regs, hw_error_code, address);
-	else
-		do_user_addr_fault(regs, hw_error_code, address);
+        return false;
+    } else {
+		return do_user_addr_fault(regs, hw_error_code, address, pftrace);
+    }
 }
 NOKPROBE_SYMBOL(__do_page_fault);
 
@@ -1524,10 +1568,28 @@ dotraplinkage void
 do_page_fault(struct pt_regs *regs, unsigned long error_code, unsigned long address)
 {
 	enum ctx_state prev_state;
+    bool huge;
+
+    struct mm_stats_pftrace pftrace;
+    mm_stats_pftrace_init(&pftrace);
+    pftrace.start_tsc = rdtsc();
 
 	prev_state = exception_enter();
 	trace_page_fault_entries(regs, error_code, address);
-	__do_page_fault(regs, error_code, address);
+	huge = __do_page_fault(regs, error_code, address, &pftrace);
 	exception_exit(prev_state);
+
+    pftrace.end_tsc = rdtsc();
+
+    if (huge) {
+	mm_stats_set_flag(&pftrace, MM_STATS_PF_HUGE_PAGE);
+        mm_stats_hist_measure(&mm_huge_page_fault_cycles,
+			pftrace.end_tsc - pftrace.start_tsc);
+    } else {
+        mm_stats_hist_measure(&mm_base_page_fault_cycles,
+			pftrace.end_tsc - pftrace.start_tsc);
+    }
+
+    mm_stats_pftrace_submit(&pftrace, regs);
 }
 NOKPROBE_SYMBOL(do_page_fault);
diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c
index e6a9edc5baaf..f37b0343f786 100644
--- a/arch/x86/mm/tlb.c
+++ b/arch/x86/mm/tlb.c
@@ -800,7 +800,7 @@ void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,
 	put_flush_tlb_info();
 	put_cpu();
 }
-
+EXPORT_SYMBOL(flush_tlb_mm_range);
 
 static void do_flush_tlb_all(void *info)
 {
diff --git a/drivers/char/random.c b/drivers/char/random.c
index ea1973d35843..5d0519232208 100644
--- a/drivers/char/random.c
+++ b/drivers/char/random.c
@@ -2484,6 +2484,25 @@ randomize_page(unsigned long start, unsigned long range)
 	return start + (get_random_long() % range << PAGE_SHIFT);
 }
 
+unsigned long
+randomize_huge_page(unsigned long start, unsigned long range)
+{
+	if (!IS_ALIGNED(start, HPAGE_SIZE)) {
+		range -= ALIGN(start, HPAGE_SIZE) - start;
+		start = ALIGN(start, HPAGE_SIZE);
+	}
+
+	if (start > ULONG_MAX - range)
+		range = ULONG_MAX - start;
+
+	range >> HPAGE_SHIFT;
+
+	if (range == 0)
+		return start;
+
+	return start + (get_random_long() % range << HPAGE_SHIFT);
+}
+
 /* Interface for in-kernel drivers of true hardware RNGs.
  * Those devices may produce endless random bits and will be throttled
  * when our pool is full.
diff --git a/drivers/iommu/amd_iommu_v2.c b/drivers/iommu/amd_iommu_v2.c
index d6d85debd01b..4c8492da2e97 100644
--- a/drivers/iommu/amd_iommu_v2.c
+++ b/drivers/iommu/amd_iommu_v2.c
@@ -17,6 +17,7 @@
 #include <linux/wait.h>
 #include <linux/pci.h>
 #include <linux/gfp.h>
+#include <linux/mm_stats.h>
 
 #include "amd_iommu_types.h"
 #include "amd_iommu_proto.h"
@@ -477,6 +478,8 @@ static void do_fault(struct work_struct *work)
 	unsigned int flags = 0;
 	struct mm_struct *mm;
 	u64 address;
+	struct mm_stats_pftrace pftrace; // dummy, not used
+	memset(&pftrace, 0, sizeof(pftrace));
 
 	mm = fault->state->mm;
 	address = fault->address;
@@ -497,7 +500,7 @@ static void do_fault(struct work_struct *work)
 	if (access_error(vma, fault))
 		goto out;
 
-	ret = handle_mm_fault(vma, address, flags);
+	ret = handle_mm_fault(vma, address, flags, &pftrace);
 out:
 	up_read(&mm->mmap_sem);
 
diff --git a/fs/binfmt_elf.c b/fs/binfmt_elf.c
index ecd8d2698515..9301a15c7df2 100644
--- a/fs/binfmt_elf.c
+++ b/fs/binfmt_elf.c
@@ -114,7 +114,7 @@ static int set_brk(unsigned long start, unsigned long end, int prot)
 		if (error)
 			return error;
 	}
-	current->mm->start_brk = current->mm->brk = end;
+	current->mm->start_brk = current->mm->brk = ALIGN(end, HPAGE_SIZE);
 	return 0;
 }
 
diff --git a/fs/exec.c b/fs/exec.c
index 74d88dab98dd..b3f481778e3c 100644
--- a/fs/exec.c
+++ b/fs/exec.c
@@ -62,6 +62,8 @@
 #include <linux/oom.h>
 #include <linux/compat.h>
 #include <linux/vmalloc.h>
+#include <linux/badger_trap.h>
+#include <linux/mm_econ.h>
 
 #include <linux/uaccess.h>
 #include <asm/mmu_context.h>
@@ -1374,6 +1376,29 @@ void setup_new_exec(struct linux_binprm * bprm)
 	perf_event_exec();
 	__set_task_comm(current, kbasename(bprm->filename), true);
 
+	/* huge_addr checking */
+	if (strncmp(current->comm, huge_addr_comm, MAX_HUGE_ADDR_COMM) == 0) {
+		huge_addr_pid = current->pid;
+		pr_warn("Setting new huge_addr_pid=%d\n", huge_addr_pid);
+	} else {
+		//pr_info("NOT huge_addr process (%s) huge_addr_comm=%s.\n",
+		//		current->comm, huge_addr_comm);
+	}
+
+	/* Check if we need to enable badger trap for this process*/
+	if(is_badger_trap_process(current->comm)) {
+		badger_trap_walk(current->mm, 0, ~0ull, true);
+	}
+
+	if(current
+		&& current->real_parent
+		&& current->real_parent != current
+		&& current->real_parent->mm
+		&& current->real_parent->mm->badger_trap_enabled)
+	{
+		badger_trap_walk(current->mm, 0, ~0ull, true);
+	}
+
 	/* Set the new mm task size. We have to do that late because it may
 	 * depend on TIF_32BIT which is only updated in flush_thread() on
 	 * some architectures like powerpc
@@ -1720,6 +1745,7 @@ static int __do_execve_file(int fd, struct filename *filename,
 	char *pathbuf = NULL;
 	struct linux_binprm *bprm;
 	struct files_struct *displaced;
+    struct mm_struct *new_mm;
 	int retval;
 
 	if (IS_ERR(filename))
@@ -1834,6 +1860,17 @@ static int __do_execve_file(int fd, struct filename *filename,
 		putname(filename);
 	if (displaced)
 		put_files_struct(displaced);
+
+	// Bijan: If exec succeded, check if this is the process we want to track
+#ifdef CONFIG_MM_ECON
+	new_mm = current->mm;
+	mm_add_memory_range(current->tgid, SectionCode, new_mm->start_code, 0, 0,
+	    new_mm->end_code - new_mm->start_code, 0, 0, 0, 0);
+	mm_add_memory_range(current->tgid, SectionData, new_mm->start_data, 0, 0,
+	    new_mm->end_data - new_mm->start_data, 0, 0, 0, 0);
+	mm_add_memory_range(current->tgid, SectionHeap, new_mm->start_brk, 0, 0,
+	    new_mm->brk - new_mm->start_brk, 0, 0, 0, 0);
+#endif
 	return retval;
 
 out:
diff --git a/fs/proc/base.c b/fs/proc/base.c
index ebea9501afb8..e23c0752785f 100644
--- a/fs/proc/base.c
+++ b/fs/proc/base.c
@@ -70,6 +70,7 @@
 #include <linux/kallsyms.h>
 #include <linux/stacktrace.h>
 #include <linux/resource.h>
+#include <linux/mm_econ.h>
 #include <linux/module.h>
 #include <linux/mount.h>
 #include <linux/security.h>
@@ -148,6 +149,11 @@ struct pid_entry {
 		NULL, &proc_pid_attr_operations,	\
 		{ .lsm = LSM })
 
+// This is a hack to get get_proc_taskin mm/estimator.c
+inline struct task_struct *extern_get_proc_task(const struct inode *inode)
+{
+    return get_proc_task(inode);
+}
 /*
  * Count the number of hardlinks for the pid_entry table, excluding the .
  * and .. links.
@@ -3097,6 +3103,10 @@ static const struct pid_entry tgid_base_stuff[] = {
 #ifdef CONFIG_PROC_PID_ARCH_STATUS
 	ONE("arch_status", S_IRUGO, proc_pid_arch_status),
 #endif
+#ifdef CONFIG_MM_ECON
+    REG("mmap_filters", S_IRUGO|S_IWUSR, proc_mmap_filters_operations),
+    REG("mem_ranges", S_IRUGO, proc_mem_ranges_operations),
+#endif
 };
 
 static int proc_tgid_base_readdir(struct file *file, struct dir_context *ctx)
@@ -3487,6 +3497,10 @@ static const struct pid_entry tid_base_stuff[] = {
 #ifdef CONFIG_PROC_PID_ARCH_STATUS
 	ONE("arch_status", S_IRUGO, proc_pid_arch_status),
 #endif
+#ifdef CONFIG_MM_ECON
+    REG("mmap_filters", S_IRUGO|S_IWUSR, proc_mmap_filters_operations),
+    REG("mem_ranges", S_IRUGO, proc_mem_ranges_operations),
+#endif
 };
 
 static int proc_tid_base_readdir(struct file *file, struct dir_context *ctx)
diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c
index 9442631fd4af..f9f057e2182d 100644
--- a/fs/proc/task_mmu.c
+++ b/fs/proc/task_mmu.c
@@ -582,9 +582,11 @@ static void smaps_pmd_entry(pmd_t *pmd, unsigned long addr,
 	page = follow_trans_huge_pmd(vma, addr, pmd, FOLL_DUMP);
 	if (IS_ERR_OR_NULL(page))
 		return;
-	if (PageAnon(page))
+	if (PageAnon(page)) {
 		mss->anonymous_thp += HPAGE_PMD_SIZE;
-	else if (PageSwapBacked(page))
+		//pr_warn("found anon page: pmd=%lx va=%lx\n",
+		//		native_pmd_val(*pmd), addr);
+	} else if (PageSwapBacked(page))
 		mss->shmem_thp += HPAGE_PMD_SIZE;
 	else if (is_zone_device_page(page))
 		/* pass */;
@@ -841,7 +843,7 @@ static int show_smap(struct seq_file *m, void *v)
 	__show_smap(m, &mss, false);
 
 	seq_printf(m, "THPeligible:		%d\n",
-		   transparent_hugepage_enabled(vma));
+		   transparent_hugepage_enabled(vma, vma->vm_start));
 
 	if (arch_pkeys_enabled())
 		seq_printf(m, "ProtectionKey:  %8u\n", vma_pkey(vma));
diff --git a/include/linux/badger_trap.h b/include/linux/badger_trap.h
new file mode 100644
index 000000000000..cab3ea778a87
--- /dev/null
+++ b/include/linux/badger_trap.h
@@ -0,0 +1,28 @@
+#ifndef _LINUX_BADGER_TRAP_H
+#define _LINUX_BADGER_TRAP_H
+
+#include <linux/mm_types.h>
+
+#include <asm/pgtable_types.h>
+
+#define MAX_NAME_LEN	16
+
+extern char badger_trap_process[CONFIG_NR_CPUS][MAX_NAME_LEN];
+
+void silence(void);
+bool is_badger_trap_process(const char* proc_name);
+bool is_badger_trap_enabled(const struct mm_struct *mm, u64 address);
+inline pte_t pte_mkreserve(pte_t pte);
+inline pte_t pte_unreserve(pte_t pte);
+inline int is_pte_reserved(pte_t pte);
+inline pmd_t pmd_mkreserve(pmd_t pmd);
+inline pmd_t pmd_unreserve(pmd_t pmd);
+inline int is_pmd_reserved(pmd_t pmd);
+inline pud_t pud_mkreserve(pud_t pud);
+inline pud_t pud_unreserve(pud_t pud);
+inline int is_pud_reserved(pud_t pud);
+void badger_trap_set_stats_loc(struct mm_struct *mm, struct badger_trap_stats *stats);
+void badger_trap_walk(struct mm_struct *mm, u64 lower, u64 upper, bool init);
+void print_badger_trap_stats(const struct mm_struct *mm);
+
+#endif /* _LINUX_BADGER_TRAP_H */
diff --git a/include/linux/huge_mm.h b/include/linux/huge_mm.h
index 0b84e13e88e2..5ea1999cf7a1 100644
--- a/include/linux/huge_mm.h
+++ b/include/linux/huge_mm.h
@@ -7,7 +7,14 @@
 
 #include <linux/fs.h> /* only for vma_is_dax() */
 
-extern vm_fault_t do_huge_pmd_anonymous_page(struct vm_fault *vmf);
+// This struct is defined in linux/mm_stats.h, but I don't want to include it
+// here because means any change to that header would require recompiling a
+// bunch of the kernel.
+struct mm_stats_pftrace;
+
+extern vm_fault_t do_huge_pmd_anonymous_page(struct vm_fault *vmf,
+					     struct mm_stats_pftrace *pftrace,
+					     bool require_prezeroed);
 extern int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 			 pmd_t *dst_pmd, pmd_t *src_pmd, unsigned long addr,
 			 struct vm_area_struct *vma);
@@ -24,7 +31,8 @@ static inline void huge_pud_set_accessed(struct vm_fault *vmf, pud_t orig_pud)
 }
 #endif
 
-extern vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd);
+extern vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd,
+				      struct mm_stats_pftrace *pftrace);
 extern struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,
 					  unsigned long addr,
 					  pmd_t *pmd,
@@ -90,13 +98,57 @@ extern struct kobj_attribute shmem_enabled_attr;
 extern bool is_vma_temporary_stack(struct vm_area_struct *vma);
 
 extern unsigned long transparent_hugepage_flags;
+extern pid_t huge_addr_pid;
+extern u64 huge_addr;
+#define MAX_HUGE_ADDR_COMM 48
+extern char huge_addr_comm[MAX_HUGE_ADDR_COMM];
+
+bool huge_addr_enabled(struct vm_area_struct *vma, unsigned long address);
+
+int promote_to_huge(struct mm_struct *mm,
+		struct vm_area_struct *vma,
+		unsigned long address,
+		struct mm_stats_pftrace *pftrace);
+
+enum scan_result {
+	SCAN_FAIL,
+	SCAN_SUCCEED,
+	SCAN_PMD_NULL,
+	SCAN_EXCEED_NONE_PTE,
+	SCAN_PTE_NON_PRESENT,
+	SCAN_PAGE_RO,
+	SCAN_LACK_REFERENCED_PAGE,
+	SCAN_PAGE_NULL,
+	SCAN_SCAN_ABORT,
+	SCAN_PAGE_COUNT,
+	SCAN_PAGE_LRU,
+	SCAN_PAGE_LOCK,
+	SCAN_PAGE_ANON,
+	SCAN_PAGE_COMPOUND,
+	SCAN_ANY_PROCESS,
+	SCAN_VMA_NULL,
+	SCAN_VMA_CHECK,
+	SCAN_ADDRESS_RANGE,
+	SCAN_SWAP_CACHE_PAGE,
+	SCAN_DEL_PAGE_LRU,
+	SCAN_ALLOC_HUGE_PAGE_FAIL,
+	SCAN_CGROUP_CHARGE_FAIL,
+	SCAN_EXCEED_SWAP_PTE,
+	SCAN_TRUNCATED,
+	SCAN_PAGE_HAS_PRIVATE,
+	SCAN_MM_ECON_CANCEL,
+};
 
 /*
  * to be used on vmas which are known to support THP.
  * Use transparent_hugepage_enabled otherwise
  */
-static inline bool __transparent_hugepage_enabled(struct vm_area_struct *vma)
+static inline bool __transparent_hugepage_enabled(struct vm_area_struct *vma, unsigned long address)
 {
+	// markm: if huge_addr is on, use a huge page no matter what...
+	if (huge_addr_enabled(vma, address))
+		return true;
+
 	if (vma->vm_flags & VM_NOHUGEPAGE)
 		return false;
 
@@ -124,7 +176,7 @@ static inline bool __transparent_hugepage_enabled(struct vm_area_struct *vma)
 	return false;
 }
 
-bool transparent_hugepage_enabled(struct vm_area_struct *vma);
+bool transparent_hugepage_enabled(struct vm_area_struct *vma, unsigned long address);
 
 #define HPAGE_CACHE_INDEX_MASK (HPAGE_PMD_NR - 1)
 
@@ -295,7 +347,7 @@ static inline bool __transparent_hugepage_enabled(struct vm_area_struct *vma)
 	return false;
 }
 
-static inline bool transparent_hugepage_enabled(struct vm_area_struct *vma)
+static inline bool transparent_hugepage_enabled(struct vm_area_struct *vma, unsigned long address)
 {
 	return false;
 }
diff --git a/include/linux/list.h b/include/linux/list.h
index 85c92555e31f..c4187c4fea23 100644
--- a/include/linux/list.h
+++ b/include/linux/list.h
@@ -514,6 +514,10 @@ static inline void list_splice_tail_init(struct list_head *list,
 	pos__ != head__ ? list_entry(pos__, type, member) : NULL; \
 })
 
+#define list_last_entry_or_null(ptr, type, member) ({ \
+	list_empty(ptr) ? NULL : list_last_entry(ptr, type, member) ;\
+})
+
 /**
  * list_next_entry - get the next element in list
  * @pos:	the type * to cursor
diff --git a/include/linux/memory.h b/include/linux/memory.h
index 4c75dae8dd29..803c49a0c2ce 100644
--- a/include/linux/memory.h
+++ b/include/linux/memory.h
@@ -144,4 +144,10 @@ extern int for_each_memory_block(void *arg, walk_memory_blocks_func_t func);
  */
 extern struct mutex text_mutex;
 
+// markm: given a virtual address, find the mapping in the huge_addr_pid. Set
+// the pfn, page struct, and a flag accordingly. If error, *page == NULL, and
+// pfn and is_huge are garbage.
+void get_page_mapping(unsigned long address, unsigned long *pfn,
+		struct page **page, bool *is_huge);
+
 #endif /* _LINUX_MEMORY_H_ */
diff --git a/include/linux/mm.h b/include/linux/mm.h
index cfaa8feecfe8..f67f5a13edd1 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -1458,8 +1458,13 @@ int generic_error_remove_page(struct address_space *mapping, struct page *page);
 int invalidate_inode_page(struct page *page);
 
 #ifdef CONFIG_MMU
+// This struct is defined in linux/mm_stats.h, but I don't want to include it
+// here because means any change to that header would require recompiling a
+// bunch of the kernel.
+struct mm_stats_pftrace;
 extern vm_fault_t handle_mm_fault(struct vm_area_struct *vma,
-			unsigned long address, unsigned int flags);
+			unsigned long address, unsigned int flags,
+			struct mm_stats_pftrace *pftrace);
 extern int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,
 			    unsigned long address, unsigned int fault_flags,
 			    bool *unlocked);
@@ -2328,6 +2333,8 @@ extern int __do_munmap(struct mm_struct *, unsigned long, size_t,
 		       struct list_head *uf, bool downgrade);
 extern int do_munmap(struct mm_struct *, unsigned long, size_t,
 		     struct list_head *uf);
+extern int do_madvise(struct mm_struct *mm, unsigned long start,
+		size_t len_in, int behavior);
 
 static inline unsigned long
 do_mmap_pgoff(struct file *file, unsigned long addr,
diff --git a/include/linux/mm_econ.h b/include/linux/mm_econ.h
new file mode 100644
index 000000000000..aab585557714
--- /dev/null
+++ b/include/linux/mm_econ.h
@@ -0,0 +1,106 @@
+#ifndef _MM_ECON_H_
+#define _MM_ECON_H_
+
+#include <linux/fs.h>
+#include <linux/types.h>
+#include <linux/range.h>
+
+// Various possible actions, to be used with `struct mm_action`.
+#define MM_ACTION_NONE                 0
+#define MM_ACTION_PROMOTE_HUGE  (1 <<  0)
+#define MM_ACTION_DEMOTE_HUGE   (1 <<  1)
+#define MM_ACTION_RUN_DEFRAG    (1 <<  2) // kcompactd
+#define MM_ACTION_ALLOC_RECLAIM (1 <<  3)
+#define MM_ACTION_EAGER_PAGING  (1 <<  4)
+#define MM_ACTION_RUN_PREZEROING (1 <<  5) // asynczero
+#define MM_ACTION_RUN_PROMOTION (1 <<  6) // khugepaged
+
+// The length of one Long Time Unit (LTU), the fundamental time accounting unit
+// of mm_econ. This value is in milliseconds (1 LTU = MM_ECON_LTU ms).
+#define MM_ECON_LTU 10000
+
+extern const struct file_operations proc_mmap_filters_operations;
+extern const struct file_operations proc_mem_ranges_operations;
+
+// An action that may be taken by the memory management subsystem.
+struct mm_action {
+    int action;
+
+    u64 address;
+
+    // Extra parameters of the action.
+    union {
+        // No extra parameters are needed.
+        u64 unused;
+
+        // How large is the huge page we are creating? This is the order (e.g. 2MB would be 9)
+        u64 huge_page_order;
+
+        // How many pages are prezeroed?
+        u64 prezero_n;
+
+        // What is the length of a memory region
+        u64 len;
+    };
+};
+
+enum mm_memory_section {
+    SectionCode,
+    SectionData,
+    SectionHeap,
+    SectionMmap,
+};
+
+// A typedef for function pointers for tlb miss estimator functions to be used
+// in estimating the number of TLB misses caused by the given page.
+//
+// The return value must be in units of `misses per LTU`.
+typedef u64 (*mm_econ_tlb_miss_estimator_fn_t)(const struct mm_action *);
+
+void register_mm_econ_tlb_miss_estimator(mm_econ_tlb_miss_estimator_fn_t f);
+
+// The cost of a particular action relative to the status quo.
+struct mm_cost_delta {
+    //// Difference in the number of TLB misses.
+    //s64 tlb_misses;
+
+    //// Difference in the number of page faults.
+    //s64 page_fault_freq;
+
+    //// CPU time spent doing things like coalescing/defraging/zeroing pages.
+    //s64 kernel_computation;
+
+    // Total estimated cost in cycles.
+    u64 cost;
+
+    // Total estimated benefit in cycles.
+    u64 benefit;
+
+    // HACK: extra info about assumptions the estimator made. This isn't
+    // fundamentally needed, but it is the fastest way to avoid races between
+    // the estimator and the execution of policies.
+    u64 extra;
+};
+
+inline bool mm_process_is_using_cbmm(pid_t pid);
+
+bool mm_econ_is_on(void);
+
+bool mm_decide(const struct mm_cost_delta *cost);
+
+void
+mm_estimate_changes(const struct mm_action *action, struct mm_cost_delta *cost);
+
+void mm_register_promotion(u64 addr);
+
+void
+mm_add_memory_range(pid_t pid, enum mm_memory_section section, u64 mapaddr, u64 section_off,
+        u64 addr, u64 len, u64 prot, u64 flags, u64 fd, u64 off);
+
+void mm_copy_profile(pid_t old_pid, pid_t new_pid);
+void mm_profile_check_exiting_proc(pid_t pid);
+
+u64 mm_estimated_prezeroed_used(void);
+
+extern int mm_econ_debugging_mode;
+#endif
diff --git a/include/linux/mm_stats.h b/include/linux/mm_stats.h
new file mode 100644
index 000000000000..1419e596d639
--- /dev/null
+++ b/include/linux/mm_stats.h
@@ -0,0 +1,223 @@
+#ifndef _MM_STATS_H_
+#define _MM_STATS_H_
+
+#include <linux/proc_fs.h>
+#include <linux/types.h>
+
+///////////////////////////////////////////////////////////////////////////////
+// Histograms.
+
+struct mm_hist;
+
+void mm_stats_init(void);
+
+// Add the measurement `val` to the histogram `hist`.
+void mm_stats_hist_measure(struct mm_hist *hist, u64 val);
+
+// Externed stats...
+extern struct mm_hist mm_base_page_fault_cycles;
+extern struct mm_hist mm_huge_page_fault_cycles;
+extern struct mm_hist mm_huge_page_fault_create_new_cycles;
+extern struct mm_hist mm_huge_page_fault_clear_cycles;
+extern struct mm_hist mm_huge_page_fault_zero_page_cycles;
+extern struct mm_hist mm_huge_page_fault_wp_cycles;
+extern struct mm_hist mm_huge_page_fault_cow_copy_huge_cycles;
+extern struct mm_hist mm_direct_compaction_cycles;
+extern struct mm_hist mm_indirect_compaction_cycles;
+extern struct mm_hist mm_direct_reclamation_cycles;
+extern struct mm_hist mm_huge_page_promotion_scanning_cycles;
+extern struct mm_hist mm_huge_page_promotion_work_cycles;
+extern struct mm_hist mm_huge_page_promotion_copy_pages_cycles;
+extern struct mm_hist mm_process_huge_page_cycles;
+extern struct mm_hist mm_process_huge_page_single_page_cycles;
+
+extern struct mm_hist mm_econ_cost;
+extern struct mm_hist mm_econ_benefit;
+
+///////////////////////////////////////////////////////////////////////////////
+// Page fault tracing.
+
+typedef u64 mm_stats_bitflags_t;
+
+struct mm_stats_pftrace {
+	// A bunch of bitflags indicating things that happened during this #PF.
+	// See `mm_econ_flags` for more info.
+	mm_stats_bitflags_t bitflags;
+
+	// The start and end TSC of the #PF.
+	u64 start_tsc;
+	u64 end_tsc;
+
+	// Timestamps at which the #PF did the following:
+	u64 alloc_start_tsc; // started allocating memory
+	u64 alloc_end_tsc;   // finished allocating memory (or OOMed)
+	// In normal linux, this will include the time to zero if GFP_ZERO was
+	// passed to the allocator. Thus, we have another measurement that
+	// includes ONLY THE TIME TO ZERO THE PAGE IN THE ALLOCATOR. This value
+	// is only preset if MM_STATS_PF_CLEARED_MEM is set:
+	u64 alloc_zeroing_duration;
+
+	u64 prep_start_tsc;  // started preparing the alloced mem
+	u64 prep_end_tsc;    // finished ...
+};
+
+// A bunch of bit flags that indicate things that could happen during a #PF.
+//
+// NOTE: Don't forget to update mm_stats_pf_flags_names!
+enum mm_stats_pf_flags {
+	// Set: a huge page was allocated/promoted/mapped.
+	// Clear: a base page was allocated/promoted/mapped.
+	MM_STATS_PF_HUGE_PAGE, // 2MB
+	MM_STATS_PF_VERY_HUGE_PAGE, // 1GB -- should never happen
+
+	// Set: this fault was a BadgerTrap fault.
+	MM_STATS_PF_BADGER_TRAP,
+
+	// Set: this fault was a write-protected page.
+	MM_STATS_PF_WP,
+
+	// Set: this fault happened on an instruction fetch.
+	MM_STATS_PF_EXEC,
+
+	// Set: this fault was a "NUMA hinting fault", possibly with a migration.
+	MM_STATS_PF_NUMA,
+
+	// Set: this fault required a swap-in.
+	MM_STATS_PF_SWAP,
+
+	// Set: this fault was not anonymous (usually this means it was a
+	// file-backed memory region).
+	MM_STATS_PF_NOT_ANON,
+
+	// Set: this non-anonymous page took a load-related fault.
+	MM_STATS_PF_NOT_ANON_READ,
+
+	// Set: this non-anonymous page took a cow-related fault.
+	MM_STATS_PF_NOT_ANON_COW,
+
+	// Set: this non-anonymous page is shared.
+	MM_STATS_PF_NOT_ANON_SHARED,
+
+	// Set: this fault mapped a zero-page.
+	MM_STATS_PF_ZERO,
+
+	// Set: attempted and failed to allocate a 2MB page.
+	MM_STATS_PF_HUGE_ALLOC_FAILED,
+
+	// Set: a huge page was split.
+	MM_STATS_PF_HUGE_SPLIT,
+
+	// Set: an address range was promoted to a huge page (as opposed to
+	// freshly created as a huge page).
+	MM_STATS_PF_HUGE_PROMOTION,
+
+	// Set: we attempted to do a promotion and failed.
+	MM_STATS_PF_HUGE_PROMOTION_FAILED,
+
+	// Set: page contents were copied during promotion.
+	MM_STATS_PF_HUGE_COPY,
+
+	// Set: when a page is zeroed/cleared.
+	MM_STATS_PF_CLEARED_MEM,
+
+	// Set: the physical memory allocator fell back to the slow path.
+	MM_STATS_PF_ALLOC_FALLBACK,
+
+	// Set: the physical memory allocator slowpath executed multiple times.
+	MM_STATS_PF_ALLOC_FALLBACK_RETRY,
+
+	// Set: the physical memory allocator slowpath executed page reclamation.
+	MM_STATS_PF_ALLOC_FALLBACK_RECLAIM,
+
+	// Set: the physical memory allocator slowpath executed page compaction.
+	MM_STATS_PF_ALLOC_FALLBACK_COMPACT,
+
+	// Set: the physical memory allocator allocated a pre-zeroed page for a
+	// request with GFP_ZERO.
+	MM_STATS_PF_ALLOC_PREZEROED,
+
+	// Set: the physical memory allocator attempted to shrink nodes in the
+	// fast path ("node reclaim"), as opposed to direct reclaim.
+	MM_STATS_PF_ALLOC_NODE_RECLAIM,
+
+	// NOTE: must be the last value in the enum... not actually a flag.
+	MM_STATS_NUM_FLAGS,
+};
+static_assert(MM_STATS_NUM_FLAGS <= sizeof(mm_stats_bitflags_t) * 8);
+
+// Names of the above flags for printing as text.
+extern char *mm_stats_pf_flags_names[MM_STATS_NUM_FLAGS];
+
+// Hacky mechanism for determining if last allocation has failed.
+DECLARE_PER_CPU(bool, pftrace_alloc_fallback);
+DECLARE_PER_CPU(bool, pftrace_alloc_fallback_retry);
+DECLARE_PER_CPU(bool, pftrace_alloc_fallback_reclaim);
+DECLARE_PER_CPU(bool, pftrace_alloc_fallback_compact);
+DECLARE_PER_CPU(bool, pftrace_alloc_node_reclaim);
+DECLARE_PER_CPU(bool, pftrace_alloc_zeroed_page);
+DECLARE_PER_CPU(u64, pftrace_alloc_zeroing_duration);
+DECLARE_PER_CPU(bool, pftrace_alloc_prezeroed);
+
+static inline void mm_stats_set_flag(
+		struct mm_stats_pftrace *trace,
+		enum mm_stats_pf_flags flag)
+{
+	trace->bitflags |= 1ull << flag;
+}
+
+static inline void mm_stats_clear_flag(
+		struct mm_stats_pftrace *trace,
+		enum mm_stats_pf_flags flag)
+{
+	trace->bitflags &= ~(1ull << flag);
+}
+
+static inline bool mm_stats_test_flag(
+		struct mm_stats_pftrace *trace,
+		enum mm_stats_pf_flags flag)
+{
+	return !!(trace->bitflags & (1ull << flag));
+}
+
+static inline void mm_stats_check_alloc_fallback(
+		struct mm_stats_pftrace *trace)
+{
+	if (get_cpu_var(pftrace_alloc_fallback)) {
+		mm_stats_set_flag(trace, MM_STATS_PF_ALLOC_FALLBACK);
+	}
+	if (get_cpu_var(pftrace_alloc_fallback_retry)) {
+		mm_stats_set_flag(trace, MM_STATS_PF_ALLOC_FALLBACK_RETRY);
+	}
+	if (get_cpu_var(pftrace_alloc_fallback_reclaim)) {
+		mm_stats_set_flag(trace, MM_STATS_PF_ALLOC_FALLBACK_RECLAIM);
+	}
+	if (get_cpu_var(pftrace_alloc_fallback_compact)) {
+		mm_stats_set_flag(trace, MM_STATS_PF_ALLOC_FALLBACK_COMPACT);
+	}
+	if (get_cpu_var(pftrace_alloc_node_reclaim)) {
+		mm_stats_set_flag(trace, MM_STATS_PF_ALLOC_NODE_RECLAIM);
+	}
+	if (get_cpu_var(pftrace_alloc_prezeroed)) {
+		mm_stats_set_flag(trace, MM_STATS_PF_ALLOC_PREZEROED);
+	}
+}
+
+static inline void mm_stats_check_alloc_zeroing(
+		struct mm_stats_pftrace *trace)
+{
+	if (get_cpu_var(pftrace_alloc_zeroed_page)) {
+		mm_stats_set_flag(trace, MM_STATS_PF_CLEARED_MEM);
+		trace->alloc_zeroing_duration =
+			get_cpu_var(pftrace_alloc_zeroing_duration);
+	}
+}
+
+// Initialize the given struct.
+void mm_stats_pftrace_init(struct mm_stats_pftrace *trace);
+
+// Registers a complete sample with the sampling system after it is complete
+// (i.e. at the end of a page fault). The sampling system may then choose to
+// store or drop the sample probablistically.
+void mm_stats_pftrace_submit(struct mm_stats_pftrace *trace, struct pt_regs *regs);
+
+#endif
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 270aa8fd2800..0eb7f2e02718 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -353,6 +353,8 @@ struct vm_area_struct {
 	struct mempolicy *vm_policy;	/* NUMA policy for the VMA */
 #endif
 	struct vm_userfaultfd_ctx vm_userfaultfd_ctx;
+
+	//struct badger_trap_stats bt_stats;
 } __randomize_layout;
 
 struct core_thread {
@@ -524,6 +526,35 @@ struct mm_struct {
 		atomic_long_t hugetlb_usage;
 #endif
 		struct work_struct async_put_work;
+
+		/* BadgerTrap */
+		bool badger_trap_enabled;
+		// Used to check if badger trap was ever enabled for a process.
+		// This helps handle the race condition where bt was just
+		// turned off, but a racing #PF needs to check if the RSVD bit
+		// can be set. Basically, it is used to taint an address space,
+		// indicating that stray reserved bits are likely not an error.
+		bool badger_trap_was_enabled;
+		// The start and end addresses for badgertrap.
+		u64 badger_trap_start;
+		u64 badger_trap_end; // inclusive
+		// Counters for TLB misses for badgertrap.
+		//
+		// Always access through bt_stats. If it doesn't point
+		// somewhere else, this pointer will point at bt_stats_inner.
+		struct badger_trap_stats *bt_stats;
+		struct badger_trap_stats bt_stats_inner;
+		// This semaphore acts as a read/write lock on the page tables.
+		// To simplify things (a lot!), we simply preclude concurrent
+		// changes to the page tables while badgertrap is walking page
+		// tables. We do this by grabbing this lock in exclusive mode
+		// in badgertrap, but grabbing it in shared mode everywhere
+		// else that page tables are modified. This allows other kernel
+		// functions to procede concurrently with each other, as in the
+		// normal linux kernel, but not concurrently with badgertrap.
+		//
+		// Ordering: grab mmap_sem before this one.
+		struct rw_semaphore badger_trap_page_table_sem;
 	} __randomize_layout;
 
 	/*
@@ -673,6 +704,7 @@ typedef __bitwise unsigned int vm_fault_t;
  *				in DAX)
  * @VM_FAULT_HINDEX_MASK:	mask HINDEX value
  *
+ * @VM_FAULT_BASE_PAGE      fault allocated a base page.
  */
 enum vm_fault_reason {
 	VM_FAULT_OOM            = (__force vm_fault_t)0x000001,
@@ -688,6 +720,7 @@ enum vm_fault_reason {
 	VM_FAULT_FALLBACK       = (__force vm_fault_t)0x000800,
 	VM_FAULT_DONE_COW       = (__force vm_fault_t)0x001000,
 	VM_FAULT_NEEDDSYNC      = (__force vm_fault_t)0x002000,
+	VM_FAULT_BASE_PAGE      = (__force vm_fault_t)0x004000,
 	VM_FAULT_HINDEX_MASK    = (__force vm_fault_t)0x0f0000,
 };
 
diff --git a/include/linux/mm_types_task.h b/include/linux/mm_types_task.h
index c1bc6731125c..89b362d5b6aa 100644
--- a/include/linux/mm_types_task.h
+++ b/include/linux/mm_types_task.h
@@ -12,6 +12,7 @@
 #include <linux/threads.h>
 #include <linux/atomic.h>
 #include <linux/cpumask.h>
+#include <linux/spinlock.h>
 
 #include <asm/page.h>
 
@@ -61,6 +62,40 @@ struct mm_rss_stat {
 	atomic_long_t count[NR_MM_COUNTERS];
 };
 
+struct badger_trap_stats {
+	atomic64_t total_dtlb_4kb_store_misses;
+	atomic64_t total_dtlb_2mb_store_misses;
+	atomic64_t total_dtlb_4kb_load_misses;
+	atomic64_t total_dtlb_2mb_load_misses;
+};
+
+static inline void badger_trap_stats_clear(struct badger_trap_stats *stats)
+{
+	atomic64_set_release(&stats->total_dtlb_4kb_store_misses, 0);
+	atomic64_set_release(&stats->total_dtlb_2mb_store_misses, 0);
+	atomic64_set_release(&stats->total_dtlb_4kb_load_misses,  0);
+	atomic64_set_release(&stats->total_dtlb_2mb_load_misses,  0);
+}
+
+static inline void badger_trap_stats_init(struct badger_trap_stats *stats)
+{
+	badger_trap_stats_clear(stats);
+}
+
+static inline void badger_trap_add_stats(
+		struct badger_trap_stats *to,
+		const struct badger_trap_stats *from)
+{
+	atomic64_add(atomic64_read_acquire(&from->total_dtlb_4kb_store_misses),
+			&to->total_dtlb_4kb_store_misses);
+	atomic64_add(atomic64_read_acquire(&from->total_dtlb_2mb_store_misses),
+			&to->total_dtlb_2mb_store_misses);
+	atomic64_add(atomic64_read_acquire(&from->total_dtlb_4kb_load_misses),
+			&to->total_dtlb_4kb_load_misses);
+	atomic64_add(atomic64_read_acquire(&from->total_dtlb_2mb_load_misses),
+			&to->total_dtlb_2mb_load_misses);
+}
+
 struct page_frag {
 	struct page *page;
 #if (BITS_PER_LONG > 32) || (PAGE_SIZE >= 65536)
diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index 5334ad8fc7bd..f6acb58aa762 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -135,10 +135,15 @@ static inline void move_to_free_area(struct page *page, struct free_area *area,
 	list_move(&page->lru, &area->free_list[migratetype]);
 }
 
+// markm: if `front`, then take from the front; otherwise, take from the tail.
 static inline struct page *get_page_from_free_area(struct free_area *area,
-					    int migratetype)
+					    int migratetype, bool front)
 {
-	return list_first_entry_or_null(&area->free_list[migratetype],
+	if (front)
+		return list_first_entry_or_null(&area->free_list[migratetype],
+					struct page, lru);
+	else
+		return list_last_entry_or_null(&area->free_list[migratetype],
 					struct page, lru);
 }
 
diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
index 1bf83c8fcaa7..b7969750346c 100644
--- a/include/linux/page-flags.h
+++ b/include/linux/page-flags.h
@@ -118,6 +118,7 @@ enum pageflags {
 	PG_reclaim,		/* To be reclaimed asap */
 	PG_swapbacked,		/* Page is backed by RAM/swap */
 	PG_unevictable,		/* Page is "unevictable"  */
+	PG_zeroed,
 #ifdef CONFIG_MMU
 	PG_mlocked,		/* Page is vma mlocked */
 #endif
@@ -836,9 +837,13 @@ static inline void ClearPageSlabPfmemalloc(struct page *page)
  *
  * __PG_HWPOISON is exceptional because it needs to be kept beyond page's
  * alloc-free cycle to prevent from reusing the page.
+ *
+ * The zeroed bit is also freed from the check here because it may or may not
+ * be zero at the time of allocation and we need to take appropriate action
+ * based on the state of zeroed bit.
  */
 #define PAGE_FLAGS_CHECK_AT_PREP	\
-	(((1UL << NR_PAGEFLAGS) - 1) & ~__PG_HWPOISON)
+	(((1UL << NR_PAGEFLAGS) - 1) & ~__PG_HWPOISON & ~(1UL << PG_zeroed))
 
 #define PAGE_FLAGS_PRIVATE				\
 	(1UL << PG_private | 1UL << PG_private_2)
@@ -859,6 +864,22 @@ static inline int page_has_private(struct page *page)
 #undef PF_ONLY_HEAD
 #undef PF_NO_TAIL
 #undef PF_NO_COMPOUND
+
+static inline int PageZeroed(struct page *page)
+{
+	return test_bit(PG_zeroed, &page->flags);
+}
+
+static inline void SetPageZeroed(struct page *page)
+{
+	set_bit(PG_zeroed, &page->flags);
+}
+
+static inline void ClearPageZeroed(struct page *page)
+{
+	clear_bit(PG_zeroed, &page->flags);
+}
+
 #endif /* !__GENERATING_BOUNDS_H */
 
 #endif	/* PAGE_FLAGS_H */
diff --git a/include/linux/random.h b/include/linux/random.h
index f189c927fdea..5c98c67587ab 100644
--- a/include/linux/random.h
+++ b/include/linux/random.h
@@ -107,6 +107,7 @@ declare_get_random_var_wait(long)
 #undef declare_get_random_var
 
 unsigned long randomize_page(unsigned long start, unsigned long range);
+unsigned long randomize_huge_page(unsigned long start, unsigned long range);
 
 u32 prandom_u32(void);
 void prandom_bytes(void *buf, size_t nbytes);
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index 5262b7a76d39..82ee6c5fed03 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -1000,6 +1000,9 @@ asmlinkage long sys_fspick(int dfd, const char __user *path, unsigned int flags)
 asmlinkage long sys_pidfd_send_signal(int pidfd, int sig,
 				       siginfo_t __user *info,
 				       unsigned int flags);
+asmlinkage long sys_init_badger_trap(const char __user** process_name,
+					unsigned long num_procs,
+					int options);
 
 /*
  * Architecture-specific system calls
diff --git a/include/trace/events/mmflags.h b/include/trace/events/mmflags.h
index a1675d43777e..2e1bc6d484b8 100644
--- a/include/trace/events/mmflags.h
+++ b/include/trace/events/mmflags.h
@@ -100,7 +100,8 @@
 	{1UL << PG_mappedtodisk,	"mappedtodisk"	},		\
 	{1UL << PG_reclaim,		"reclaim"	},		\
 	{1UL << PG_swapbacked,		"swapbacked"	},		\
-	{1UL << PG_unevictable,		"unevictable"	}		\
+	{1UL << PG_unevictable,		"unevictable"	},		\
+	{1UL << PG_zeroed, 		"zeroed" 	} 		\
 IF_HAVE_PG_MLOCK(PG_mlocked,		"mlocked"	)		\
 IF_HAVE_PG_UNCACHED(PG_uncached,	"uncached"	)		\
 IF_HAVE_PG_HWPOISON(PG_hwpoison,	"hwpoison"	)		\
diff --git a/init/Kconfig b/init/Kconfig
index 47d40f399000..48d5173e1ec9 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -823,6 +823,7 @@ config PAGE_COUNTER
 
 config MEMCG
 	bool "Memory controller"
+	default y
 	select PAGE_COUNTER
 	select EVENTFD
 	help
diff --git a/init/main.c b/init/main.c
index da1bc0b60a7d..5a8dd70ac730 100644
--- a/init/main.c
+++ b/init/main.c
@@ -93,6 +93,7 @@
 #include <linux/rodata_test.h>
 #include <linux/jump_label.h>
 #include <linux/mem_encrypt.h>
+#include <linux/mm_stats.h>
 
 #include <asm/io.h>
 #include <asm/bugs.h>
@@ -1190,6 +1191,8 @@ static noinline void __init kernel_init_freeable(void)
 
 	init_mm_internals();
 
+    mm_stats_init();
+
 	do_pre_smp_initcalls();
 	lockup_detector_init();
 
diff --git a/kernel/exit.c b/kernel/exit.c
index 2833ffb0c211..2452d63f6411 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -63,6 +63,8 @@
 #include <linux/random.h>
 #include <linux/rcuwait.h>
 #include <linux/compat.h>
+#include <linux/badger_trap.h>
+#include <linux/mm_econ.h>
 
 #include <linux/uaccess.h>
 #include <asm/unistd.h>
@@ -716,6 +718,42 @@ void __noreturn do_exit(long code)
 	profile_task_exit(tsk);
 	kcov_task_exit(tsk);
 
+	/*
+	 * Statistics for Badger Trap
+	 */
+	if(current->mm && current->mm->badger_trap_enabled)
+	{
+		// Each thread will print. We just need to take the last one.
+		pr_warn("BadgerTrap: Task terminating. current=%p pid=%d tgid=%d mm=%p parent_pid=%d\n",
+				current, current->pid, current->tgid, current->mm,
+				current->real_parent->pid);
+		print_badger_trap_stats(current->mm);
+	}
+
+#ifdef CONFIG_MM_ECON
+	// Bijan: When a process exits, check if we should stop tracking it for
+	// the memory profile
+	// We only care if the main thread exits, so check against tsk->pid
+	// instead of tsk->tgid
+	mm_profile_check_exiting_proc(tsk->pid);
+#endif
+
+	/* markm: doesn't seem to work properly if main thread exits not-last.
+	if(current->mm && current->mm->badger_trap_enabled
+			&& current->tgid == current->pid)
+	{
+		if(current->real_parent->mm->badger_trap_enabled)
+		{
+			badger_trap_add_stats(&current->real_parent->mm->bt_stats,
+					&current->mm->bt_stats);
+		}
+		else
+		{
+			print_badger_trap_stats(current->mm);
+		}
+	}
+	*/
+
 	WARN_ON(blk_needs_flush_plug(tsk));
 
 	if (unlikely(in_interrupt()))
diff --git a/kernel/fork.c b/kernel/fork.c
index 080809560072..51e3b66606de 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -94,6 +94,8 @@
 #include <linux/thread_info.h>
 #include <linux/stackleak.h>
 #include <linux/kasan.h>
+#include <linux/badger_trap.h>
+#include <linux/mm_econ.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -1047,6 +1049,12 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
 		goto fail_nocontext;
 
 	mm->user_ns = get_user_ns(user_ns);
+
+	// Clear badger trap stats for new address space...
+	badger_trap_set_stats_loc(mm, NULL);
+	badger_trap_stats_init(mm->bt_stats);
+	init_rwsem(&mm->badger_trap_page_table_sem);
+
 	return mm;
 
 fail_nocontext:
@@ -2442,6 +2450,9 @@ long _do_fork(struct kernel_clone_args *args)
 		get_task_struct(p);
 	}
 
+	/* Copy the mm_econ profile if applicable */
+	mm_copy_profile(current->tgid, p->tgid);
+
 	wake_up_new_task(p);
 
 	/* forking complete and child started to run, tell ptracer */
diff --git a/mm/Kconfig b/mm/Kconfig
index ab80933be65f..6a1c1b408ff6 100644
--- a/mm/Kconfig
+++ b/mm/Kconfig
@@ -407,6 +407,11 @@ choice
 	  benefit.
 endchoice
 
+config MM_ECON
+    bool "Use economic models for memory management"
+    help
+        Use economic models for memory management.
+
 config ARCH_WANTS_THP_SWAP
 	def_bool n
 
@@ -739,4 +744,7 @@ config ARCH_HAS_HUGEPD
 config MAPPING_DIRTY_HELPERS
         bool
 
+source "mm/kbadgerd/Kconfig"
+source "mm/asynczero/Kconfig"
+
 endmenu
diff --git a/mm/Makefile b/mm/Makefile
index 1937cc251883..b076e448426d 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -42,7 +42,7 @@ obj-y			:= filemap.o mempool.o oom_kill.o fadvise.o \
 			   mm_init.o mmu_context.o percpu.o slab_common.o \
 			   compaction.o vmacache.o \
 			   interval_tree.o list_lru.o workingset.o \
-			   debug.o gup.o $(mmu-y)
+			   debug.o gup.o mm_stats.o badger_trap.o $(mmu-y)
 
 # Give 'page_alloc' its own module-parameter namespace
 page-alloc-y := page_alloc.o
@@ -108,3 +108,6 @@ obj-$(CONFIG_ZONE_DEVICE) += memremap.o
 obj-$(CONFIG_HMM_MIRROR) += hmm.o
 obj-$(CONFIG_MEMFD_CREATE) += memfd.o
 obj-$(CONFIG_MAPPING_DIRTY_HELPERS) += mapping_dirty_helpers.o
+obj-$(CONFIG_MM_ECON) += estimator.o
+obj-$(CONFIG_KBADGERD) += kbadgerd/
+obj-$(CONFIG_ASYNCZERO) += asynczero/
diff --git a/mm/asynczero/Kconfig b/mm/asynczero/Kconfig
new file mode 100644
index 000000000000..d3b3ed24fd2c
--- /dev/null
+++ b/mm/asynczero/Kconfig
@@ -0,0 +1,9 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+config ASYNCZERO
+	tristate "Adds the asynczero module."
+	default m
+	depends on MEMCG && TRANSPARENT_HUGEPAGE
+	help
+	  Adds the asynczero module.
+
diff --git a/mm/asynczero/Makefile b/mm/asynczero/Makefile
new file mode 100644
index 000000000000..f137d1b5a7cd
--- /dev/null
+++ b/mm/asynczero/Makefile
@@ -0,0 +1 @@
+obj-m += asynczero.o
diff --git a/mm/asynczero/asynczero.c b/mm/asynczero/asynczero.c
new file mode 100644
index 000000000000..f1849e1244b8
--- /dev/null
+++ b/mm/asynczero/asynczero.c
@@ -0,0 +1,276 @@
+#include <linux/module.h>	/* Needed by all modules */
+#include <linux/kernel.h>	/* Needed for KERN_INFO */
+#include <linux/mmzone.h>
+#include <linux/mm.h>
+#include <linux/delay.h>
+#include <linux/highmem.h>
+#include <linux/kthread.h>
+#include <linux/sched/task.h>
+#include <linux/mm_econ.h>
+#include <asm/page_64.h>
+
+#define HUGE_PAGE_ORDER 9
+
+static struct task_struct *asynczero_task = NULL;
+static volatile bool asynczero_should_stop = false;
+
+// Only used if mm_econ is off.
+int sleep = 1000;
+module_param(sleep, int, 0644);
+
+int count = 10;
+module_param(count, int, 0644);
+
+// For debugging...
+// 0 = mm_econ, 1 = act as if mm_econ is off (even if it is on)
+int mode = 0;
+module_param(mode, int, 0644);
+
+//int zero_fill_order = MAX_ORDER - 1;
+//module_param(zero_fill_order, int, 0644);
+
+u64 pages_zeroed = 0;
+module_param(pages_zeroed, ullong, 0444);
+
+static inline bool skip_zone(struct zone *zone)
+{
+	// Skip the zone if it is ZONE_DMA or ZONE_DMA32
+	enum zone_type zt = zone_idx(zone);
+	return zt == ZONE_DMA || zt == ZONE_DMA32;
+}
+
+/*
+ * preferrably use the architecture specific extensions to zero-fill a page.
+ * use memset as a fallback option.
+ */
+static inline void zero_fill_page_ntstores(struct page *page)
+{
+	void *kaddr;
+	kaddr = kmap_atomic(page);
+	__asm__ (
+		"push %%rax;"
+		"push %%rcx;"
+		"push %%rdi;"
+		"movq	%0, %%rdi;"
+		"xorq    %%rax, %%rax;"
+		"movl    $4096/64, %%ecx;"
+		".p2align 4;"
+		"1:;"
+		"decl    %%ecx;"
+		"movnti  %%rax,(%%rdi);"
+		"movnti  %%rax,0x8(%%rdi);"
+		"movnti  %%rax,0x10(%%rdi);"
+		"movnti  %%rax,0x18(%%rdi);"
+		"movnti  %%rax,0x20(%%rdi);"
+		"movnti  %%rax,0x28(%%rdi);"
+		"movnti  %%rax,0x30(%%rdi);"
+		"movnti  %%rax,0x38(%%rdi);"
+		"leaq    64(%%rdi),%%rdi;"
+		"jnz     1b;"
+		"nop;"
+		"pop %%rdi;"
+		"pop %%rcx;"
+		"pop %%rax;"
+		: /* output */
+		: "a" (kaddr)
+	);
+	kunmap_atomic(kaddr);
+	SetPageZeroed(page);
+}
+
+/* the core logic to zero-fill a compound page */
+static inline void zero_fill_compound_page(struct page *page, int order)
+{
+	int i;
+
+	if (PageZeroed(page))
+		return;
+
+	for (i = 0; i < (1 << order); i++) {
+		/* kernel's in-built zeroing function */
+		//clear_highpage(page + i);
+
+		/* custom zero-filling logic */
+		zero_fill_page_ntstores(page + i);
+	}
+
+	pages_zeroed += 1 << order;
+}
+
+static int zero_fill_zone_pages(struct zone *zone, int *n)
+{
+	struct page *page;
+	struct free_area *area;
+	unsigned long flags;
+	int order;
+	bool zeroed_something = false;
+
+        for (order = HUGE_PAGE_ORDER; order < MAX_ORDER; ++order) {
+		unsigned long retries = 0;
+		while (retries < 100) {
+			area = &(zone->free_area[order]);
+
+			/* remove one page from freelist with the lock held */
+			spin_lock_irqsave(&zone->lock, flags);
+			page = list_first_entry_or_null(&area->free_list[MIGRATE_MOVABLE],
+					struct page, lru);
+			if (!page) {
+				spin_unlock_irqrestore(&zone->lock, flags);
+				break;
+			}
+			if (PageZeroed(page)) {
+				retries++;
+				// Move to tail
+				list_move_tail(&page->lru, &area->free_list[MIGRATE_MOVABLE]);
+				spin_unlock_irqrestore(&zone->lock, flags);
+				continue;
+			}
+			list_del(&page->lru);
+			__ClearPageBuddy(page);
+			set_page_private(page, 0);
+			area->nr_free--;
+			spin_unlock_irqrestore(&zone->lock, flags);
+
+			// zero fill
+			zero_fill_compound_page(page, order);
+			zeroed_something = true;
+
+			// add back to freelist
+			spin_lock_irqsave(&zone->lock, flags);
+			set_page_private(page, order);
+			__SetPageBuddy(page);
+			list_add_tail(&page->lru, &area->free_list[MIGRATE_MOVABLE]);
+			area->nr_free++;
+			spin_unlock_irqrestore(&zone->lock, flags);
+
+			// One down... (n-1) to go...
+			*n -= 1 << order;
+			if (*n <= 0) {
+				return 0;
+			}
+		}
+	}
+
+	// If we get here, we completed both loops without zeroing n pages.
+	// This could be because the zone doesn't have n pages or because all
+	// of the freelists we tried are already zeroed. If they are already
+	// zeroed, we want to return a distinct exit code so that we don't
+	// waste time continue to zero.
+	if (zeroed_something)
+		return -1; // ran out of pages to zero
+	else
+		return -2; // everything was already zeroed
+}
+
+static void zero_n_pages(int n)
+{
+	// STATIC: Keep track of where we left off...
+	static struct zone *current_zone = NULL;
+
+	int ret;
+	bool all_zeroed = false;
+
+	if (current_zone == NULL)
+		current_zone = (first_online_pgdat())->node_zones;
+
+	while (true) {
+		// starts from wherever we left off last time...
+		while(current_zone) {
+			if (!populated_zone(current_zone) || skip_zone(current_zone)) {
+				current_zone = next_zone(current_zone);
+				continue;
+			}
+
+			ret = zero_fill_zone_pages(current_zone, &n);
+
+			switch (ret) {
+				case -2:
+					all_zeroed = true;
+					break;
+				case 0:
+					all_zeroed = false;
+					break;
+
+				case -1:
+					break;
+
+				default:
+					BUG();
+			}
+
+			// If we have zeroed enough, exit for now.
+			if (n <= 0) return;
+
+			current_zone = next_zone(current_zone);
+		}
+
+		// restart from the beginning next time.
+		current_zone = (first_online_pgdat())->node_zones;
+
+		// If this is true it is likely all zones are zeroed (it
+		// could be just the last zone, though... best effort).
+		if (all_zeroed) return;
+	}
+}
+
+static int asynczero_do_work(void *data)
+{
+	while (!asynczero_should_stop) {
+		// We just woke up. Check the cost-benefit of doing another iteration.
+		struct mm_cost_delta mm_cost_delta;
+		struct mm_action mm_action = {
+			.action = MM_ACTION_RUN_PREZEROING,
+			.prezero_n = count,
+		};
+		bool should_run;
+
+		if (mm_econ_is_on() && mode == 0) {
+			mm_estimate_changes(&mm_action, &mm_cost_delta);
+			should_run = mm_decide(&mm_cost_delta);
+		} else {
+			should_run = true;
+		}
+
+		// If worth it, zero some pages.
+		if (should_run) zero_n_pages(count);
+		msleep(10); // 10ms... short, allows fine-grained decisions
+
+		// Yield CPU.
+		if (mm_econ_is_on() && mode == 0)
+			cond_resched();
+		else
+			msleep(sleep - 10);
+	}
+
+	return 0;
+}
+
+int init_module(void)
+{
+	int err;
+
+	asynczero_should_stop = false;
+	asynczero_task = kthread_run(asynczero_do_work, NULL, "kasynczerod");
+
+	if (IS_ERR(asynczero_task)) {
+		err = PTR_ERR(asynczero_task);
+		asynczero_task = NULL;
+		return err;
+	}
+
+	return 0;
+}
+
+void cleanup_module(void)
+{
+	if (asynczero_task) {
+		asynczero_should_stop = true;
+		kthread_stop(asynczero_task);
+	}
+
+	printk(KERN_INFO"asynczero: exiting\n");
+}
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Ashish Panwar");
+MODULE_AUTHOR("Mark Mansi");
diff --git a/mm/badger_trap.c b/mm/badger_trap.c
new file mode 100644
index 000000000000..cd6b22cd9705
--- /dev/null
+++ b/mm/badger_trap.c
@@ -0,0 +1,437 @@
+#include <asm/pgalloc.h>
+#include <asm/uaccess.h>
+#include <asm/pgtable.h>
+#include <linux/badger_trap.h>
+#include <linux/syscalls.h>
+#include <linux/hugetlb.h>
+#include <linux/kernel.h>
+#include <linux/pagewalk.h>
+#include <linux/sched/mm.h>
+
+char badger_trap_process[CONFIG_NR_CPUS][MAX_NAME_LEN];
+
+static bool silent = false;
+
+void silence(void) {
+	silent = true;
+}
+EXPORT_SYMBOL(silence);
+
+/*
+ * This syscall is generic way of setting up badger trap.
+ * There are three options to start badger trap.
+ * (1) 	option > 0: provide all process names with number of processes.
+ * 	This will mark the process names for badger trap to start when any
+ * 	process with names specified will start.
+ *
+ * (2) 	option == 0: starts badger trap for the process calling the syscall itself.
+ *  	This requires binary to be updated for the workload to call badger trap. This
+ *  	option is useful when you want to skip the warmup phase of the program. You can
+ *  	introduce the syscall in the program to invoke badger trap after that phase.
+ *
+ * (3) 	option < 0: provide all pid with number of processes. This will start badger
+ *  	trap for all pids provided immidiately.
+ *
+ *  Note: 	(1) will allow all the child processes to be marked for badger trap when
+ *  		forked from a badger trap process.
+
+ *		(2) and (3) will not mark the already spawned child processes for badger
+ *		trap when you mark the parent process for badger trap on the fly. But (2) and (3)
+ *		will mark all child spwaned from the parent process adter being marked for badger trap.
+ */
+SYSCALL_DEFINE3(init_badger_trap,
+		const char __user**, process_name_user,
+		unsigned long, num_procs, int, option)
+{
+	unsigned int i;
+	char *temp;
+	unsigned long ret = 0;
+	char **process_name = NULL;
+	char proc[MAX_NAME_LEN];
+	struct task_struct * tsk;
+	unsigned long pid;
+
+	process_name = vmalloc(sizeof(char*) * num_procs);
+	if (!process_name) {
+		return -ENOMEM;
+	}
+
+	ret = copy_from_user(process_name, process_name_user, sizeof(char*) * num_procs);
+	if (ret) {
+		return ret;
+	}
+
+	pr_warn("init_badger_trap %p %lu %d", process_name, num_procs, option);
+
+	if(option > 0)
+	{
+		for(i = 0; i < CONFIG_NR_CPUS; i++)
+		{
+			if(i<num_procs) {
+				pr_warn("copy from user name=%p", process_name[i]);
+				ret = strncpy_from_user(proc, process_name[i], MAX_NAME_LEN);
+				pr_warn("copy from user name=%s", proc);
+			} else
+				temp = strncpy(proc, "", MAX_NAME_LEN);
+			temp = strncpy(badger_trap_process[i], proc, MAX_NAME_LEN-1);
+		}
+	}
+	else if(option == 0)
+	{
+		badger_trap_walk(current->mm, 0, ~0ull, true);
+	}
+	else if(option < 0)
+	{
+		for(i = 0; i < CONFIG_NR_CPUS; i++)
+		{
+			if(i < num_procs)
+			{
+				ret = kstrtoul(process_name[i], 10, &pid);
+				if(ret == 0)
+				{
+					tsk = find_task_by_vpid(pid);
+					badger_trap_walk(tsk->mm, 0, ~0ull, true);
+				}
+			}
+		}
+	}
+
+	return 0;
+}
+
+/*
+ * This function checks whether a process name provided matches from the list
+ * of process names stored to be marked for badger trap.
+ */
+bool is_badger_trap_process(const char* proc_name)
+{
+	unsigned int i;
+	for(i = 0; i < CONFIG_NR_CPUS; i++)
+	{
+		if(!strncmp(proc_name, badger_trap_process[i], MAX_NAME_LEN)) {
+			pr_warn("Badger Trap process (%s).", proc_name);
+			return true;
+		}
+	}
+	//pr_info("NOT Badger Trap process (%s).", proc_name);
+	return false;
+}
+
+/*
+ * This function checks whether a process name provided matches from the list
+ * of process names stored to be marked for badger trap.
+ */
+bool is_badger_trap_enabled(const struct mm_struct *mm, u64 address)
+{
+	if (!mm)
+		return false;
+
+	if (!mm->badger_trap_enabled)
+		return false;
+
+	if (address < mm->badger_trap_start)
+		return false;
+
+	if (mm->badger_trap_end < address)
+		return false;
+
+	return true;
+}
+
+/*
+ * Helper functions to manipulate all the TLB entries for reservation.
+ */
+inline pte_t pte_mkreserve(pte_t pte)
+{
+        return pte_set_flags(pte, _PAGE_RESERVED);
+}
+
+inline pte_t pte_unreserve(pte_t pte)
+{
+        return pte_clear_flags(pte, _PAGE_RESERVED);
+}
+
+inline int is_pte_reserved(pte_t pte)
+{
+        if(native_pte_val(pte) & _PAGE_RESERVED)
+                return 1;
+        else
+                return 0;
+}
+
+inline pmd_t pmd_mkreserve(pmd_t pmd)
+{
+        return pmd_set_flags(pmd, _PAGE_RESERVED);
+}
+
+inline pmd_t pmd_unreserve(pmd_t pmd)
+{
+        return pmd_clear_flags(pmd, _PAGE_RESERVED);
+}
+
+inline int is_pmd_reserved(pmd_t pmd)
+{
+        if(native_pmd_val(pmd) & _PAGE_RESERVED)
+                return 1;
+        else
+                return 0;
+}
+
+inline pud_t pud_mkreserve(pud_t pud)
+{
+        return pud_set_flags(pud, _PAGE_RESERVED);
+}
+
+inline pud_t pud_unreserve(pud_t pud)
+{
+        return pud_clear_flags(pud, _PAGE_RESERVED);
+}
+
+inline int is_pud_reserved(pud_t pud)
+{
+        if(native_pud_val(pud) & _PAGE_RESERVED)
+                return 1;
+        else
+                return 0;
+}
+
+static int bt_init_pud(pud_t *pud, unsigned long addr,
+	 unsigned long next, struct mm_walk *walk)
+{
+	if (pud_none(*pud) || !pud_present(*pud))
+		return 0;
+
+	if (!(pud_flags(*pud) & _PAGE_USER))
+		return 0;
+
+	//pr_warn("mm=%p vma=%p addr=%lx pud=%p\n", walk->mm, walk->vma, addr, pud);
+	// We can only get huge puds here.
+	if (*(bool*)walk->private) {
+		*pud = pud_mkreserve(*pud);
+	} else {
+		*pud = pud_unreserve(*pud);
+	}
+
+	return 0;
+}
+
+static int bt_init_pmd(pmd_t *pmd, unsigned long addr,
+	 unsigned long next, struct mm_walk *walk)
+{
+	// We get normal as well as huge pmds here.
+	if (pmd_none(*pmd) || !pmd_present(*pmd))
+		return 0;
+
+	if (!(pmd_flags(*pmd) & _PAGE_USER))
+		return 0;
+
+	if (pmd_trans_huge(*pmd)) {
+		if (*(bool*)walk->private) {
+			//pr_warn("mm=%p vma=%p addr=%lx pmd=%p\n", walk->mm, walk->vma, addr, pmd);
+			*pmd = pmd_mkreserve(*pmd);
+		} else {
+			*pmd = pmd_unreserve(*pmd);
+		}
+	}
+
+	return 0;
+}
+static int bt_init_pte(pte_t *pte, unsigned long addr,
+	 unsigned long next, struct mm_walk *walk)
+{
+	if (pte_none(*pte) || !pte_present(*pte))
+		return 0;
+
+	if (!(pte_flags(*pte) & _PAGE_USER))
+		return 0;
+
+	if (*(bool*)walk->private) {
+		//pr_warn("mm=%p vma=%p addr=%lx ptep=%p pte=%lx\n",
+		//		walk->mm, walk->vma, addr, pte, pte_val(*pte));
+		*pte = pte_mkreserve(*pte);
+	} else {
+		*pte = pte_unreserve(*pte);
+	}
+
+	return 0;
+}
+
+static int bt_init_hugetlb_entry(pte_t *ptep, unsigned long hmask,
+	     unsigned long addr, unsigned long next,
+	     struct mm_walk *walk)
+{
+	pte_t pte = huge_ptep_get(ptep);
+
+	if (pte_none(pte) || !pte_present(pte))
+		return 0;
+
+	if (!(pte_flags(pte) & _PAGE_USER))
+		return 0;
+
+	/*
+	if (*(bool*)walk->private) {
+		*pte = pte_mkreserve(*pte);
+	} else {
+		*pte = pte_unreserve(*pte);
+	}
+	*/
+	set_huge_pte_at(walk->mm, addr, ptep, pte);
+
+	return 0;
+}
+
+static int bt_init_test_walk(unsigned long addr, unsigned long next,
+	struct mm_walk *walk)
+{
+	//pr_warn("test_walk(addr=%lx, next=%lx, mm=%p, vma=%p is_exec=%d may_exec=%d is_anon=%d\n",
+	//		addr, next, walk->mm, walk->vma,
+	//		walk->vma && (walk->vma->vm_flags & VM_EXEC),
+	//		walk->vma && (walk->vma->vm_flags & VM_MAYEXEC),
+	//		walk->vma && vma_is_anonymous(walk->vma));
+
+	// Skip unmapped regions
+	if (!walk->vma)
+		return 1;
+
+	// Skip executable regions, since we don't handle instruction TLB misses.
+	if (walk->vma->vm_flags & VM_EXEC)
+		return 1;
+
+	return 0;
+}
+
+void badger_trap_set_stats_loc(struct mm_struct *mm, struct badger_trap_stats *stats)
+{
+	BUG_ON(!mm);
+	if (stats)
+		mm->bt_stats = stats;
+	else
+		mm->bt_stats = &mm->bt_stats_inner;
+}
+EXPORT_SYMBOL(badger_trap_set_stats_loc);
+
+/*
+ * This function walks the page tables of the given mm_struct for pages mapped
+ * between the given lower and upper addresses (inclusive). Depending on the
+ * value of init, we either set or clear the _PAGE_RESERVED bit in all relevant
+ * page table entries (init == true => set; init == false => clear).
+ *
+ * This function takes care of transparent hugepages and hugepages in general.
+ *
+ * NOTE: The upper and lower boundaries are rounded to the up and down,
+ * respectively, to the nearest 2MB boundaries. This makes it easier to deal
+ * with huge pages being formed or broken up.
+ *
+ * NOTE: This function acquires and releases mmap_sem.
+ *
+ * NOTE: If `init == false`, there MUST have been a prior call with `init ==
+ * true` first for the same `mm`. Otherwise, there will be a double free.
+ */
+void badger_trap_walk(struct mm_struct *mm, u64 lower, u64 upper, bool init)
+{
+	// markm: see comments in <linux/pagewalk.h>
+	const struct mm_walk_ops ops = {
+		.pud_entry = bt_init_pud,
+		.pmd_entry = bt_init_pmd,
+		.pte_entry = bt_init_pte,
+		.hugetlb_entry = bt_init_hugetlb_entry,
+		.test_walk = bt_init_test_walk,
+	};
+	int ret;
+	u64 upper_rounded;
+
+	BUG_ON(!mm);
+	BUG_ON(lower >= upper);
+
+	// Grab when turning on BadgerTrap, and don't release until BadgerTrap
+	// is turned off. Under the assumption that there is at least one call
+	// with `init == true` for each call with `init == false`, the worse
+	// that can happen is a memory leak.
+	if (init) {
+		mmgrab(mm);
+	}
+
+	down_write(&mm->mmap_sem);
+
+	// When initializing, we want to set these first. If deinitializing, we
+	// set them after walking.
+	if (init) {
+		// Round down to hpage boundary.
+		mm->badger_trap_start = lower & HPAGE_PMD_MASK;
+		// Round up to hpage boundary, but subtract 1 to make it inclusive.
+		mm->badger_trap_end = ((upper - 1) & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE - 1;
+		mm->badger_trap_enabled = true;
+		mm->badger_trap_was_enabled = true;
+		badger_trap_stats_clear(mm->bt_stats);
+	}
+
+	// Block any other page faults from changing the mappings while we walk.
+	//
+	// TODO: probably also need to guard migration, thp promotion/demotion,
+	// mmap, mprotect, mlock...
+	down_write(&mm->badger_trap_page_table_sem);
+
+	pr_warn("BadgerTrap: walk(%llx, %llx) init = %d [%llx, %llx]\n",
+			lower, upper, init, mm->badger_trap_start,
+			mm->badger_trap_end);
+
+	// upper is inclusive of the end point, whereas walk_page_range expects
+	// an exclusive endpoint. But we need to be careful of overflow.
+	upper_rounded = upper == ~0ull ?
+				upper & (PAGE_SIZE - 1) :
+				upper + 1;
+
+	ret = walk_page_range(mm, lower, upper_rounded, &ops, &init);
+	if (ret != 0) {
+		pr_err("BadgerTrap: walk_page_range returned %d\n", ret);
+		BUG();
+	}
+
+	up_write(&mm->badger_trap_page_table_sem);
+
+	if (!init) {
+		mm->badger_trap_enabled = false;
+	}
+
+	up_write(&mm->mmap_sem);
+
+	if (!init) {
+		mmdrop(mm);
+	}
+}
+EXPORT_SYMBOL(badger_trap_walk);
+
+void print_badger_trap_stats(const struct mm_struct *mm) {
+	//struct vm_area_struct *vma;
+
+	if (silent) return;
+
+	pr_warn("===================================\n");
+	pr_warn("BadgerTrap: Statistics for Process %s\n",
+			mm->owner ? mm->owner->comm : "<unknown process>");
+	pr_warn("BadgerTrap: DTLB load miss for 4KB page detected %llu\n",
+			atomic64_read_acquire(&mm->bt_stats->total_dtlb_4kb_load_misses));
+	pr_warn("BadgerTrap: DTLB load miss for 2MB page detected %llu\n",
+			atomic64_read_acquire(&mm->bt_stats->total_dtlb_2mb_load_misses));
+	pr_warn("BadgerTrap: DTLB store miss for 4KB page detected %llu\n",
+			atomic64_read_acquire(&mm->bt_stats->total_dtlb_4kb_store_misses));
+	pr_warn("BadgerTrap: DTLB store miss for 2MB page detected %llu\n",
+			atomic64_read_acquire(&mm->bt_stats->total_dtlb_2mb_store_misses));
+	/*
+	pr_warn("-----------------------------------\n");
+	for (vma = mm->mmap; vma; vma = vma->vm_next) {
+		pr_warn("BadgerTrap: [%lx, %lx]\n", vma->vm_start, vma->vm_end);
+		pr_warn("BadgerTrap: DTLB load miss for 4KB page detected %llu\n",
+				vma->bt_stats.total_dtlb_4kb_load_misses);
+		pr_warn("BadgerTrap: DTLB load miss for 2MB page detected %llu\n",
+				vma->bt_stats.total_dtlb_2mb_load_misses);
+		pr_warn("BadgerTrap: DTLB store miss for 4KB page detected %llu\n",
+				vma->bt_stats.total_dtlb_4kb_store_misses);
+		pr_warn("BadgerTrap: DTLB store miss for 2MB page detected %llu\n",
+				vma->bt_stats.total_dtlb_2mb_store_misses);
+	}
+	*/
+	pr_warn("BadgerTrap: END Statistics\n");
+	pr_warn("===================================\n");
+}
+EXPORT_SYMBOL(print_badger_trap_stats);
diff --git a/mm/compaction.c b/mm/compaction.c
index 672d3c78c6ab..dbb820301308 100644
--- a/mm/compaction.c
+++ b/mm/compaction.c
@@ -23,6 +23,7 @@
 #include <linux/freezer.h>
 #include <linux/page_owner.h>
 #include <linux/psi.h>
+#include <linux/mm_stats.h>
 #include "internal.h"
 
 #ifdef CONFIG_COMPACTION
@@ -2629,6 +2630,7 @@ static int kcompactd(void *p)
 {
 	pg_data_t *pgdat = (pg_data_t*)p;
 	struct task_struct *tsk = current;
+    u64 start;
 
 	const struct cpumask *cpumask = cpumask_of_node(pgdat->node_id);
 
@@ -2648,7 +2650,9 @@ static int kcompactd(void *p)
 				kcompactd_work_requested(pgdat));
 
 		psi_memstall_enter(&pflags);
+        start = rdtsc();
 		kcompactd_do_work(pgdat);
+        mm_stats_hist_measure(&mm_indirect_compaction_cycles, rdtsc() - start);
 		psi_memstall_leave(&pflags);
 	}
 
diff --git a/mm/estimator.c b/mm/estimator.c
new file mode 100644
index 000000000000..e6c5c567b335
--- /dev/null
+++ b/mm/estimator.c
@@ -0,0 +1,2027 @@
+/*
+ * Implementation of cost-benefit based memory management.
+ */
+
+#include <linux/printk.h>
+#include <linux/mm_econ.h>
+#include <linux/mm.h>
+#include <linux/kobject.h>
+#include <linux/init.h>
+#include <linux/hashtable.h>
+#include <linux/mm_stats.h>
+#include <linux/sched/loadavg.h>
+#include <linux/sched/task.h>
+#include <linux/rwsem.h>
+
+#define HUGE_PAGE_ORDER 9
+
+#define MMAP_FILTER_BUF_SIZE 4096
+#define MMAP_FILTER_BUF_DEAD_ZONE 128
+
+///////////////////////////////////////////////////////////////////////////////
+// Globals...
+
+// Modes:
+// - 0: off (just use default linux behavior)
+// - 1: on (cost-benefit estimation)
+static int mm_econ_mode = 0;
+
+// Turns on various debugging printks...
+int mm_econ_debugging_mode = 0;
+
+// Number of cycles per unit time page allocator zone lock is NOT held.
+// In this case, the unit time is 10ms because that is the granularity async
+// zero daemon uses.
+static u64 mm_econ_contention_ms = 10;
+
+// Set this properly via the sysfs file.
+static u64 mm_econ_freq_mhz = 3000;
+
+// The Preloaded Profile, if any.
+struct profile_range {
+    u64 start;
+    u64 end;
+    // The benefit depends on what the profile is measuring
+    u64 benefit;
+
+    struct rb_node node;
+};
+
+// The policy the filter applies to
+enum mm_policy {
+    PolicyHugePage,
+    PolicyEagerPage
+};
+
+// The operator to use when deciding if quantity from an mmap matches
+// the filter.
+enum mmap_comparator {
+    CompEquals,
+    CompGreaterThan,
+    CompLessThan
+};
+
+// The different quantities that can be compared in an mmap
+enum mmap_quantity {
+    QuantSectionOff,
+    QuantAddr,
+    QuantLen,
+    QuantProt,
+    QuantFlags,
+    QuantFD,
+    QuantOff
+};
+
+// A comaprison for filtering an mmap with and how to compare the quantity
+struct mmap_comparison {
+    struct list_head node;
+    enum mmap_quantity quant;
+    enum mmap_comparator comp;
+    u64 val;
+};
+
+// A list of quantities of a mmap to use for deciding if that mmap would
+// benefit from being huge.
+struct mmap_filter {
+    struct list_head node;
+    enum mm_memory_section section;
+    enum mm_policy policy;
+    u64 benefit;
+    struct list_head comparisons;
+};
+
+// A process using mmap filters
+struct mmap_filter_proc {
+    struct list_head node;
+    pid_t pid;
+    struct list_head filters;
+    struct rb_root hp_ranges_root;
+    struct rb_root eager_ranges_root;
+};
+
+// List of processes using mmap filters
+static LIST_HEAD(filter_procs);
+static DECLARE_RWSEM(filter_procs_sem);
+
+// The TLB misses estimator, if any.
+static mm_econ_tlb_miss_estimator_fn_t tlb_miss_est_fn = NULL;
+
+// Some stats...
+
+// Number of estimates made.
+static u64 mm_econ_num_estimates = 0;
+// Number of decisions made.
+static u64 mm_econ_num_decisions = 0;
+// Number of decisions that are "yes".
+static u64 mm_econ_num_decisions_yes = 0;
+// Number of huge page promotions in #PFs.
+static u64 mm_econ_num_hp_promotions = 0;
+// Number of times we decided to run async compaction.
+static u64 mm_econ_num_async_compaction = 0;
+// Number of times we decided to run async prezeroing.
+static u64 mm_econ_num_async_prezeroing = 0;
+// Number of allocated bytes for various data structures.
+static u64 mm_econ_vmalloc_bytes = 0;
+
+extern inline struct task_struct *extern_get_proc_task(const struct inode *inode);
+
+///////////////////////////////////////////////////////////////////////////////
+// Actual implementation
+//
+// There are two possible estimators:
+// 1. kbadgerd (via tlb_miss_est_fn).
+// 2. A pre-loaded profile (via preloaded_profile).
+//
+// In both cases, the required units are misses/huge-page/LTU.
+
+// A wrapper around vmalloc to keep track of allocated memory.
+static void *mm_econ_vmalloc(unsigned long size)
+{
+    void *mem = vmalloc(size);
+
+    if (mem)
+        mm_econ_vmalloc_bytes += size;
+
+    return mem;
+}
+
+static void mm_econ_vfree(const void *addr, unsigned long size)
+{
+    mm_econ_vmalloc_bytes -= size;
+    vfree(addr);
+}
+
+void register_mm_econ_tlb_miss_estimator(
+        mm_econ_tlb_miss_estimator_fn_t f)
+{
+    BUG_ON(!f);
+    tlb_miss_est_fn = f;
+    pr_warn("mm: registered TLB miss estimator %p\n", f);
+}
+EXPORT_SYMBOL(register_mm_econ_tlb_miss_estimator);
+
+/*
+ * Find the profile of a process by PID, if any.
+ *
+ * Caller must hold `filter_procs_sem` in either read or write mode.
+ */
+static struct mmap_filter_proc *
+find_filter_proc_by_pid(pid_t pid)
+{
+    struct mmap_filter_proc *proc;
+    list_for_each_entry(proc, &filter_procs, node) {
+        if (proc->pid == pid) {
+            return proc;
+        }
+    }
+
+    return NULL;
+}
+
+inline bool mm_process_is_using_cbmm(pid_t pid)
+{
+    return find_filter_proc_by_pid(pid) != NULL;
+}
+
+/*
+ * Search the profile for the range containing the given address, and return
+ * it. Otherwise, return NULL.
+ */
+static struct profile_range *
+profile_search(struct rb_root *ranges_root, u64 addr)
+{
+    struct rb_node *node = ranges_root->rb_node;
+
+    while (node) {
+        struct profile_range *range =
+            container_of(node, struct profile_range, node);
+
+        if (range->start <= addr && addr < range->end)
+            return range;
+
+        if (addr < range->start)
+            node = node->rb_left;
+        else
+            node = node->rb_right;
+    }
+
+    return NULL;
+}
+
+/*
+ * Search the tree for the first range that satisfies the condition
+ * of "there exists some address x in range s.t. x <comp> addr."
+ * This is only used for filter comparisons on the section_off quantity
+ */
+static struct profile_range *
+profile_find_first_range(struct rb_root *ranges_root, u64 addr,
+        enum mmap_comparator comp)
+{
+    struct profile_range *result = NULL;
+    struct profile_range *range = NULL;
+    struct rb_node *node = ranges_root->rb_node;
+
+    // First find any range that satisfies the condition
+    while (node) {
+        range = container_of(node, struct profile_range, node);
+
+        if (comp == CompLessThan) {
+            if (range->start < addr) {
+                result = range;
+                break;
+            } else {
+                node = node->rb_left;
+            }
+        } else if (comp == CompGreaterThan) {
+            if (range->end > addr) {
+                result = range;
+                break;
+            } else {
+                node = node->rb_right;
+            }
+        } else if (comp == CompEquals) {
+            if (range->start <= addr && addr < range->end) {
+                // Since only ranges do not overlap, we just need
+                // to find one range that overlaps with addr
+                return range;
+            } else if (range->start < addr) {
+                node = node->rb_right;
+            } else {
+                node = node->rb_left;
+            }
+        } else {
+            return NULL;
+        }
+    }
+
+    if (!node)
+        return NULL;
+
+    while (node) {
+        range = container_of(node, struct profile_range, node);
+
+        if (comp == CompLessThan) {
+            if (range->start >= addr)
+                break;
+
+            result = range;
+            node = rb_next(node);
+        } else if (comp == CompGreaterThan) {
+            if (range->end <= addr)
+                break;
+
+            result = range;
+            node = rb_prev(node);
+        }
+    }
+
+    return result;
+}
+
+static inline bool
+ranges_overlap(struct profile_range *r1, struct profile_range *r2)
+{
+    return (((r1->start <= r2->start && r2->start < r1->end)
+        || (r2->start <= r1->start && r1->start < r2->end)));
+}
+
+/*
+ * Remove all ranges overlapping with the new range
+ */
+static void remove_overlapping_ranges(struct rb_root *ranges_root,
+    struct profile_range *new_range)
+{
+    struct rb_node *node = ranges_root->rb_node;
+    struct rb_node *first_overlapping = NULL;
+    struct rb_node *next;
+    struct profile_range *cur_range;
+
+    // First, find the earliest range that overlaps with the new range, if there is any
+    while (node) {
+        cur_range = container_of(node, struct profile_range, node);
+
+
+        if (ranges_overlap(new_range, cur_range)) {
+            first_overlapping = node;
+            // We've found one node that overlaps, but keep going to see if we
+            // can find an earlier one
+            node = node->rb_left;
+            continue;
+        }
+
+        if (new_range->start < cur_range->start)
+            node = node->rb_left;
+        else
+            node = node->rb_right;
+    }
+
+    // If no overlapping range exists, we're done
+    if (!first_overlapping)
+        return;
+
+    // Now we can delete all of the overlapping ranges
+    node = first_overlapping;
+    next = rb_next(node);
+    cur_range = container_of(node, struct profile_range, node);
+    while (ranges_overlap(new_range, cur_range)) {
+        rb_erase(node, ranges_root);
+        mm_econ_vfree(cur_range, sizeof(struct profile_range));
+
+        if (!next)
+            break;
+
+        node = next;
+        next = rb_next(node);
+        cur_range = container_of(node, struct profile_range, node);
+    }
+}
+
+/*
+ * Insert the given range into the profile.
+ * If the new range overlaps with any existing ranges, delete the
+ * existing ones as must have been unmapped.
+ */
+static void
+profile_range_insert(struct rb_root *ranges_root, struct profile_range *new_range)
+{
+    struct rb_node **new = &(ranges_root->rb_node), *parent = NULL;
+
+    remove_overlapping_ranges(ranges_root, new_range);
+
+    while (*new) {
+        struct profile_range *this =
+            container_of(*new, struct profile_range, node);
+
+        parent = *new;
+        if (new_range->start < this->start)
+            new = &((*new)->rb_left);
+        else if (new_range->start > this->start)
+            new = &((*new)->rb_right);
+        else
+            break;
+    }
+
+    rb_link_node(&new_range->node, parent, new);
+    rb_insert_color(&new_range->node, ranges_root);
+}
+
+/*
+ * Move the ranges in one rb_tree to another
+ */
+static void
+profile_move(struct rb_root *src, struct rb_root *dst)
+{
+    struct profile_range *range;
+    struct rb_node *node = src->rb_node;
+
+    while (node) {
+        range = container_of(node, struct profile_range, node);
+
+        // Remove the entry from the source
+        rb_erase(node, src);
+
+        // Add the entry to the destination
+        profile_range_insert(dst, range);
+
+        node = src->rb_node;
+    }
+}
+
+static void
+profile_free_all(struct rb_root *ranges_root)
+{
+    struct rb_node *node = ranges_root->rb_node;
+
+    while(node) {
+        struct profile_range *range =
+            container_of(node, struct profile_range, node);
+
+        rb_erase(node, ranges_root);
+        node = ranges_root->rb_node;
+
+        mm_econ_vfree(range, sizeof(struct profile_range));
+    }
+}
+
+static void mmap_filters_free_all(struct mmap_filter_proc *proc)
+{
+    struct mmap_filter *filter;
+    struct mmap_comparison *comparison;
+    struct list_head *pos, *n;
+    struct list_head *cPos, *cN;
+
+    list_for_each_safe(pos, n, &proc->filters) {
+        filter = list_entry(pos, struct mmap_filter, node);
+
+        // Free each comparison in this filter
+        list_for_each_safe(cPos, cN, &filter->comparisons) {
+            comparison = list_entry(cPos, struct mmap_comparison, node);
+            list_del(cPos);
+            mm_econ_vfree(comparison, sizeof(struct mmap_comparison));
+        }
+
+        list_del(pos);
+        mm_econ_vfree(filter, sizeof(struct mmap_filter));
+    }
+}
+
+enum free_huge_page_status {
+    fhps_none, // no free huge pages
+    fhps_free, // huge pages are available
+    fhps_zeroed, // huge pages are available and prezeroed!
+};
+
+static enum free_huge_page_status
+have_free_huge_pages(void)
+{
+    int zone_idx;
+    struct zone *zone;
+    struct page *page;
+    struct free_area *area;
+    bool is_free = false, is_zeroed = false;
+    int order;
+    unsigned long flags;
+
+    pg_data_t *pgdat = NODE_DATA(numa_node_id());
+    for (zone_idx = ZONE_NORMAL; zone_idx < MAX_NR_ZONES; zone_idx++) {
+        zone = &pgdat->node_zones[zone_idx];
+
+        for (order = HUGE_PAGE_ORDER; order < MAX_ORDER; ++order) {
+            area = &(zone->free_area[order]);
+            is_free = area->nr_free > 0;
+
+            if (is_free) {
+                spin_lock_irqsave(&zone->lock, flags);
+
+                page = list_last_entry_or_null(
+                        &area->free_list[MIGRATE_MOVABLE], struct page,
+                        lru);
+                is_zeroed = page && PageZeroed(page);
+
+                spin_unlock_irqrestore(&zone->lock, flags);
+
+                if (mm_econ_debugging_mode == 1) {
+                    pr_warn("estimator: found "
+                            "free page %p node %d zone %p (%s) "
+                            "order %d prezeroed %d list %d",
+                            page, zone->zone_pgdat->node_id,
+                            zone, zone->name, order,
+                            is_zeroed, MIGRATE_MOVABLE);
+                }
+
+                goto exit;
+            }
+        }
+    }
+
+exit:
+    return is_zeroed ? fhps_zeroed :
+        is_free ? fhps_free :
+        fhps_none;
+}
+
+static u64
+compute_hpage_benefit_from_profile(
+        const struct mm_action *action)
+{
+    u64 ret = 0;
+    struct mmap_filter_proc *proc;
+    struct profile_range *range = NULL;
+
+    down_read(&filter_procs_sem);
+    if ((proc = find_filter_proc_by_pid(current->tgid))) // NOTE: assignment
+        range = profile_search(&proc->hp_ranges_root, action->address);
+
+    if (range) {
+        ret = range->benefit;
+
+        //pr_warn("mm_econ: estimating page benefit: "
+        //        "misses=%llu size=%llu per-page=%llu\n",
+        //        range->benefit,
+        //        (range->end - range->start) >> HPAGE_SHIFT,
+        //        ret);
+    }
+    up_read(&filter_procs_sem);
+
+    return ret;
+}
+
+static u64
+compute_hpage_benefit(const struct mm_action *action)
+{
+    if (tlb_miss_est_fn)
+        return tlb_miss_est_fn(action);
+    else
+        return compute_hpage_benefit_from_profile(action);
+}
+
+static void
+compute_eager_page_benefit(const struct mm_action *action, struct mm_cost_delta *cost)
+{
+    u64 benefit = 0;
+    u64 start, end;
+    int range_count = 0;
+    struct mmap_filter_proc *proc;
+    struct profile_range *first_range = NULL;
+    struct profile_range *range = NULL;
+    struct rb_node *node = NULL;
+    struct range *ranges = NULL;
+
+    start = action->address;
+    end = action->address + action->len;
+
+    down_read(&filter_procs_sem);
+    // First find the first range with an address g.t. the given address
+    if ((proc = find_filter_proc_by_pid(current->tgid))) { // NOTE: assignment
+        first_range = profile_find_first_range(&proc->eager_ranges_root,
+            start, CompGreaterThan);
+    }
+    if (!first_range)
+        goto out;
+
+    node = &first_range->node;
+
+    // Count up all the ranges that have greater benefit than cost so we
+    // know how big of an array to allocate later
+    while (node) {
+        range = container_of(node, struct profile_range, node);
+
+        if (start >= range->end || end <= range->start)
+            break;
+
+        if (range->benefit > cost->cost)
+            range_count++;
+
+        node = rb_next(node);
+    }
+
+    if (range_count == 0)
+        goto out;
+
+    // +1 for the ending signal
+    cost->extra = 0;
+    ranges = vmalloc(sizeof(struct range) * (range_count + 1));
+    if (!ranges)
+        goto out;
+
+    // Fill in the list of ranges to page in
+    range_count = 0;
+    node = &first_range->node;
+    while (node) {
+        range = container_of(node, struct profile_range, node);
+
+        if (start >= range->end || end <= range->start)
+            break;
+
+        if (range->benefit > cost->cost) {
+            ranges[range_count].start = range->start;
+            ranges[range_count].end = range->end;
+
+            if (range->benefit > benefit)
+                benefit = range->benefit;
+
+            range_count++;
+        }
+
+        node = rb_next(node);
+    }
+    // Hacky: Just use -1 in the start and end to signal the end of the list
+    ranges[range_count].start = ranges[range_count].end = -1;
+
+    // Pass the list of ranges to promote to the decider in the extra field
+    cost->extra = (u64)ranges;
+out:
+    up_read(&filter_procs_sem);
+
+    cost->benefit = benefit;
+}
+
+// Estimate cost/benefit of a huge page promotion for the current process.
+void
+mm_estimate_huge_page_promote_cost_benefit(
+       const struct mm_action *action, struct mm_cost_delta *cost)
+{
+    // Estimated cost.
+    //
+    // For now, we hard-code a bunch of stuff, and we make a lot of
+    // assumptions. We can relax these assumptions later if we need to.
+
+    // TODO: Assume allocation is free if we have free huge pages.
+    // TODO: Assume we don't care what node it is on...
+    // TODO: Maybe account for opportunity cost as rate/ratio?
+    const enum free_huge_page_status fhps = have_free_huge_pages();
+    const u64 alloc_cost = fhps > fhps_none ? 0 : (1ul << 32);
+
+    // TODO: Assume constant prep costs (zeroing or copying).
+    const u64 prep_cost = fhps > fhps_free ? 0 : 100 * 2000; // ~100us
+
+    // Compute total cost.
+    cost->cost = alloc_cost + prep_cost;
+    cost->extra = fhps == fhps_zeroed;
+
+    // Estimate benefit.
+    cost->benefit = compute_hpage_benefit(action);
+}
+
+// Update the given cost/benefit to also account for reclamation of a huge
+// page. This assumes that there is already a cost/benefit in `cost`.
+void
+mm_estimate_huge_page_reclaim_cost(
+       const struct mm_action *action, struct mm_cost_delta *cost)
+{
+    // TODO(markm): for now just assume it is very expensive. We might want to
+    // do something more clever later. For example, we can look at the amount
+    // of fragmentation or the amount of free memory. If we are heavily
+    // fragmented and under memory pressure, then reclaim will be expensive.
+    const u64 reclaim_cost = 1000000000; // ~hundreds of ms
+
+    cost->cost += reclaim_cost;
+}
+
+// Estimate the cost of running a daemon. In general, this is just the time
+// that the daemon runs unless the system is idle -- idle time is considered
+// free to consume.
+void
+mm_estimate_daemon_cost(
+       const struct mm_action *action, struct mm_cost_delta *cost)
+{
+    // FIXME(markm): for now we just use the average system load on all cores
+    // because this is easy and cheap. However, we can get something more
+    // precise by looking at the number of currently running tasks on only
+    // local cores or something like that...
+    //
+    // nrunning = 0;
+    // for_each_cpu_and(cpu, cpumask_of_node(node), cpu_online_mask)
+    //   nrunning += cpu_rq(cpu)->nr_running;
+    //
+    // if (nrunning < ncpus_local)
+    //   cost = 0;
+    // else
+    //   cost = time_to_run;
+
+    const u64 huge_page_zeroing_cost = 1000000;
+
+    __kernel_ulong_t loads[3]; /* 1, 5, and 15 minute load averages */
+    int ncpus = num_online_cpus();
+
+    get_avenrun(loads, FIXED_1/200, 0);
+
+    // If we have more cpus than load, running a background daemon is free.
+    // Otherwise, the cost is however many cycles the daemon runs, as this is
+    // time that is taken away from applications.
+    if (ncpus > LOAD_INT(loads[0])) {
+        cost->cost = 0;
+    } else {
+        switch (action->action) {
+            case MM_ACTION_RUN_PREZEROING:
+                cost->cost = huge_page_zeroing_cost * action->prezero_n;
+                break;
+
+            case MM_ACTION_RUN_DEFRAG:
+            case MM_ACTION_RUN_PROMOTION:
+                // TODO(markm): this should be however long the daemon runs
+                // for, which means we need to cap the run time. There are also
+                // costs for copying pages and scanning.
+                //
+                // For now, we just make these really expensive.
+                cost->cost = 1ul << 32; // >1s
+                break;
+
+            default: // Not a daemon...
+                BUG();
+                return;
+        }
+    }
+}
+
+// Estimate the benefit of prezeroing memory based on the rate of usage of
+// zeroed pages so far.
+void mm_estimate_async_prezeroing_benefit(
+       const struct mm_action *action, struct mm_cost_delta *cost)
+{
+    // FIXME(markm): we assume that the cost to zero a 2MB region is about 10^6
+    // cycles. This is based on previous measurements we've made.
+    const u64 zeroing_per_page_cost = 1000000; // cycles
+
+    // TODO: we want to scale down the benefit to 10ms instead of 1 LTU, I think...
+
+    // The maximum amount of benefit is based on the number of pages we
+    // actually zero and actually use. That is, we don't benefit from zeroed
+    // pages that are not used, and we do not benefit from unzeroed pages.
+    //
+    // We will zero no more than `action->prezero_n` pages, and we will use (we
+    // estimate) no more than `recent_used` pages, so the benefit is capped at
+    // the minimum of these. The `recent_used` is the estimated number of pages
+    // used recently.
+    const u64 recent_used = mm_estimated_prezeroed_used();
+
+    cost->benefit = min(action->prezero_n, recent_used) * zeroing_per_page_cost;
+}
+
+// Estimate the cost of lock contention due to prezeroing.
+//
+// During the LTU, we can grab the lock at times when it would otherwise be
+// idle for free. If we assume that the critical section of the async
+// prezeroing is about 150 cycles (to acquire/release and add/remove from
+// linked list), then we get the number of times per LTU we can do prezeroing
+// for free.
+//
+// We can then discount action->prezero_n operations by the number of free
+// items and expense the rest at the cost of the critical section.
+void mm_estimate_async_prezeroing_lock_contention_cost(
+       const struct mm_action *action, struct mm_cost_delta *cost)
+{
+    const u64 critical_section_cost = 150 * 2; // cycles
+    const u64 nfree = mm_econ_contention_ms * mm_econ_freq_mhz * 1000
+                      / critical_section_cost;
+
+    cost->cost += (action->prezero_n > nfree ? action->prezero_n - nfree  : 0)
+                    * critical_section_cost;
+}
+
+// Estimate the cost of eagerly allocating a page
+void mm_estimate_eager_page_cost_benefit(
+        const struct mm_action *action, struct mm_cost_delta *cost)
+{
+    // Based on our measurements of page fault latency, almost all of the base page
+    // faults take less than 10us, so convert that to cycles and use that for the
+    // cost.
+    // We do not have to consider the cost of faulting in a huge page, since that
+    // will be handled by the huge page cost/benefit logic
+    cost->cost = mm_econ_freq_mhz * 10;
+    // Populates cost->benefit and cost->extra
+    compute_eager_page_benefit(action, cost);
+}
+
+bool mm_econ_is_on(void)
+{
+    return mm_econ_mode > 0;
+}
+EXPORT_SYMBOL(mm_econ_is_on);
+
+// Estimates the change in the given metrics under the given action. Updates
+// the given cost struct in place.
+//
+// Note that this is a pure function! It should not keep state regarding to
+// previous queries.
+void
+mm_estimate_changes(const struct mm_action *action, struct mm_cost_delta *cost)
+{
+    switch (action->action) {
+        case MM_ACTION_NONE:
+            cost->cost = 0;
+            cost->benefit = 0;
+            break;
+
+        case MM_ACTION_PROMOTE_HUGE:
+            mm_estimate_huge_page_promote_cost_benefit(action, cost);
+            break;
+
+        case MM_ACTION_DEMOTE_HUGE:
+            // TODO(markm)
+            cost->cost = 0;
+            cost->benefit = 0;
+            break;
+
+        case MM_ACTION_RUN_DEFRAG:
+            mm_estimate_daemon_cost(action, cost);
+            cost->benefit = 0; // TODO(markm)
+            if (cost->cost < cost->benefit)
+                mm_econ_num_async_compaction += 1;
+            break;
+
+        case MM_ACTION_RUN_PROMOTION:
+            mm_estimate_daemon_cost(action, cost);
+            // TODO(markm)
+            cost->benefit = 0;
+            break;
+
+        case MM_ACTION_RUN_PREZEROING:
+            mm_estimate_daemon_cost(action, cost);
+            mm_estimate_async_prezeroing_lock_contention_cost(action, cost);
+            mm_estimate_async_prezeroing_benefit(action, cost);
+            if (cost->cost < cost->benefit)
+                mm_econ_num_async_prezeroing += 1;
+            break;
+
+        case MM_ACTION_ALLOC_RECLAIM: // Alloc reclaim for thp allocation.
+            // Estimate the cost/benefit of the promotion itself.
+            mm_estimate_huge_page_promote_cost_benefit(action, cost);
+            // Update the cost if we also need to do reclaim.
+            mm_estimate_huge_page_reclaim_cost(action, cost);
+            break;
+
+        case MM_ACTION_EAGER_PAGING:
+            mm_estimate_eager_page_cost_benefit(action, cost);
+            break;
+
+        default:
+            printk(KERN_WARNING "Unknown mm_action %d\n", action->action);
+            break;
+    }
+
+    // Record some stats for debugging.
+    mm_econ_num_estimates += 1;
+    mm_stats_hist_measure(&mm_econ_cost, cost->cost);
+    mm_stats_hist_measure(&mm_econ_benefit, cost->benefit);
+
+    if (mm_econ_debugging_mode == 2) {
+        pr_warn("estimator: action=%d cost=%llu benefit=%llu",
+                action->action, cost->cost, cost->benefit);
+    }
+}
+EXPORT_SYMBOL(mm_estimate_changes);
+
+// Decide whether to take an action with the given cost. Returns true if the
+// action associated with `cost` should be TAKEN, and false otherwise.
+bool mm_decide(const struct mm_cost_delta *cost)
+{
+    bool should_do;
+    mm_econ_num_decisions += 1;
+
+    if (mm_econ_mode == 0) {
+        return true;
+    } else if (mm_econ_mode == 1) {
+        should_do = cost->benefit > cost->cost;
+
+        if (should_do)
+            mm_econ_num_decisions_yes += 1;
+
+        //pr_warn("mm_econ: cost=%llu benefit=%llu\n", cost->cost, cost->benefit); // TODO remove
+        return should_do;
+    } else {
+        BUG();
+        return false;
+    }
+}
+EXPORT_SYMBOL(mm_decide);
+
+// Inform the estimator of the promotion of the given huge page.
+void mm_register_promotion(u64 addr)
+{
+    mm_econ_num_hp_promotions += 1;
+}
+
+static bool mm_does_quantity_match(struct mmap_comparison *c, u64 val)
+{
+    if (c->comp == CompEquals) {
+        return val == c->val;
+    } else if (c->comp == CompGreaterThan) {
+        return val > c->val;
+    } else if (c->comp == CompLessThan) {
+        return val < c->val;
+    } else {
+        pr_err("Invalid mmap comparatori\n");
+        BUG();
+    }
+
+    // Should never reach here
+    return false;
+}
+
+// Split base_range, which is in subranges, at addr based on comp and add
+// the new range(s) to subranges.
+static bool mm_split_ranges(struct profile_range *base_range, struct rb_root *subranges,
+        u64 addr, enum mmap_comparator comp)
+{
+    struct profile_range *split_range;
+
+    if (comp == CompGreaterThan) {
+        if (base_range->start >= addr) {
+            return true;
+        }
+
+        split_range = mm_econ_vmalloc(sizeof(struct profile_range));
+        if (!split_range)
+            return false;
+
+        split_range->benefit = 0;
+        split_range->start = base_range->start;
+        split_range->end = addr;
+        base_range->start = addr;
+
+        profile_range_insert(subranges, split_range);
+    } else if (comp == CompLessThan) {
+        if (base_range->end <= addr) {
+            return true;
+        }
+
+        split_range = mm_econ_vmalloc(sizeof(struct profile_range));
+        if (!split_range)
+            return false;
+
+        split_range->benefit = 0;
+        split_range->start = addr;
+        split_range->end = base_range->end;
+        base_range->end = addr;
+
+        profile_range_insert(subranges, split_range);
+    } else if (comp == CompEquals) {
+        // Do we need to split on the left?
+        if (base_range->start < addr) {
+            split_range = mm_econ_vmalloc(sizeof(struct profile_range));
+            if (!split_range)
+                return false;
+
+            split_range->benefit = 0;
+            split_range->start = base_range->start;
+            split_range->end = addr;
+            base_range->start = addr;
+
+            profile_range_insert(subranges, split_range);
+        }
+        // Do we need to split on the right?
+        if (base_range->end > addr + PAGE_SIZE) {
+            split_range = mm_econ_vmalloc(sizeof(struct profile_range));
+            if (!split_range)
+                return false;
+
+            split_range->benefit = 0;
+            split_range->start = addr + PAGE_SIZE;
+            split_range->end = base_range->end;
+            base_range->end = addr + PAGE_SIZE;
+
+            profile_range_insert(subranges, split_range);
+        }
+    }
+
+    return true;
+}
+
+static int mm_copy_profile_range(
+        struct rb_root* old_root, struct rb_root* new_root)
+{
+    struct rb_node *node = NULL;
+    struct profile_range *range = NULL;
+    struct profile_range *new_range = NULL;
+
+    node = rb_first(old_root);
+    while (node) {
+        new_range = mm_econ_vmalloc(sizeof(struct profile_range));
+        if (!new_range)
+            return -1;
+
+        range = container_of(node, struct profile_range, node);
+        new_range->start = range->start;
+        new_range->end = range->end;
+        new_range->benefit = range->benefit;
+
+        profile_range_insert(new_root, new_range);
+
+        node = rb_next(node);
+    }
+
+    return 0;
+}
+
+// Search mmap_filters for a filter that matches this new memory map
+// and add it to the list of ranges.
+// pid: The pid of the process who made this mmap
+// section: The memory section the memory range belongs to: code, data, heap, or mmap
+// mapaddr: The actual address the new mmap is mapped to
+// section_off: The offset of the memory range from the start of the section it belongs to
+// addr: The hint from the caller for what address the new mmap should be mapped to
+// len: The length of the new mmap
+// prot: The protection bits for the mmap
+// flags: The flags specified in the mmap call
+// fd: Descriptor of the file to map
+// off: Offset within the file to start the mapping
+// Do we need to lock mmap_filters?
+// We might need to lock the profile_ranges rb_tree
+void mm_add_memory_range(pid_t pid, enum mm_memory_section section, u64 mapaddr, u64 section_off,
+        u64 addr, u64 len, u64 prot, u64 flags, u64 fd, u64 off)
+{
+    struct mmap_filter_proc *proc;
+    struct mmap_filter *filter;
+    struct mmap_comparison *comp;
+    struct profile_range *range = NULL;
+    struct list_head *filter_head = NULL;
+    // Used to keep track of the subranges of the new memory range that are
+    // from splitting a range due to a addr or section_off constraint.
+    struct rb_root huge_subranges = RB_ROOT;
+    struct rb_root eager_subranges = RB_ROOT;
+    struct rb_node *range_node = NULL;
+    bool passes_filter;
+    u64 val;
+
+    // If this isn't the process we care about, move on
+    down_read(&filter_procs_sem);
+    proc = find_filter_proc_by_pid(pid);
+    up_read(&filter_procs_sem);
+
+    if (!proc)
+        return;
+
+    filter_head = &proc->filters;
+
+    // Start with the original range of the new mapping
+    range = mm_econ_vmalloc(sizeof(struct profile_range));
+    if (!range) {
+        pr_warn("mm_add_memory_range: no memory for new range");
+        return;
+    }
+    // Align the range bounds to a page
+    range->start = mapaddr & PAGE_MASK;
+    range->end = (mapaddr + len + PAGE_SIZE - 1) & PAGE_MASK;
+    range->benefit = 0;
+    profile_range_insert(&huge_subranges, range);
+
+    if (mm_copy_profile_range(&huge_subranges, &eager_subranges) != 0) {
+        pr_warn("mm_add_memory_range: no memory for new range");
+        return;
+    }
+
+    // Check if this mmap matches any of our filters
+    down_read(&filter_procs_sem);
+    list_for_each_entry(filter, filter_head, node) {
+        // Each filter only applies to either the eager or huge page policy
+        // This variable points to the applicable subranges tree
+        struct rb_root *subranges = NULL;
+        // We need a second rb_tree because we don't want to change the
+        // subranges tree unless we are sure a filter matches
+        struct rb_root temp_subranges = RB_ROOT;
+        // The range in the subranges tree that we are splitting
+        struct profile_range *parent_range = NULL;
+
+        if (filter->policy == PolicyHugePage) {
+            subranges = &huge_subranges;
+        } else if (filter->policy == PolicyEagerPage) {
+            subranges = &eager_subranges;
+        } else {
+            BUG();
+        }
+
+        passes_filter = section == filter->section;
+
+        list_for_each_entry(comp, &filter->comparisons, node) {
+            if (!passes_filter)
+                break;
+
+            // Determine the value to use for this comparison
+            if (comp->quant == QuantSectionOff || comp->quant == QuantAddr) {
+                // This type of filter comparison is the most complex because
+                // it may cause the region to be split one or more times.
+                // This happens when the new region overlaps with multiple filters.
+                // To handle this case, while we check if the region matches the
+                // filter, we also keep track of how we would need to split the
+                // regions using temp_subregions. These subregions then replace
+                // the larger region if the filter passes the region.
+
+                enum mmap_comparator comparator;
+                u64 section_base;
+                u64 search_key;
+                // Because ranges can be split, we need to handle this more
+                // carefully.
+
+                // Find the range to do the comparison on
+                // If the comparator is Addr, this is straight forward.
+                // Otherwise, this step basically involves converting the section offset
+                // given in the filter to a virtual address corresponding to
+                // that offset. We need to do this because the memory ranges
+                // we are operating on are virtual addresses.
+                // We need to account for the mmap section growing down
+                if (comp->quant == QuantAddr) {
+                    search_key = comp->val;
+                    comparator = comp->comp;
+                } else if (section == SectionMmap) {
+                    section_base = mapaddr + section_off;
+                    search_key = section_base - comp->val;
+
+                    if (comp->comp == CompGreaterThan)
+                        comparator = CompLessThan;
+                    else if (comp->comp == CompLessThan)
+                        comparator = CompGreaterThan;
+                    else
+                        comparator = comp->comp;
+                } else {
+                    section_base = mapaddr - section_off;
+                    search_key = section_base + comp->val;
+                    comparator = comp->comp;
+                }
+
+                if (!parent_range) {
+                    // Find the range to potentially split, and add it to
+                    // temp_subranges
+                    parent_range = profile_find_first_range(subranges, search_key, comparator);
+                    if (!parent_range) {
+                        passes_filter = false;
+                        break;
+                    }
+
+                    // If the found range has already matched with a filter, we
+                    // are done
+                    if (parent_range->benefit != 0) {
+                        passes_filter = false;
+                        break;
+                    }
+
+                    range = mm_econ_vmalloc(sizeof(struct profile_range));
+                    if (!range) {
+                        profile_free_all(&temp_subranges);
+                        goto err;
+                    }
+                    range->start = parent_range->start;
+                    range->end = parent_range->end;
+                    range->benefit = parent_range->benefit;
+
+                    profile_range_insert(&temp_subranges, range);
+                } else {
+                    // Find the range from the temp_subranges
+                    range = profile_find_first_range(&temp_subranges, search_key, comparator);
+                    if (!range) {
+                        passes_filter = false;
+                        break;
+                    }
+                }
+
+                // Assign the benefit value.
+                range->benefit = filter->benefit;
+
+                // Split the range if necessary
+                if (!mm_split_ranges(range, &temp_subranges, search_key, comparator)) {
+                    profile_free_all(&temp_subranges);
+                    goto err;
+                }
+
+                continue;
+            }
+            else if (comp->quant == QuantLen)
+                val = len;
+            else if (comp->quant == QuantProt)
+                val = prot;
+            else if (comp->quant == QuantFlags)
+                val = flags;
+            else if (comp->quant == QuantFD)
+                val = fd;
+            else
+                val = off;
+
+            passes_filter = passes_filter && mm_does_quantity_match(comp, val);
+        }
+
+        // If we split a range for this filter, remove the old range
+        // from the subranges tree, and add the new ones
+        if (passes_filter && parent_range) {
+            range_node = &parent_range->node;
+            rb_erase(range_node, subranges);
+            mm_econ_vfree(parent_range, sizeof(struct profile_range));
+
+            profile_move(&temp_subranges, subranges);
+        }
+        // If the entire new range matches this filter, set the benefit
+        // value for all of the subranges that have not been set yet
+        else if(passes_filter) {
+            range_node = rb_first(subranges);
+
+            while (range_node) {
+                range = container_of(range_node, struct profile_range, node);
+
+                if (range->benefit == 0)
+                    range->benefit = filter->benefit;
+
+                range_node = rb_next(range_node);
+            }
+
+            // Because the entire new range matched a filter, we no longer
+            // have to check the rest of the filters
+            break;
+        }
+    }
+    up_read(&filter_procs_sem);
+
+    // Finally, insert all of the new ranges into the proc's tree
+    down_write(&filter_procs_sem);
+    profile_move(&huge_subranges, &proc->hp_ranges_root);
+    profile_move(&eager_subranges, &proc->eager_ranges_root);
+    up_write(&filter_procs_sem);
+    return;
+
+err:
+    pr_warn("mm_add_memory_range: no memory for new range");
+    profile_free_all(&huge_subranges);
+    profile_free_all(&eager_subranges);
+    up_read(&filter_procs_sem);
+    //printk("Added range %d %llx %llx %lld %llx\n", section, range->start, range->end, range->benefit, len);
+}
+
+void mm_copy_profile(pid_t old_pid, pid_t new_pid)
+{
+    struct mmap_filter_proc *proc = NULL;
+    struct mmap_filter_proc *new_proc = NULL;
+    struct mmap_filter *filter = NULL;
+    struct mmap_filter *new_filter = NULL;
+    struct mmap_comparison *comparison = NULL;
+    struct mmap_comparison *new_comparison = NULL;
+
+    down_read(&filter_procs_sem);
+
+    // First, find out if a profile for old_pid exists
+    proc = find_filter_proc_by_pid(old_pid);
+
+    if (!proc) {
+        up_read(&filter_procs_sem);
+        return;
+    }
+
+    new_proc = mm_econ_vmalloc(sizeof(struct mmap_filter_proc));
+    if (!new_proc)
+        goto err;
+    new_proc->pid = new_pid;
+    INIT_LIST_HEAD(&new_proc->filters);
+    new_proc->hp_ranges_root = RB_ROOT;
+    new_proc->eager_ranges_root = RB_ROOT;
+
+    // First, copy the filters
+    list_for_each_entry(filter, &proc->filters, node) {
+        new_filter = mm_econ_vmalloc(sizeof(struct mmap_filter));
+        if (!new_filter)
+            goto err;
+
+        new_filter->section = filter->section;
+        new_filter->benefit = filter->benefit;
+        new_filter->policy = filter->policy;
+        INIT_LIST_HEAD(&new_filter->comparisons);
+
+        list_add_tail(&new_filter->node, &new_proc->filters);
+
+        list_for_each_entry(comparison, &filter->comparisons, node) {
+            new_comparison = mm_econ_vmalloc(sizeof(struct mmap_comparison));
+            if (!new_comparison)
+                goto err;
+
+            new_comparison->quant = comparison->quant;
+            new_comparison->comp = comparison->comp;
+            new_comparison->val = comparison->val;
+
+            list_add_tail(&new_comparison->node, &new_filter->comparisons);
+        }
+    }
+
+    // Now, copy the ranges
+    if (mm_copy_profile_range(&proc->hp_ranges_root, &new_proc->hp_ranges_root) != 0)
+        goto err;
+    if (mm_copy_profile_range(&proc->eager_ranges_root, &new_proc->eager_ranges_root) != 0)
+        goto err;
+
+    up_read(&filter_procs_sem);
+
+    // Now, add the new proc to the list of procs
+    down_write(&filter_procs_sem);
+    list_add_tail(&new_proc->node, &filter_procs);
+    up_write(&filter_procs_sem);
+
+    return;
+err:
+    up_read(&filter_procs_sem);
+
+    pr_warn("mm_econ: Unable to copy profile from %d to %d", old_pid, new_pid);
+    if (new_proc) {
+        profile_free_all(&new_proc->hp_ranges_root);
+        profile_free_all(&new_proc->eager_ranges_root);
+        mmap_filters_free_all(new_proc);
+        mm_econ_vfree(new_proc, sizeof(struct mmap_filter_proc));
+    }
+}
+
+void mm_profile_check_exiting_proc(pid_t pid)
+{
+    struct mmap_filter_proc *proc;
+
+    down_read(&filter_procs_sem);
+    proc = find_filter_proc_by_pid(pid);
+    up_read(&filter_procs_sem);
+
+    if (proc) {
+        down_write(&filter_procs_sem);
+        // If the process exits, we should also clear its profile
+        profile_free_all(&proc->hp_ranges_root);
+        profile_free_all(&proc->eager_ranges_root);
+        mmap_filters_free_all(proc);
+
+        // Remove the node from the list
+        list_del(&proc->node);
+        mm_econ_vfree(proc, sizeof(struct mmap_filter_proc));
+        up_write(&filter_procs_sem);
+    }
+}
+
+///////////////////////////////////////////////////////////////////////////////
+// sysfs files
+
+static ssize_t enabled_show(struct kobject *kobj,
+        struct kobj_attribute *attr, char *buf)
+{
+    return sprintf(buf, "%d\n", mm_econ_mode);
+}
+
+static ssize_t enabled_store(struct kobject *kobj,
+        struct kobj_attribute *attr,
+        const char *buf, size_t count)
+{
+    int mode;
+    int ret;
+
+    ret = kstrtoint(buf, 0, &mode);
+
+    if (ret != 0) {
+        mm_econ_mode = 0;
+        return ret;
+    }
+    else if (mode >= 0 && mode <= 1) {
+        mm_econ_mode = mode;
+        return count;
+    }
+    else {
+        mm_econ_mode = 0;
+        return -EINVAL;
+    }
+}
+static struct kobj_attribute enabled_attr =
+__ATTR(enabled, 0644, enabled_show, enabled_store);
+
+static ssize_t debugging_mode_show(struct kobject *kobj,
+        struct kobj_attribute *attr, char *buf)
+{
+    return sprintf(buf, "%d\n", mm_econ_debugging_mode);
+}
+
+static ssize_t debugging_mode_store(struct kobject *kobj,
+        struct kobj_attribute *attr,
+        const char *buf, size_t count)
+{
+    int mode;
+    int ret;
+
+    ret = kstrtoint(buf, 0, &mode);
+
+    if (ret != 0) {
+        mm_econ_debugging_mode = 0;
+        return ret;
+    }
+    else {
+        mm_econ_debugging_mode = mode;
+        return count;
+    }
+}
+static struct kobj_attribute debugging_mode_attr =
+__ATTR(debugging_mode, 0644, debugging_mode_show, debugging_mode_store);
+
+static ssize_t contention_cycles_show(struct kobject *kobj,
+        struct kobj_attribute *attr, char *buf)
+{
+    return sprintf(buf, "%llu\n", mm_econ_contention_ms);
+}
+
+static ssize_t contention_cycles_store(struct kobject *kobj,
+        struct kobj_attribute *attr,
+        const char *buf, size_t count)
+{
+    u64 ms;
+    int ret;
+
+    ret = kstrtou64(buf, 0, &ms);
+
+    if (ret != 0) {
+        return ret;
+    }
+    else {
+        mm_econ_contention_ms = ms;
+        return count;
+    }
+}
+static struct kobj_attribute contention_cycles_attr =
+__ATTR(contention_cyles, 0644, contention_cycles_show, contention_cycles_store);
+
+static ssize_t freq_mhz_show(struct kobject *kobj,
+        struct kobj_attribute *attr, char *buf)
+{
+    return sprintf(buf, "%llu\n", mm_econ_freq_mhz);
+}
+
+static ssize_t freq_mhz_store(struct kobject *kobj,
+        struct kobj_attribute *attr,
+        const char *buf, size_t count)
+{
+    u64 mhz;
+    int ret;
+
+    ret = kstrtou64(buf, 0, &mhz);
+
+    if (ret != 0) {
+        return ret;
+    }
+    else {
+        mm_econ_freq_mhz = mhz;
+        return count;
+    }
+}
+static struct kobj_attribute freq_mhz_attr =
+__ATTR(freq_mhz, 0644, freq_mhz_show, freq_mhz_store);
+
+static ssize_t stats_show(struct kobject *kobj,
+        struct kobj_attribute *attr, char *buf)
+{
+    return sprintf(buf,
+            "estimated=%lld\ndecided=%lld\n"
+            "yes=%lld\npromoted=%lld\n"
+            "compactions=%lld\nprezerotry=%lld\n"
+            "vmallocbytes=%lld\n",
+            mm_econ_num_estimates,
+            mm_econ_num_decisions,
+            mm_econ_num_decisions_yes,
+            mm_econ_num_hp_promotions,
+            mm_econ_num_async_compaction,
+            mm_econ_num_async_prezeroing,
+            mm_econ_vmalloc_bytes);
+}
+
+static ssize_t stats_store(struct kobject *kobj,
+        struct kobj_attribute *attr,
+        const char *buf, size_t count)
+{
+    return -EINVAL;
+}
+static struct kobj_attribute stats_attr =
+__ATTR(stats, 0444, stats_show, stats_store);
+
+static struct attribute *mm_econ_attr[] = {
+    &enabled_attr.attr,
+    &contention_cycles_attr.attr,
+    &stats_attr.attr,
+    &debugging_mode_attr.attr,
+    &freq_mhz_attr.attr,
+    NULL,
+};
+
+static const struct attribute_group mm_econ_attr_group = {
+    .attrs = mm_econ_attr,
+};
+
+///////////////////////////////////////////////////////////////////////////////
+// procfs files
+
+static void mm_memory_section_get_str(char *buf, enum mm_memory_section section)
+{
+    if (section == SectionCode) {
+        strcpy(buf, "code");
+    } else if (section == SectionData) {
+        strcpy(buf, "data");
+    } else if (section == SectionHeap) {
+        strcpy(buf, "heap");
+    } else if (section == SectionMmap) {
+        strcpy(buf, "mmap");
+    } else {
+        printk(KERN_WARNING "Invalid memory section");
+        BUG();
+    }
+}
+
+static void mm_policy_get_str(char *buf, enum mm_policy policy)
+{
+    if (policy == PolicyHugePage) {
+        strcpy(buf, "huge");
+    } else if (policy == PolicyEagerPage) {
+        strcpy(buf, "eager");
+    } else {
+        pr_warn("Invalid mm policy");
+        BUG();
+    }
+}
+
+static char mmap_comparator_get_char(enum mmap_comparator comp)
+{
+    if (comp == CompEquals) {
+        return '=';
+    } else if (comp == CompGreaterThan) {
+        return '>';
+    } else if (comp == CompLessThan) {
+        return '<';
+    } else {
+        printk(KERN_WARNING "Invalid mmap comparator");
+        BUG();
+    }
+}
+
+static void mmap_quantity_get_str(char *buf, enum mmap_quantity quant)
+{
+    if (quant == QuantSectionOff) {
+        strcpy(buf, "section_off");
+    } else if (quant == QuantAddr) {
+        strcpy(buf, "addr");
+    } else if (quant == QuantLen) {
+        strcpy(buf, "len");
+    } else if (quant == QuantProt) {
+        strcpy(buf, "prot");
+    } else if (quant == QuantFlags) {
+        strcpy(buf, "flags");
+    } else if (quant == QuantFD) {
+        strcpy(buf, "fd");
+    } else if (quant == QuantOff) {
+        strcpy(buf, "off");
+    } else {
+        pr_warn("Invalid mmap quantity");
+        BUG();
+    }
+}
+
+static ssize_t mmap_filters_read(struct file *file,
+        char __user *buf, size_t count, loff_t *ppos)
+{
+    struct task_struct *task = extern_get_proc_task(file_inode(file));
+    char *buffer;
+    ssize_t len = 0;
+    ssize_t ret = 0;
+    struct mmap_filter *filter;
+    struct mmap_comparison *comparison;
+    struct mmap_filter_proc *proc;
+    struct list_head *filter_head = NULL;
+
+    if (!task)
+        return -ESRCH;
+
+    buffer = mm_econ_vmalloc(MMAP_FILTER_BUF_SIZE);
+    if (!buffer) {
+        put_task_struct(task);
+        return -ENOMEM;
+    }
+
+    // First, print the CSV Header for easier reading
+    len = sprintf(buffer, "POLICY,SECTION,MISSES,CONSTRAINTS...\n");
+
+    // Find the filters that correspond to this process if there are any
+    down_read(&filter_procs_sem);
+    proc = find_filter_proc_by_pid(task->tgid);
+    if (!proc)
+        goto out;
+
+    filter_head = &proc->filters;
+
+    // Print out all of the filters
+    list_for_each_entry(filter, filter_head, node) {
+        char policy[8];
+        char section[8];
+        char quantity[16];
+        u64 benefit = filter->benefit;
+        char comparator;
+        u64 val;
+
+        mm_policy_get_str(policy, filter->policy);
+        mm_memory_section_get_str(section, filter->section);
+
+        // Make sure we don't overflow the buffer
+        if (len > MMAP_FILTER_BUF_SIZE - MMAP_FILTER_BUF_DEAD_ZONE)
+            goto out;
+
+        // Print the per filter information
+        len += sprintf(&buffer[len], "%s,%s,0x%llx", policy, section, benefit);
+
+        list_for_each_entry(comparison, &filter->comparisons, node) {
+            mmap_quantity_get_str(quantity, comparison->quant);
+            comparator = mmap_comparator_get_char(comparison->comp);
+            val = comparison->val;
+
+            // Make sure we don't overflow the buffer
+            if (len > MMAP_FILTER_BUF_SIZE - MMAP_FILTER_BUF_DEAD_ZONE)
+                goto out;
+
+            // Print the per comparison information
+            len += sprintf(&buffer[len], ",%s,%c,0x%llx", quantity,
+                comparator, val);
+        }
+
+        // Remember to end with a newline
+        len += sprintf(&buffer[len], "\n");
+    }
+
+out:
+    up_read(&filter_procs_sem);
+
+    ret = simple_read_from_buffer(buf, count, ppos, buffer, len);
+
+    // Remember to free the buffer
+    mm_econ_vfree(buffer, MMAP_FILTER_BUF_SIZE);
+
+    put_task_struct(task);
+
+    return ret;
+}
+
+static int get_memory_section(char *buf, enum mm_memory_section *section)
+{
+    int ret = 0;
+
+    if (strcmp(buf, "code") == 0) {
+        *section = SectionCode;
+    } else if (strcmp(buf, "data") == 0) {
+        *section = SectionData;
+    } else if (strcmp(buf, "heap") == 0) {
+        *section = SectionHeap;
+    } else if (strcmp(buf, "mmap") == 0) {
+        *section = SectionMmap;
+    } else {
+        ret = -1;
+    }
+
+    return ret;
+}
+
+static int get_mm_policy(char *buf, enum mm_policy *policy)
+{
+    int ret = 0;
+
+    if (strcmp(buf, "huge") == 0) {
+        *policy = PolicyHugePage;
+    } else if (strcmp(buf, "eager") == 0) {
+        *policy = PolicyEagerPage;
+    } else {
+        ret = -1;
+    }
+
+    return ret;
+}
+
+static int get_mmap_quantity(char *buf, enum mmap_quantity *quant)
+{
+    int ret = 0;
+
+    if (strcmp(buf, "section_off") == 0) {
+        *quant = QuantSectionOff;
+    } else if (strcmp(buf, "addr") == 0) {
+        *quant = QuantAddr;
+    } else if (strcmp(buf, "len") == 0) {
+        *quant = QuantLen;
+    } else if (strcmp(buf, "prot") == 0) {
+        *quant = QuantProt;
+    } else if (strcmp(buf, "flags") == 0) {
+        *quant = QuantFlags;
+    } else if (strcmp(buf, "fd") == 0) {
+        *quant = QuantFD;
+    } else if (strcmp(buf, "off") == 0) {
+        *quant = QuantOff;
+    } else {
+        ret = -1;
+    }
+
+    return ret;
+}
+
+static int get_mmap_comparator(char *buf, enum mmap_comparator *comp)
+{
+    int ret = 0;
+
+    if (strcmp(buf, "=") == 0) {
+        *comp = CompEquals;
+    } else if (strcmp(buf, ">") == 0) {
+        *comp = CompGreaterThan;
+    } else if (strcmp(buf, "<") == 0) {
+        *comp = CompLessThan;
+    } else {
+        ret = -1;
+    }
+
+    return ret;
+}
+
+static int mmap_filter_read_comparison(char **tok, struct mmap_comparison *c)
+{
+    int ret = 0;
+    u64 value = 0;
+    char *value_buf;
+
+    // Get the quantity
+    value_buf = strsep(tok, ",");
+    if (!value_buf) {
+        return -1;
+    }
+
+    ret = get_mmap_quantity(value_buf, &c->quant);
+    if (ret != 0) {
+        return -1;
+    }
+
+    // Get the comparator
+    value_buf = strsep(tok, ",");
+    if (!value_buf) {
+        return -1;
+    }
+
+    ret = get_mmap_comparator(value_buf, &c->comp);
+    if (ret != 0) {
+        return -1;
+    }
+
+    // Get the value
+    value_buf = strsep(tok, ",");
+    if (!value_buf) {
+        return -1;
+    }
+
+    ret = kstrtoull(value_buf, 0, &value);
+    if (ret != 0) {
+        return -1;
+    }
+
+    c->val = value;
+
+    return 0;
+}
+
+static ssize_t mmap_filters_write(struct file *file,
+        const char __user *buf, size_t count,
+        loff_t *ppos)
+{
+    struct task_struct *task = NULL;
+    char *buf_from_user = NULL;
+    char *outerTok = NULL;
+    char *tok = NULL;
+    struct mmap_filter *filter = NULL;
+    struct mmap_comparison *comparison = NULL;
+    struct mmap_filter_proc *proc = NULL;
+    bool alloc_new_proc;
+    ssize_t error = 0;
+    size_t bytes_read = 0;
+    size_t filter_len = 0;
+    int ret;
+    u64 value;
+    char * value_buf;
+
+    // Copy the input from userspace
+    buf_from_user = mm_econ_vmalloc(count + 1);
+    if (!buf_from_user)
+        return -ENOMEM;
+    if (copy_from_user(buf_from_user, buf, count)) {
+        error = -EFAULT;
+        goto err;
+    }
+    buf_from_user[count] = 0;
+    outerTok = buf_from_user;
+
+    task = extern_get_proc_task(file_inode(file));
+    if (!task) {
+        error = -ESRCH;
+        goto err;
+    }
+
+    down_write(&filter_procs_sem);
+    // Allocate the proc structure if necessary
+    if ((proc = find_filter_proc_by_pid(task->tgid))) { // NOTE: assignment
+        alloc_new_proc = false;
+    } else {
+        alloc_new_proc = true;
+        proc = mm_econ_vmalloc(sizeof(struct mmap_filter_proc));
+        if (!proc) {
+            up_write(&filter_procs_sem);
+            error = -ENOMEM;
+            goto err;
+        }
+
+        // Initialize the new proc
+        proc->pid = task->tgid;
+        INIT_LIST_HEAD(&proc->filters);
+        proc->hp_ranges_root = RB_ROOT;
+    }
+    up_write(&filter_procs_sem);
+
+    // Read in the filters
+    tok = strsep(&outerTok, "\n");
+    while (outerTok) {
+        bool invalid_filter = false;
+
+        if (tok[0] == '\0') {
+            break;
+        }
+
+        // Include the \n that was removed by strsep
+        filter_len = strlen(tok) + 1;
+
+        filter = mm_econ_vmalloc(sizeof(struct mmap_filter));
+        if (!filter) {
+            error = -ENOMEM;
+            goto err;
+        }
+
+        // Get the policy the filter applies to
+        value_buf = strsep(&tok, ",");
+        if (!value_buf) {
+            break;
+        }
+        ret = get_mm_policy(value_buf, &filter->policy);
+        if (ret != 0) {
+            break;
+        }
+
+        // Get the section of the memory map
+        value_buf = strsep(&tok, ",");
+        if (!value_buf) {
+            break;
+        }
+        ret = get_memory_section(value_buf, &filter->section);
+        if (ret != 0) {
+            break;
+        }
+
+        // Get the benefit for the filter
+        value_buf = strsep(&tok, ",");
+        if (!value_buf) {
+            break;
+        }
+
+        ret = kstrtoull(value_buf, 0, &value);
+        if (ret != 0) {
+            break;
+        }
+
+        filter->benefit = value;
+
+        // Read in the comparisons of the filter
+        INIT_LIST_HEAD(&filter->comparisons);
+        while (tok) {
+            if (tok[0] == '\0')
+                break;
+
+            comparison = mm_econ_vmalloc(sizeof(struct mmap_comparison));
+            if (!comparison) {
+                error = -ENOMEM;
+                goto err;
+            }
+
+            ret = mmap_filter_read_comparison(&tok, comparison);
+            if (ret != 0) {
+                invalid_filter = true;
+                mm_econ_vfree(comparison, sizeof(struct mmap_comparison));
+                break;
+            }
+
+            // Add the comparison to the list of comparisons
+            list_add_tail(&comparison->node, &filter->comparisons);
+        }
+
+        if (invalid_filter)
+            break;
+
+        // Add the new filter to the list
+        down_write(&filter_procs_sem);
+        list_add_tail(&filter->node, &proc->filters);
+        up_write(&filter_procs_sem);
+
+        // Get the next filter
+        tok = strsep(&outerTok, "\n");
+
+        bytes_read += filter_len;
+    }
+
+    // The write system call might not write the entire filter file in one go.
+    // We most handle the case where the file is seperated in the middle of a
+    // filter, making it look invalid.
+    // If that happens, we simply say we read up until the last full filter.
+    // However, if we read no good filters before the first invalid filter,
+    // just assume the filter is bad.
+    if (bytes_read == 0) {
+        error = -EINVAL;
+        goto err;
+    }
+
+    // Link the new proc if we need to
+    if (alloc_new_proc) {
+        down_write(&filter_procs_sem);
+        list_add_tail(&proc->node, &filter_procs);
+        up_write(&filter_procs_sem);
+    }
+
+    mm_econ_vfree(buf_from_user, count + 1);
+    put_task_struct(task);
+
+    return bytes_read;
+
+err:
+    if (filter)
+        mm_econ_vfree(filter, sizeof(struct mmap_filter));
+    if (proc) {
+        down_write(&filter_procs_sem);
+        mmap_filters_free_all(proc);
+        up_write(&filter_procs_sem);
+        if (alloc_new_proc)
+            mm_econ_vfree(proc, sizeof(struct mmap_filter_proc));
+    }
+    if (task)
+        put_task_struct(task);
+    if (buf_from_user)
+        mm_econ_vfree(buf_from_user, count + 1);
+    return error;
+}
+
+const struct file_operations proc_mmap_filters_operations = {
+    .read = mmap_filters_read,
+    .write = mmap_filters_write,
+    .llseek = default_llseek,
+};
+
+static ssize_t print_range_tree(char *buffer, ssize_t buf_size, struct rb_node *node)
+{
+    ssize_t len = 0;
+
+    while (node) {
+        struct profile_range *range =
+            container_of(node, struct profile_range, node);
+
+        // Make sure we don't overflow the buffer
+        if (len > buf_size - MMAP_FILTER_BUF_DEAD_ZONE)
+            return len;
+
+        len += sprintf(
+            &buffer[len],
+            "[0x%llx, 0x%llx) (%llu bytes) benefit=0x%llx\n",
+            range->start,
+            range->end,
+            range->end - range->start,
+            range->benefit
+        );
+
+        node = rb_next(node);
+    }
+
+    return len;
+}
+
+static ssize_t print_profile(struct file *file,
+        char __user *buf, size_t count, loff_t *ppos)
+{
+    struct task_struct *task = extern_get_proc_task(file_inode(file));
+    char *buffer;
+    ssize_t len = 0;
+    ssize_t ret = 0;
+    struct mmap_filter_proc *proc;
+    struct rb_node *node = NULL;
+    bool found = false;
+
+    if (!task)
+        return -ESRCH;
+
+    // Find the data for the process this relates to
+    down_read(&filter_procs_sem);
+    list_for_each_entry(proc, &filter_procs, node) {
+        if (proc->pid == task->tgid) {
+            found = true;
+            break;
+        }
+    }
+    up_read(&filter_procs_sem);
+    if (!found) {
+        put_task_struct(task);
+        return 0;
+    }
+
+    buffer = mm_econ_vmalloc(MMAP_FILTER_BUF_SIZE);
+    if (!buffer) {
+        put_task_struct(task);
+        return -ENOMEM;
+    }
+
+    down_read(&filter_procs_sem);
+    len += sprintf(buffer, "Huge Page Ranges:\n");
+    node = rb_first(&proc->hp_ranges_root);
+    len += print_range_tree(&buffer[len], MMAP_FILTER_BUF_SIZE - len, node);
+
+    len += sprintf(&buffer[len], "Eager Page Ranges:\n");
+    node = rb_first(&proc->eager_ranges_root);
+    len += print_range_tree(&buffer[len], MMAP_FILTER_BUF_SIZE - len, node);
+
+    up_read(&filter_procs_sem);
+
+    ret = simple_read_from_buffer(buf, count, ppos, buffer, len);
+
+    mm_econ_vfree(buffer, MMAP_FILTER_BUF_SIZE);
+
+    put_task_struct(task);
+
+    return ret;
+}
+
+const struct file_operations proc_mem_ranges_operations = {
+    .read = print_profile,
+    .llseek = default_llseek,
+};
+///////////////////////////////////////////////////////////////////////////////
+// Init
+
+static int __init mm_econ_init(void)
+{
+    struct kobject *mm_econ_kobj;
+    int err;
+
+    mm_econ_kobj = kobject_create_and_add("mm_econ", mm_kobj);
+    if (unlikely(!mm_econ_kobj)) {
+        pr_err("failed to create mm_econ kobject\n");
+        return -ENOMEM;
+    }
+
+    err = sysfs_create_group(mm_econ_kobj, &mm_econ_attr_group);
+    if (err) {
+        pr_err("failed to register mm_econ group\n");
+        kobject_put(mm_econ_kobj);
+        return err;
+    }
+
+    return 0;
+}
+subsys_initcall(mm_econ_init);
diff --git a/mm/gup.c b/mm/gup.c
index 5244b8090440..a16aeb176247 100644
--- a/mm/gup.c
+++ b/mm/gup.c
@@ -17,6 +17,7 @@
 #include <linux/migrate.h>
 #include <linux/mm_inline.h>
 #include <linux/sched/mm.h>
+#include <linux/mm_stats.h>
 
 #include <asm/mmu_context.h>
 #include <asm/pgtable.h>
@@ -620,6 +621,8 @@ static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,
 {
 	unsigned int fault_flags = 0;
 	vm_fault_t ret;
+	struct mm_stats_pftrace pftrace; // dummy, not used
+	mm_stats_pftrace_init(&pftrace);
 
 	/* mlock all present pages, but do not fault in new pages */
 	if ((*flags & (FOLL_POPULATE | FOLL_MLOCK)) == FOLL_MLOCK)
@@ -637,7 +640,7 @@ static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,
 		fault_flags |= FAULT_FLAG_TRIED;
 	}
 
-	ret = handle_mm_fault(vma, address, fault_flags);
+	ret = handle_mm_fault(vma, address, fault_flags, &pftrace);
 	if (ret & VM_FAULT_ERROR) {
 		int err = vm_fault_to_errno(ret, *flags);
 
@@ -957,6 +960,8 @@ int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,
 {
 	struct vm_area_struct *vma;
 	vm_fault_t ret, major = 0;
+	struct mm_stats_pftrace pftrace; // dummy, not used
+	mm_stats_pftrace_init(&pftrace);
 
 	address = untagged_addr(address);
 
@@ -971,7 +976,7 @@ int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,
 	if (!vma_permits_fault(vma, fault_flags))
 		return -EFAULT;
 
-	ret = handle_mm_fault(vma, address, fault_flags);
+	ret = handle_mm_fault(vma, address, fault_flags, &pftrace);
 	major |= ret & VM_FAULT_MAJOR;
 	if (ret & VM_FAULT_ERROR) {
 		int err = vm_fault_to_errno(ret, 0);
diff --git a/mm/hmm.c b/mm/hmm.c
index d379cb6496ae..7356e136225e 100644
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@ -25,6 +25,7 @@
 #include <linux/dma-mapping.h>
 #include <linux/mmu_notifier.h>
 #include <linux/memory_hotplug.h>
+#include <linux/mm_stats.h>
 
 struct hmm_vma_walk {
 	struct hmm_range	*range;
@@ -41,6 +42,8 @@ static int hmm_vma_do_fault(struct mm_walk *walk, unsigned long addr,
 	struct hmm_range *range = hmm_vma_walk->range;
 	struct vm_area_struct *vma = walk->vma;
 	vm_fault_t ret;
+	struct mm_stats_pftrace pftrace; // dummy, not used
+	mm_stats_pftrace_init(&pftrace);
 
 	if (!vma)
 		goto err;
@@ -50,7 +53,7 @@ static int hmm_vma_do_fault(struct mm_walk *walk, unsigned long addr,
 	if (write_fault)
 		flags |= FAULT_FLAG_WRITE;
 
-	ret = handle_mm_fault(vma, addr, flags);
+	ret = handle_mm_fault(vma, addr, flags, &pftrace);
 	if (ret & VM_FAULT_RETRY) {
 		/* Note, handle_mm_fault did up_read(&mm->mmap_sem)) */
 		return -EAGAIN;
diff --git a/mm/huge-addr-mm-econ-profile.py b/mm/huge-addr-mm-econ-profile.py
new file mode 100755
index 000000000000..1ca9c61d11f5
--- /dev/null
+++ b/mm/huge-addr-mm-econ-profile.py
@@ -0,0 +1,52 @@
+#!/usr/bin/env python3
+
+import sys
+import csv
+
+if len(sys.argv) == 1:
+    print("./script <file>, where <file> is a CSV dump of the Averages sheet.")
+
+FILE = sys.argv[1]
+
+# { (start addr, end addr) : total TLB miss delta }
+data = {}
+
+def total_misses(row):
+    return float(row["avg dtlb_load_misses.walk_active"]) \
+         + float(row["avg dtlb_store_misses.walk_active"])
+
+with open(FILE, 'r') as f:
+    reader = csv.DictReader(f, delimiter=',')
+
+    prev_end = 0
+    prev_misses = None
+
+    for row in reader:
+        # handle the control run specially
+        if row["Huge page"] == "none":
+            prev_end = 0
+            prev_misses = total_misses(row)
+            continue
+
+        if row["Huge page"] == "thp":
+            continue
+
+        # general case
+        end = int(row["Huge page"], base=16)
+        misses = total_misses(row)
+
+        diff = prev_misses - misses
+        
+        if diff > 0:
+            data[(prev_end, end)] = diff
+
+        prev_end = end
+        prev_misses = misses
+
+        # TODO: convert to misses / page / LTU
+
+datap = [hex(r[0])+" "+hex(r[1])+" "+str(int(count)) for r, count in data.items()]
+print("mm_econ", ";".join(datap))
+print()
+datap = [hex(r[0])+","+hex(r[1])+","+str(int(count)) for r, count in data.items()]
+print("runner", ";".join(datap))
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 54c106bdbafd..b7053fb947a6 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -33,6 +33,10 @@
 #include <linux/oom.h>
 #include <linux/numa.h>
 #include <linux/page_owner.h>
+#include <linux/mm_stats.h>
+#include <linux/rbtree.h>
+#include <linux/badger_trap.h>
+#include <linux/mm_econ.h>
 
 #include <asm/tlb.h>
 #include <asm/pgalloc.h>
@@ -62,7 +66,260 @@ static struct shrinker deferred_split_shrinker;
 static atomic_t huge_zero_refcount;
 struct page *huge_zero_page __read_mostly;
 
-bool transparent_hugepage_enabled(struct vm_area_struct *vma)
+struct huge_addr_range {
+	struct rb_node node;
+	u64 start;
+	u64 end;
+};
+
+/*
+ * With the `huge_addr` sysfs flag, userspace can choose a single pid and
+ * 2MB-aligned address to make into a huge page.
+ *
+ * Values of 0 indicate that the value is unset. Otherwise, the PID must be a
+ * valid process ID and huge_addr must be a 2MB-aligned address.
+ *
+ * The mode toggles different modes of operations:
+ *   0: use huge_addr as a single address to make huge
+ *   1: use huge_addr as the highest allowed huge page
+ *   2: use huge_addr as the lowest allowed huge page
+ *   3: use huge_addr_ranges to see if a page should be huge
+ */
+int huge_addr_mode = 0;
+pid_t huge_addr_pid = 0;
+u64 huge_addr = 0;
+struct rb_root huge_addr_range_tree = RB_ROOT;
+char huge_addr_comm[MAX_HUGE_ADDR_COMM];
+
+void get_page_mapping(unsigned long address, unsigned long *pfn,
+		struct page **page, bool *is_huge)
+{
+	struct task_struct *task;
+	struct mm_struct *mm;
+	struct vm_area_struct *vma;
+	pgd_t *pgd;
+	p4d_t *p4d;
+	pud_t *pud;
+	pmd_t *pmd;
+	spinlock_t *ptl;
+	pte_t *ptep;
+
+	*pfn = 0;
+	*page = NULL;
+	*is_huge = false;
+
+	// Get the VMA. If the process or VMA no longer exist. Exit early.
+	if (huge_addr_pid == 0) {
+		pr_warn("dump_mapping: no pid specified\n");
+		return;
+	}
+
+	task = get_pid_task(find_get_pid(huge_addr_pid), PIDTYPE_PID);
+	if (!task) {
+		pr_warn("no such pid for dump_mapping\n");
+		return;
+	}
+
+	pr_warn("dump_mapping: using task pid %d, comm=%16s, mm=%p\n",
+			huge_addr_pid, task->comm, task->mm);
+	mm = task->mm;
+	down_read(&mm->mmap_sem);
+
+	vma = find_vma(mm, address);
+	if (!vma) {
+		pr_warn("Unable to find VMA...\n");
+		goto out;
+	}
+
+	// Walk the page table until we find the mapping or confirm there is
+	// none.
+	pgd = pgd_offset(mm, address);
+	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd))) {
+		pr_warn("Unable to get pgd (pgd->p4d->pud->pmd->pte->page).\n");
+		goto out;
+	}
+
+	p4d = p4d_offset(pgd, address);
+	if (p4d_none(*p4d) || unlikely(p4d_bad(*p4d))) {
+		pr_warn("Unable to get p4d (pgd->p4d->pud->pmd->pte->page).\n");
+		goto out;
+	}
+
+	pud = pud_offset(p4d, address);
+	if (pud_none(*pud) || unlikely(pud_bad(*pud))) {
+		pr_warn("Unable to get pud (pgd->p4d->pud->pmd->pte->page).\n");
+		goto out;
+	}
+
+	pmd = pmd_offset(pud, address);
+	if (pmd_none(*pmd)) {
+		pr_warn("Unable to get pmd (pgd->p4d->pud->pmd->pte->page): %lx.\n",
+				native_pmd_val(*pmd));
+		goto out;
+	}
+
+	ptl = pmd_trans_huge_lock(pmd, vma);
+	if (ptl) {
+		if (pmd_present(*pmd)) {
+			*page = follow_trans_huge_pmd(vma, address, pmd, 0);
+			if (IS_ERR_OR_NULL(page)) {
+				pr_warn("dump_mapping: unexpectedly unable to follow huge page\n");
+				*page = NULL;
+				spin_unlock(ptl);
+				goto out;
+			}
+			pr_warn("dump_mapping: found huge page\n");
+
+			*is_huge = true;
+			*pfn = page_to_pfn(*page);
+
+			spin_unlock(ptl);
+			goto out;
+		}
+		spin_unlock(ptl);
+	}
+
+	*page = NULL;
+
+	if (pmd_trans_unstable(pmd)) {
+		pr_warn("dump_mapping: Unstable THP pmd. Try again.\n");
+		goto out;
+	}
+
+	ptep = pte_offset_map(pmd, address);
+	if (!pte_present(*ptep)) {
+		pr_warn("Unable to get pte (pgd->p4d->pud->pmd->pte->page).\n");
+		goto out;
+	}
+
+	// It's a base page
+	*is_huge = false;
+	*pfn = pte_pfn(*ptep);
+	*page = pfn_to_page(*pfn);
+
+out:
+	up_read(&mm->mmap_sem);
+	put_task_struct(task);
+	return;
+}
+
+static void huge_addr_range_insert(struct rb_root *root, struct huge_addr_range *new_range)
+{
+	struct rb_node **link = &root->rb_node;
+	struct rb_node *parent = NULL;
+	struct huge_addr_range *range;
+	u64 start = new_range->start;
+
+	while (*link) {
+		parent = *link;
+		range = container_of(parent, struct huge_addr_range, node);
+
+		if (start < range->start)
+			link = &(*link)->rb_left;
+		else if (start > range->start)
+			link = &(*link)->rb_right;
+		else {
+			printk(KERN_WARNING "Cannot use the same start value in more than one range!\n");
+			return;
+		}
+	}
+
+	rb_link_node(&new_range->node, parent, link);
+	rb_insert_color(&new_range->node, root);
+}
+
+static bool huge_addr_in_range(struct rb_root *root, u64 addr)
+{
+	struct rb_node *node = root->rb_node;
+	struct huge_addr_range *range;
+
+	while (node) {
+		range = container_of(node, struct huge_addr_range, node);
+
+		if (addr < range->start)
+			node = node->rb_left;
+		else if (addr >= range->start && addr < range->end)
+			return true;
+		else
+			node = node->rb_right;
+	}
+
+	return false;
+}
+
+static void huge_addr_free_tree(struct rb_root *root)
+{
+	struct huge_addr_range *range;
+	struct rb_node *node;
+
+	while ((node = rb_first(root))) {
+		range = container_of(node, struct huge_addr_range, node);
+		rb_erase(node, root);
+		kfree(range);
+	}
+}
+
+// Is huge_addr enabled for the huge page containing `address`?
+bool huge_addr_enabled(struct vm_area_struct *vma, unsigned long address)
+{
+	pid_t vma_owner_pid;
+	unsigned long fault_address_aligned = address & PMD_PAGE_MASK;
+
+	if (huge_addr_pid == 0 || (huge_addr == 0 && huge_addr_mode != 3)) {
+		return false;
+	}
+
+	// Check the PID of the faulting process...
+	rcu_read_lock();
+	vma_owner_pid = vma->vm_mm->owner->pid;
+	rcu_read_unlock();
+
+	if (vma_owner_pid != huge_addr_pid) {
+		return false;
+	}
+
+	// Check if the fault address's vma is large enough for a huge page.
+	if (vma->vm_start > fault_address_aligned ||
+	    vma->vm_end <= (fault_address_aligned + HPAGE_PMD_SIZE))
+	{
+		return false;
+	}
+
+	// Check if the fault address is within the huge region...
+	switch (huge_addr_mode) {
+		case 0:
+			if (huge_addr == fault_address_aligned) {
+				return true;
+			}
+			break;
+
+		case 1: // huge addr is highest, so true if lower
+			if (address < huge_addr) {
+				return true;
+			}
+			break;
+
+		case 2:
+			if (address >= huge_addr) {
+				return true;
+			}
+			break;
+
+		case 3:
+			if (huge_addr_in_range(&huge_addr_range_tree, address)) {
+				return true;
+			}
+			break;
+
+		default:
+			BUG();
+	}
+
+	// Any other case.
+	return false;
+}
+
+bool transparent_hugepage_enabled(struct vm_area_struct *vma, unsigned long address)
 {
 	/* The addr is used to check if the vma size fits */
 	unsigned long addr = (vma->vm_end & HPAGE_PMD_MASK) - HPAGE_PMD_SIZE;
@@ -70,7 +327,7 @@ bool transparent_hugepage_enabled(struct vm_area_struct *vma)
 	if (!transhuge_vma_suitable(vma, addr))
 		return false;
 	if (vma_is_anonymous(vma))
-		return __transparent_hugepage_enabled(vma);
+		return __transparent_hugepage_enabled(vma, address);
 	if (vma_is_shmem(vma))
 		return shmem_huge_enabled(vma);
 
@@ -280,6 +537,227 @@ static ssize_t defrag_store(struct kobject *kobj,
 static struct kobj_attribute defrag_attr =
 	__ATTR(defrag, 0644, defrag_show, defrag_store);
 
+static ssize_t huge_addr_pid_show(struct kobject *kobj,
+			   struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", huge_addr_pid);
+}
+
+static ssize_t huge_addr_pid_store(struct kobject *kobj,
+			       struct kobj_attribute *attr,
+			       const char *buf, size_t count)
+{
+	pid_t pid;
+	int ret;
+
+	ret = kstrtoint(buf, 0, &pid);
+
+	if (ret != 0) {
+		huge_addr_pid = 0;
+		return ret;
+	}
+	// Check that this is an existing process.
+	else if (find_vpid(pid) != NULL) {
+		huge_addr_pid = pid;
+		return count;
+	}
+	// Not a valid PID.
+	else {
+		huge_addr_pid = 0;
+		return -EINVAL;
+	}
+}
+static struct kobj_attribute huge_addr_pid_attr =
+	__ATTR(huge_addr_pid, 0644, huge_addr_pid_show, huge_addr_pid_store);
+
+static ssize_t huge_addr_comm_show(struct kobject *kobj,
+			   struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%s\n", huge_addr_comm);
+}
+
+static ssize_t huge_addr_comm_store(struct kobject *kobj,
+			       struct kobj_attribute *attr,
+			       const char *buf, size_t count)
+{
+	huge_addr_comm[MAX_HUGE_ADDR_COMM - 1] = 0;
+	strncpy(huge_addr_comm, buf, MAX_HUGE_ADDR_COMM-1);
+
+	return count;
+}
+static struct kobj_attribute huge_addr_comm_attr =
+	__ATTR(huge_addr_comm, 0644, huge_addr_comm_show, huge_addr_comm_store);
+
+static ssize_t huge_addr_mode_show(struct kobject *kobj,
+			   struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%d\n", huge_addr_mode);
+}
+
+static ssize_t huge_addr_mode_store(struct kobject *kobj,
+			       struct kobj_attribute *attr,
+			       const char *buf, size_t count)
+{
+	int mode;
+	int ret;
+
+	ret = kstrtoint(buf, 0, &mode);
+
+	if (ret != 0) {
+		huge_addr_mode = 0;
+		return ret;
+	}
+	else if (mode >= 0 && mode <= 3) {
+		huge_addr_mode = mode;
+		return count;
+	}
+	else {
+		huge_addr_mode = 0;
+		return -EINVAL;
+	}
+}
+static struct kobj_attribute huge_addr_mode_attr =
+	__ATTR(huge_addr_mode, 0644, huge_addr_mode_show, huge_addr_mode_store);
+
+static ssize_t huge_addr_show(struct kobject *kobj,
+			   struct kobj_attribute *attr, char *buf)
+{
+	if (huge_addr_mode == 3) {
+		int write_cnt = 0;
+		struct huge_addr_range *range;
+		struct rb_node *node = rb_first(&huge_addr_range_tree);
+
+		while (node) {
+			range = container_of(node, struct huge_addr_range, node);
+
+			write_cnt += sprintf(&buf[write_cnt], "0x%llx 0x%llx;",
+				range->start, range->end);
+
+			node = rb_next(node);
+		}
+		//Replace the last ';' with a newline
+		buf[write_cnt - 1] = '\n';
+
+		return write_cnt;
+	} else {
+		return sprintf(buf, "0x%llx\n", huge_addr);
+	}
+}
+
+static void try_huge_addr_promote(pid_t pid, u64 addr)
+{
+	struct mm_struct *mm;
+	struct vm_area_struct *vma;
+	struct task_struct *target_task = get_pid_task(find_get_pid(pid), PIDTYPE_PID);
+	struct mm_stats_pftrace pftrace; // dummy, not used
+	mm_stats_pftrace_init(&pftrace);
+
+	if (!target_task) {
+		pr_info("no such pid for promotion");
+		return;
+	}
+
+	mm = target_task->mm;
+	vma = find_vma(mm, addr);
+
+	promote_to_huge(mm, vma, addr, &pftrace);
+
+	put_task_struct(target_task);
+}
+
+static ssize_t huge_addr_store(struct kobject *kobj,
+			       struct kobj_attribute *attr,
+			       const char *buf, size_t count)
+{
+	u64 addr;
+	int ret;
+
+	if (huge_addr_mode == 3) {
+		char *tok = (char *)buf;
+		struct rb_root new_tree = RB_ROOT;
+		struct huge_addr_range *range = NULL;
+		ssize_t error;
+
+		// Try to read in all of the ranges
+		while (tok) {
+			char *addr_buf;
+
+			range = kmalloc(sizeof(struct huge_addr_range), GFP_KERNEL);
+			if (!range) {
+				error = -ENOMEM;
+				goto err;
+			}
+
+			// Get the beginning of the range
+			addr_buf = strsep(&tok, " ");
+			if (!addr_buf) {
+				error = -EINVAL;
+				goto err;
+			}
+
+			ret = kstrtoull(addr_buf, 0, &addr);
+			if (ret != 0) {
+				error = -EINVAL;
+				goto err;
+			}
+
+			range->start = addr;
+
+			// Get the end of the range
+			addr_buf = strsep(&tok, ";");
+			if (!addr_buf) {
+				error = -EINVAL;
+				goto err;
+			}
+
+			ret = kstrtoull(addr_buf, 0, &addr);
+			if (ret != 0) {
+				error = -EINVAL;
+				goto err;
+			}
+
+			range->end = addr;
+
+			huge_addr_range_insert(&new_tree, range);
+		}
+
+		//Free the old tree if it exists
+		huge_addr_free_tree(&huge_addr_range_tree);
+		//Set the new tree
+		huge_addr_range_tree = new_tree;
+
+		return count;
+
+err:
+		if (range)
+			kfree(range);
+		huge_addr_free_tree(&new_tree);
+		return error;
+	} else {
+		ret = kstrtoull(buf, 0, &addr);
+
+		if (ret != 0) {
+			huge_addr = 0;
+			return ret;
+		} else if ((addr & PMD_PAGE_MASK) == addr) {
+			huge_addr = addr;
+
+			// If the pid is set, the we should check if the page is
+			// already mapped. If so, then we should promote it.
+			if (huge_addr_pid != 0) {
+				try_huge_addr_promote(huge_addr_pid, huge_addr);
+			}
+
+			return count;
+		} else {
+			huge_addr = 0;
+			return -EINVAL;
+		}
+	}
+}
+static struct kobj_attribute huge_addr_attr =
+	__ATTR(huge_addr, 0644, huge_addr_show, huge_addr_store);
+
 static ssize_t use_zero_page_show(struct kobject *kobj,
 		struct kobj_attribute *attr, char *buf)
 {
@@ -332,6 +810,10 @@ static struct attribute *hugepage_attr[] = {
 #ifdef CONFIG_DEBUG_VM
 	&debug_cow_attr.attr,
 #endif
+	&huge_addr_attr.attr,
+	&huge_addr_pid_attr.attr,
+	&huge_addr_comm_attr.attr,
+	&huge_addr_mode_attr.attr,
 	NULL,
 };
 
@@ -572,8 +1054,13 @@ unsigned long thp_get_unmapped_area(struct file *filp, unsigned long addr,
 }
 EXPORT_SYMBOL_GPL(thp_get_unmapped_area);
 
+// markm: defined in page_alloc.c... but I don't want to touch one of the big
+// header files and have to recompile everything...
+inline void lfpa_update(u64);
+
 static vm_fault_t __do_huge_pmd_anonymous_page(struct vm_fault *vmf,
-			struct page *page, gfp_t gfp)
+			struct page *page, gfp_t gfp,
+		        struct mm_stats_pftrace *pftrace)
 {
 	struct vm_area_struct *vma = vmf->vma;
 	struct mem_cgroup *memcg;
@@ -595,7 +1082,13 @@ static vm_fault_t __do_huge_pmd_anonymous_page(struct vm_fault *vmf,
 		goto release;
 	}
 
-	clear_huge_page(page, vmf->address, HPAGE_PMD_NR);
+	pftrace->prep_start_tsc = rdtsc();
+	lfpa_update(rdtsc());
+	if(PageZeroed(page))
+		mm_stats_set_flag(pftrace, MM_STATS_PF_ALLOC_PREZEROED);
+	else
+		clear_huge_page(page, vmf->address, HPAGE_PMD_NR);
+	pftrace->prep_end_tsc = rdtsc();
 	/*
 	 * The memory barrier inside __SetPageUptodate makes sure that
 	 * clear_huge_page writes become visible before the set_pmd_at()
@@ -632,6 +1125,12 @@ static vm_fault_t __do_huge_pmd_anonymous_page(struct vm_fault *vmf,
 		mem_cgroup_commit_charge(page, memcg, false, true);
 		lru_cache_add_active_or_unevictable(page, vma);
 		pgtable_trans_huge_deposit(vma->vm_mm, vmf->pmd, pgtable);
+		/* Make the page table entry as reserved for TLB miss tracking */
+		if(is_badger_trap_enabled(vma->vm_mm, haddr)
+			&& !(vmf->flags & FAULT_FLAG_INSTRUCTION))
+		{
+			entry = pmd_mkreserve(entry);
+		}
 		set_pmd_at(vma->vm_mm, haddr, vmf->pmd, entry);
 		add_mm_counter(vma->vm_mm, MM_ANONPAGES, HPAGE_PMD_NR);
 		mm_inc_nr_ptes(vma->vm_mm);
@@ -661,10 +1160,33 @@ static vm_fault_t __do_huge_pmd_anonymous_page(struct vm_fault *vmf,
  *	    available
  * never: never stall for any thp allocation
  */
-static inline gfp_t alloc_hugepage_direct_gfpmask(struct vm_area_struct *vma)
+static inline gfp_t alloc_hugepage_direct_gfpmask(
+		struct vm_area_struct *vma,
+		unsigned long haddr)
 {
 	const bool vma_madvised = !!(vma->vm_flags & VM_HUGEPAGE);
 
+	if (mm_econ_is_on()) {
+		struct mm_cost_delta mm_cost_delta;
+		struct mm_action mm_action;
+		bool should_do;
+
+		mm_action.address = haddr;
+		mm_action.action = MM_ACTION_ALLOC_RECLAIM;
+		mm_action.huge_page_order = HPAGE_PUD_SHIFT-PAGE_SHIFT;
+		mm_estimate_changes(&mm_action, &mm_cost_delta);
+		should_do = mm_decide(&mm_cost_delta);
+
+		// TODO: Also eval if __GFP_KSWAPD_RECLAIM is worth it...
+		// perhaps if value of THP is high but cost of direct reclaim
+		// is too high.
+		if (should_do) {
+			return GFP_TRANSHUGE_LIGHT | __GFP_DIRECT_RECLAIM;
+		} else {
+			return GFP_TRANSHUGE_LIGHT;
+		}
+	}
+
 	/* Always do synchronous compaction */
 	if (test_bit(TRANSPARENT_HUGEPAGE_DEFRAG_DIRECT_FLAG, &transparent_hugepage_flags))
 		return GFP_TRANSHUGE | (vma_madvised ? 0 : __GFP_NORETRY);
@@ -699,17 +1221,28 @@ static bool set_huge_zero_page(pgtable_t pgtable, struct mm_struct *mm,
 	entry = pmd_mkhuge(entry);
 	if (pgtable)
 		pgtable_trans_huge_deposit(mm, pmd, pgtable);
+	/* Make the page table entry as reserved for TLB miss tracking
+	 * No need to worry for zero page with instruction faults.
+	 * Instruction faults will never reach here.
+	 */
+	if(is_badger_trap_enabled(mm, haddr)) {
+		entry = pmd_mkreserve(entry);
+	}
 	set_pmd_at(mm, haddr, pmd, entry);
 	mm_inc_nr_ptes(mm);
 	return true;
 }
 
-vm_fault_t do_huge_pmd_anonymous_page(struct vm_fault *vmf)
+vm_fault_t do_huge_pmd_anonymous_page(struct vm_fault *vmf,
+				      struct mm_stats_pftrace *pftrace,
+				      bool require_prezeroed)
 {
 	struct vm_area_struct *vma = vmf->vma;
 	gfp_t gfp;
 	struct page *page;
 	unsigned long haddr = vmf->address & HPAGE_PMD_MASK;
+	vm_fault_t ret;
+	u64 start = rdtsc();
 
 	if (!transhuge_vma_suitable(vma, haddr))
 		return VM_FAULT_FALLBACK;
@@ -723,7 +1256,6 @@ vm_fault_t do_huge_pmd_anonymous_page(struct vm_fault *vmf)
 		pgtable_t pgtable;
 		struct page *zero_page;
 		bool set;
-		vm_fault_t ret;
 		pgtable = pte_alloc_one(vma->vm_mm);
 		if (unlikely(!pgtable))
 			return VM_FAULT_OOM;
@@ -754,16 +1286,51 @@ vm_fault_t do_huge_pmd_anonymous_page(struct vm_fault *vmf)
 			spin_unlock(vmf->ptl);
 		if (!set)
 			pte_free(vma->vm_mm, pgtable);
+		else {
+			mm_stats_set_flag(pftrace, MM_STATS_PF_ZERO);
+			mm_stats_hist_measure(
+				&mm_huge_page_fault_zero_page_cycles,
+				rdtsc() - start);
+		}
 		return ret;
 	}
-	gfp = alloc_hugepage_direct_gfpmask(vma);
+	pftrace->alloc_start_tsc = rdtsc();
+	gfp = alloc_hugepage_direct_gfpmask(vma, haddr);
 	page = alloc_hugepage_vma(gfp, vma, haddr, HPAGE_PMD_ORDER);
+	pftrace->alloc_end_tsc = rdtsc();
+	mm_stats_check_alloc_fallback(pftrace);
+	mm_stats_check_alloc_zeroing(pftrace);
 	if (unlikely(!page)) {
+		mm_stats_set_flag(pftrace, MM_STATS_PF_HUGE_ALLOC_FAILED);
+		count_vm_event(THP_FAULT_FALLBACK);
+		return VM_FAULT_FALLBACK;
+	} else if (mm_econ_is_on() && require_prezeroed && !PageZeroed(page)) {
+		// HACK (markm): If the estimator assumed that a prezeroed page
+		// would be availabe, make sure that we don't use an unzeroed
+		// page. The correct way to do this would be to fix the way we
+		// estimate if a prezeroed page is available or to fix the page
+		// allocator to be more predictable. However, after some time
+		// trying, the existing allocator is a bit convoluted and we
+		// have a deadline, so....
+
+		if (mm_econ_debugging_mode == 3) {
+			pr_warn("estimator: conflict page %p", page);
+		}
+
+		// Free the page to avoid a leak.
+		__free_pages(page, HPAGE_PMD_ORDER);
+
+		// Act like a failure.
+		mm_stats_set_flag(pftrace, MM_STATS_PF_HUGE_ALLOC_FAILED);
 		count_vm_event(THP_FAULT_FALLBACK);
 		return VM_FAULT_FALLBACK;
 	}
 	prep_transhuge_page(page);
-	return __do_huge_pmd_anonymous_page(vmf, page, gfp);
+	ret = __do_huge_pmd_anonymous_page(vmf, page, gfp, pftrace);
+
+	mm_stats_hist_measure(&mm_huge_page_fault_create_new_cycles, rdtsc() - start);
+
+	return ret;
 }
 
 static void insert_pfn_pmd(struct vm_area_struct *vma, unsigned long addr,
@@ -1038,7 +1605,7 @@ int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		 * reference.
 		 */
 		zero_page = mm_get_huge_zero_page(dst_mm);
-		set_huge_zero_page(pgtable, dst_mm, vma, addr, dst_pmd,
+		set_huge_zero_page(pgtable, dst_mm, vma, addr, dst_pmd, // TODO markm do we need something here?
 				zero_page);
 		ret = 0;
 		goto out_unlock;
@@ -1054,7 +1621,7 @@ int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 
 	pmdp_set_wrprotect(src_mm, addr, src_pmd);
 	pmd = pmd_mkold(pmd_wrprotect(pmd));
-	set_pmd_at(dst_mm, addr, dst_pmd, pmd);
+	set_pmd_at(dst_mm, addr, dst_pmd, pmd); // TODO markm do we need something here
 
 	ret = 0;
 out_unlock:
@@ -1143,7 +1710,7 @@ int copy_huge_pud(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 
 	pudp_set_wrprotect(src_mm, addr, src_pud);
 	pud = pud_mkold(pud_wrprotect(pud));
-	set_pud_at(dst_mm, addr, dst_pud, pud);
+	set_pud_at(dst_mm, addr, dst_pud, pud); // TODO markm maybe need something here?
 
 	ret = 0;
 out_unlock:
@@ -1277,6 +1844,16 @@ static vm_fault_t do_huge_pmd_wp_page_fallback(struct vm_fault *vmf,
 		lru_cache_add_active_or_unevictable(pages[i], vma);
 		vmf->pte = pte_offset_map(&_pmd, haddr);
 		VM_BUG_ON(!pte_none(*vmf->pte));
+		/* Make the page table entry as reserved for TLB miss tracking */
+		if(is_badger_trap_enabled(vma->vm_mm, haddr)
+				&& !(vmf->flags & FAULT_FLAG_INSTRUCTION))
+		{
+			// TODO markm: handle the general case... right now, we
+			// only handle anon pages.
+			if (vma_is_anonymous(vma)) {
+				entry = pte_mkreserve(entry);
+			}
+		}
 		set_pte_at(vma->vm_mm, haddr, vmf->pte, entry);
 		pte_unmap(vmf->pte);
 	}
@@ -1312,7 +1889,8 @@ static vm_fault_t do_huge_pmd_wp_page_fallback(struct vm_fault *vmf,
 	goto out;
 }
 
-vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd)
+vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd,
+			       struct mm_stats_pftrace *pftrace)
 {
 	struct vm_area_struct *vma = vmf->vma;
 	struct page *page = NULL, *new_page;
@@ -1321,6 +1899,7 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd)
 	struct mmu_notifier_range range;
 	gfp_t huge_gfp;			/* for allocation and charge */
 	vm_fault_t ret = 0;
+	u64 start = rdtsc();
 
 	vmf->ptl = pmd_lockptr(vma->vm_mm, vmf->pmd);
 	VM_BUG_ON_VMA(!vma->anon_vma, vma);
@@ -1362,10 +1941,17 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd)
 	get_page(page);
 	spin_unlock(vmf->ptl);
 alloc:
-	if (__transparent_hugepage_enabled(vma) &&
+	if (__transparent_hugepage_enabled(vma, vmf->address) &&
 	    !transparent_hugepage_debug_cow()) {
-		huge_gfp = alloc_hugepage_direct_gfpmask(vma);
+		pftrace->alloc_start_tsc = rdtsc();
+		huge_gfp = alloc_hugepage_direct_gfpmask(vma, haddr);
 		new_page = alloc_hugepage_vma(huge_gfp, vma, haddr, HPAGE_PMD_ORDER);
+		pftrace->alloc_end_tsc = rdtsc();
+		mm_stats_check_alloc_fallback(pftrace);
+		mm_stats_check_alloc_zeroing(pftrace);
+		if (unlikely(!new_page)) {
+			mm_stats_set_flag(pftrace, MM_STATS_PF_HUGE_ALLOC_FAILED);
+		}
 	} else
 		new_page = NULL;
 
@@ -1374,11 +1960,13 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd)
 	} else {
 		if (!page) {
 			split_huge_pmd(vma, vmf->pmd, vmf->address);
+			mm_stats_set_flag(pftrace, MM_STATS_PF_HUGE_SPLIT);
 			ret |= VM_FAULT_FALLBACK;
 		} else {
 			ret = do_huge_pmd_wp_page_fallback(vmf, orig_pmd, page);
 			if (ret & VM_FAULT_OOM) {
 				split_huge_pmd(vma, vmf->pmd, vmf->address);
+				mm_stats_set_flag(pftrace, MM_STATS_PF_HUGE_SPLIT);
 				ret |= VM_FAULT_FALLBACK;
 			}
 			put_page(page);
@@ -1391,6 +1979,7 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd)
 					huge_gfp, &memcg, true))) {
 		put_page(new_page);
 		split_huge_pmd(vma, vmf->pmd, vmf->address);
+		mm_stats_set_flag(pftrace, MM_STATS_PF_HUGE_SPLIT);
 		if (page)
 			put_page(page);
 		ret |= VM_FAULT_FALLBACK;
@@ -1401,11 +1990,13 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd)
 	count_vm_event(THP_FAULT_ALLOC);
 	count_memcg_events(memcg, THP_FAULT_ALLOC, 1);
 
+	pftrace->prep_start_tsc = rdtsc();
 	if (!page)
 		clear_huge_page(new_page, vmf->address, HPAGE_PMD_NR);
 	else
 		copy_user_huge_page(new_page, page, vmf->address,
 				    vma, HPAGE_PMD_NR);
+	pftrace->prep_end_tsc = rdtsc();
 	__SetPageUptodate(new_page);
 
 	mmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, vma->vm_mm,
@@ -1426,6 +2017,13 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd)
 		entry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);
 		pmdp_huge_clear_flush_notify(vma, haddr, vmf->pmd);
 		page_add_new_anon_rmap(new_page, vma, haddr, true);
+		/* Make the page table entry as reserved for TLB miss tracking */
+		if(is_badger_trap_enabled(vma->vm_mm, haddr)
+				&& !(vmf->flags & FAULT_FLAG_INSTRUCTION))
+		{
+			// Can only get anon mappings here...
+			entry = pmd_mkreserve(entry);
+		}
 		mem_cgroup_commit_charge(new_page, memcg, false, true);
 		lru_cache_add_active_or_unevictable(new_page, vma);
 		set_pmd_at(vma->vm_mm, haddr, vmf->pmd, entry);
@@ -1447,6 +2045,7 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd)
 	 */
 	mmu_notifier_invalidate_range_only_end(&range);
 out:
+	mm_stats_hist_measure(&mm_huge_page_fault_wp_cycles, rdtsc() - start);
 	return ret;
 out_unlock:
 	spin_unlock(vmf->ptl);
@@ -1782,6 +2381,11 @@ int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,
 	ptl = __pmd_trans_huge_lock(pmd, vma);
 	if (!ptl)
 		return 0;
+
+	if (vma->vm_mm->badger_trap_was_enabled && is_pmd_reserved(*pmd)) {
+		*pmd = pmd_unreserve(*pmd);
+	}
+
 	/*
 	 * For architectures like ppc64 we look at deposited pgtable
 	 * when calling pmdp_huge_get_and_clear. So do the
@@ -2047,6 +2651,11 @@ int zap_huge_pud(struct mmu_gather *tlb, struct vm_area_struct *vma,
 	ptl = __pud_trans_huge_lock(pud, vma);
 	if (!ptl)
 		return 0;
+
+	if (vma->vm_mm->badger_trap_was_enabled && is_pud_reserved(*pud)) {
+		*pud = pud_unreserve(*pud);
+	}
+
 	/*
 	 * For architectures like ppc64 we look at deposited pgtable
 	 * when calling pudp_huge_get_and_clear. So do the
@@ -2130,6 +2739,13 @@ static void __split_huge_zero_page_pmd(struct vm_area_struct *vma,
 		entry = pte_mkspecial(entry);
 		pte = pte_offset_map(&_pmd, haddr);
 		VM_BUG_ON(!pte_none(*pte));
+		/* Make the page table entry as reserved for TLB miss tracking
+		 * No need to worry for zero page with instruction faults.
+		 * Instruction faults will never reach here.
+		 */
+		if(is_badger_trap_enabled(mm, haddr)) {
+			entry = pte_mkreserve(entry);
+		}
 		set_pte_at(mm, haddr, pte, entry);
 		pte_unmap(pte);
 	}
@@ -2144,7 +2760,8 @@ static void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,
 	struct page *page;
 	pgtable_t pgtable;
 	pmd_t old_pmd, _pmd;
-	bool young, write, soft_dirty, pmd_migration = false;
+	bool young, write, soft_dirty, pmd_migration = false,
+	     is_old_reserved = is_pmd_reserved(*pmd);
 	unsigned long addr;
 	int i;
 
@@ -2262,6 +2879,16 @@ static void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,
 		}
 		pte = pte_offset_map(&_pmd, addr);
 		BUG_ON(!pte_none(*pte));
+		/*
+		 * Make the page table entry as reserved for TLB miss tracking
+		 * if the PMD was marked as reserved.
+		 */
+		if(is_old_reserved && mm->badger_trap_enabled) {
+			// TODO markm currently only handle anon memory
+			if (vma_is_anonymous(vma)) {
+				entry = pte_mkreserve(entry);
+			}
+		}
 		set_pte_at(mm, addr, pte, entry);
 		atomic_inc(&page[i]._mapcount);
 		pte_unmap(pte);
@@ -3070,7 +3697,7 @@ void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)
 		page_add_anon_rmap(new, vma, mmun_start, true);
 	else
 		page_add_file_rmap(new, true);
-	set_pmd_at(mm, mmun_start, pvmw->pmd, pmde);
+	set_pmd_at(mm, mmun_start, pvmw->pmd, pmde); // TODO markm do we need somethign ehre?
 	if ((vma->vm_flags & VM_LOCKED) && !PageDoubleMap(new))
 		mlock_vma_page(new);
 	update_mmu_cache_pmd(vma, address, pvmw->pmd);
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index dd8737a94bec..827d60f8e9f2 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -28,6 +28,7 @@
 #include <linux/jhash.h>
 #include <linux/numa.h>
 #include <linux/llist.h>
+#include <linux/badger_trap.h>
 
 #include <asm/page.h>
 #include <asm/pgtable.h>
@@ -3607,9 +3608,10 @@ static void unmap_ref_private(struct mm_struct *mm, struct vm_area_struct *vma,
  */
 static vm_fault_t hugetlb_cow(struct mm_struct *mm, struct vm_area_struct *vma,
 		       unsigned long address, pte_t *ptep,
-		       struct page *pagecache_page, spinlock_t *ptl)
+		       struct page *pagecache_page, spinlock_t *ptl,
+		       unsigned int flags)
 {
-	pte_t pte;
+	pte_t pte, new_entry;
 	struct hstate *h = hstate_vma(vma);
 	struct page *old_page, *new_page;
 	int outside_reserve = 0;
@@ -3709,8 +3711,14 @@ static vm_fault_t hugetlb_cow(struct mm_struct *mm, struct vm_area_struct *vma,
 		/* Break COW */
 		huge_ptep_clear_flush(vma, haddr, ptep);
 		mmu_notifier_invalidate_range(mm, range.start, range.end);
-		set_huge_pte_at(mm, haddr, ptep,
-				make_huge_pte(vma, new_page, 1));
+		new_entry = make_huge_pte(vma, new_page, 1);
+		/* Make the page table entry as reserved for TLB miss tracking */
+		if(is_badger_trap_enabled(mm, haddr)
+				&& !(flags & FAULT_FLAG_INSTRUCTION))
+		{
+			// new_entry = pte_mkreserve(new_entry); // TODO markm uncomment
+		}
+		set_huge_pte_at(mm, haddr, ptep, new_entry);
 		page_remove_rmap(old_page, true);
 		hugepage_add_new_anon_rmap(new_page, vma, haddr);
 		set_page_huge_active(new_page);
@@ -3940,12 +3948,18 @@ static vm_fault_t hugetlb_no_page(struct mm_struct *mm,
 		page_dup_rmap(page, true);
 	new_pte = make_huge_pte(vma, page, ((vma->vm_flags & VM_WRITE)
 				&& (vma->vm_flags & VM_SHARED)));
+
+	/* Make the page table entry as reserved for TLB miss tracking */
+	if(is_badger_trap_enabled(mm, haddr) && !(flags & FAULT_FLAG_INSTRUCTION)) {
+		// new_pte = pte_mkreserve(new_pte);// TODO markm uncomment
+	}
+
 	set_huge_pte_at(mm, haddr, ptep, new_pte);
 
 	hugetlb_count_add(pages_per_huge_page(h), mm);
 	if ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {
 		/* Optimization, do the COW without a second fault */
-		ret = hugetlb_cow(mm, vma, address, ptep, page, ptl);
+		ret = hugetlb_cow(mm, vma, address, ptep, page, ptl, flags);
 	}
 
 	spin_unlock(ptl);
@@ -3971,6 +3985,65 @@ static vm_fault_t hugetlb_no_page(struct mm_struct *mm,
 	goto out;
 }
 
+static int hugetlb_fake_fault(struct mm_struct *mm, struct vm_area_struct *vma,
+                        unsigned long address, pte_t *page_table, unsigned int flags)
+{
+        unsigned long *touch_page_addr;
+        unsigned long touched;
+        unsigned long ret;
+        static unsigned int consecutive = 0;
+        static unsigned long prev_address = 0;
+
+        if(address == prev_address)
+                consecutive++;
+        else
+        {
+                consecutive = 0;
+                prev_address = address;
+        }
+
+        if(consecutive > 1)
+        {
+                *page_table = pte_unreserve(*page_table);
+                return 0;
+        }
+
+        if(flags & FAULT_FLAG_WRITE)
+                *page_table = huge_pte_mkdirty(*page_table);
+
+        *page_table = pte_mkyoung(*page_table);
+        *page_table = pte_unreserve(*page_table);
+
+	/* Touch the page to get the translation into the TLB before we add the
+	 * reserved flag back to the page table entry.
+	 */
+        touch_page_addr = (void *)(address & PAGE_MASK);
+        ret = copy_from_user(&touched,
+		(__force const void __user *)touch_page_addr,
+		sizeof(unsigned long));
+
+	if(ret)
+		return VM_FAULT_SIGBUS;
+
+	/* Here where we do all our analysis */
+	if (flags & FAULT_FLAG_WRITE)
+	    atomic64_inc(&mm->bt_stats->total_dtlb_2mb_store_misses);
+	else
+	    atomic64_inc(&mm->bt_stats->total_dtlb_2mb_load_misses);
+
+	/*
+	if (vma) {
+		if (flags & FAULT_FLAG_WRITE)
+		    vma->bt_stats.total_dtlb_2mb_store_misses++;
+		else
+		    vma->bt_stats.total_dtlb_2mb_load_misses++;
+	}
+	*/
+
+	*page_table = pte_mkreserve(*page_table);
+	return 0;
+}
+
 #ifdef CONFIG_SMP
 u32 hugetlb_fault_mutex_hash(struct address_space *mapping, pgoff_t idx)
 {
@@ -4011,6 +4084,47 @@ vm_fault_t hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 	unsigned long haddr = address & huge_page_mask(h);
 
 	ptep = huge_pte_offset(mm, haddr, huge_page_size(h));
+
+	/*
+	 * Here we check for Huge page that are marked as reserved.
+	 *
+	 * As of Linux 5.5.8, hugetlb doesn't apparently support 1GB pages.
+	 */
+	if(mm && mm->badger_trap_was_enabled
+		&& !(flags & FAULT_FLAG_INSTRUCTION) && ptep)
+	{
+		mapping = vma->vm_file->f_mapping;
+		idx = vma_hugecache_offset(h, vma, haddr);
+		hash = hugetlb_fault_mutex_hash(mapping, idx);
+		mutex_lock(&hugetlb_fault_mutex_table[hash]);
+		entry = huge_ptep_get(ptep);
+		if((flags & FAULT_FLAG_WRITE) && is_pte_reserved(entry)
+			&& !huge_pte_write(entry) && pte_present(entry))
+		{
+			ptl = huge_pte_lock(h, mm, ptep);
+		        page = pte_page(entry);
+			if (page != pagecache_page)
+				if (!trylock_page(page)) {
+					need_wait_lock = 1;
+					ret = 0;
+					goto out_ptl;
+				}
+		        get_page(page);
+		        spin_lock(&mm->page_table_lock);
+			ret = hugetlb_cow(mm, vma, address, ptep,
+                                                 pagecache_page, ptl, flags);
+			goto out_put_page;
+		}
+		if(is_pte_reserved(entry) && pte_present(entry)) {
+			ret = hugetlb_fake_fault(mm, vma, address, ptep, flags);
+			goto out_mutex;
+		}
+		if(pte_present(entry) && is_badger_trap_enabled(mm, address)) {
+			//*ptep = pte_mkreserve(*ptep);  // TODO markm uncomment
+		}
+		mutex_unlock(&hugetlb_fault_mutex_table[hash]);
+	}
+
 	if (ptep) {
 		entry = huge_ptep_get(ptep);
 		if (unlikely(is_hugetlb_entry_migration(entry))) {
@@ -4098,7 +4212,7 @@ vm_fault_t hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 	if (flags & FAULT_FLAG_WRITE) {
 		if (!huge_pte_write(entry)) {
 			ret = hugetlb_cow(mm, vma, address, ptep,
-					  pagecache_page, ptl);
+					  pagecache_page, ptl, flags);
 			goto out_put_page;
 		}
 		entry = huge_pte_mkdirty(entry);
diff --git a/mm/internal.h b/mm/internal.h
index 3cf20ab3ca01..96577644f23e 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -34,7 +34,7 @@
 
 void page_writeback_init(void);
 
-vm_fault_t do_swap_page(struct vm_fault *vmf);
+vm_fault_t do_swap_page(struct vm_fault *vmf, struct mm_stats_pftrace *pftrace);
 
 void free_pgtables(struct mmu_gather *tlb, struct vm_area_struct *start_vma,
 		unsigned long floor, unsigned long ceiling);
diff --git a/mm/kbadgerd/Kconfig b/mm/kbadgerd/Kconfig
new file mode 100644
index 000000000000..10af8f85a789
--- /dev/null
+++ b/mm/kbadgerd/Kconfig
@@ -0,0 +1,8 @@
+# SPDX-License-Identifier: GPL-2.0-only
+
+config KBADGERD
+	tristate "Adds the kbadgerd module."
+	default m
+	depends on MEMCG && TRANSPARENT_HUGEPAGE
+	help
+	  Adds the kbadgerd module.
diff --git a/mm/kbadgerd/Makefile b/mm/kbadgerd/Makefile
new file mode 100644
index 000000000000..cb10c4aa762b
--- /dev/null
+++ b/mm/kbadgerd/Makefile
@@ -0,0 +1,6 @@
+# SPDX-License-Identifier: GPL-2.0
+#
+# Makefile for kbadgerd
+#
+
+obj-$(CONFIG_KBADGERD) := kbadgerd.o
diff --git a/mm/kbadgerd/kbadgerd-plot.py b/mm/kbadgerd/kbadgerd-plot.py
new file mode 100644
index 000000000000..452b616d5764
--- /dev/null
+++ b/mm/kbadgerd/kbadgerd-plot.py
@@ -0,0 +1,188 @@
+#!/usr/bin/env python3
+
+import matplotlib
+matplotlib.use("Agg")
+
+import matplotlib.pyplot as plt
+import matplotlib.patches as patches
+import matplotlib.ticker as ticker
+import numpy as np
+import sys
+import re
+
+FILE = sys.argv[1]
+
+REGIONRE = "^.*kbadgerd: \[(.*), (.*)\) .*$"
+DATARE = "^.*kbadgerd:\s+((4KB)|(2MB)) ((load)|(store)) misses: (.*)$"
+
+def get_data(fname):
+    began = False
+
+    data = []
+
+    start = None
+    end = None
+    misses_ld_4kb = 0
+    misses_st_4kb = 0
+    misses_ld_2mb = 0
+    misses_st_2mb = 0
+
+    with open(fname) as f:
+        for line in f.readlines():
+            if "Results of inspection" in line:
+                began = True
+                continue
+
+            if not began:
+                continue
+
+            match = re.search(REGIONRE, line)
+            if match is not None:
+                if start is not None:
+                    data.append((start, end, misses_ld_4kb, misses_st_4kb, misses_ld_2mb, misses_st_2mb))
+
+                start = int(match.group(1), 16)
+                end = int(match.group(2), 16)
+
+                misses_ld_4kb = 0
+                misses_st_4kb = 0
+                misses_ld_2mb = 0
+                misses_st_2mb = 0
+
+                continue
+
+            if "No misses" in line:
+                continue
+
+            match = re.search(DATARE, line)
+            if match is not None:
+                size = match.group(1)
+                ldst = match.group(4)
+                count = int(match.group(7))
+
+                if (size, ldst) == ("4KB", "load"):
+                    misses_ld_4kb = count
+                elif (size, ldst) == ("4KB", "store"):
+                    misses_st_4kb = count
+                elif (size, ldst) == ("2MB", "load"):
+                    misses_ld_2mb = count
+                elif (size, ldst) == ("2MB", "store"):
+                    misses_st_2mb = count
+                else:
+                    raise Exception("Should never happen.")
+
+                continue
+
+    return data
+
+# raw data in the form of a bunch of tuples:
+#   (start, end, counts...)
+#
+# The ranges may be overlapping though, so we need to handle that case.
+data = sorted(get_data(FILE), key=lambda x: x[0])
+
+for x in data:
+    print([hex(v) for v in x])
+
+fig, axs = plt.subplots(4, sharex=True, figsize=(15, 15))
+
+minx = float("inf")
+maxx = 0
+
+# The plot is very sparse. Keep track of the gaps and don't draw them.
+#   (start, length)
+gaps = []
+
+for (start, end, _ld4k, _st4k, _ld2m, _st2m) in data:
+    if start > maxx:
+        gap = (maxx, start - maxx)
+        gaps.append(gap)
+        print("gap: %x %x" % (maxx, start))
+
+    minx = min(minx, start)
+    maxx = max(maxx, end)
+
+def get_group(x, i):
+    return x[i + 2]
+
+def plot_group(i):
+    ax = axs[i]
+
+    maxy = max(map(lambda x: get_group(x, i), data))
+
+    gap_idx = 0 # index of the first gap _greater than_ start
+    gap_offset = 0 # sum of all gap lengths prior to start
+    for x in data:
+        start = x[0]
+        end = x[1]
+        count = get_group(x, i)
+        if gap_idx < len(gaps) and gaps[gap_idx][0] < start:
+            gap_offset += gaps[gap_idx][1]
+            gap_idx += 1
+
+        #print("start=%x new=%x" % (start, start-gap_offset))
+
+        rect = patches.Rectangle((start - gap_offset, 0), end-start, count,
+                fc=(0,0,1, 0.1), ec="r", linewidth=0.25)
+        ax.add_patch(rect)
+
+    label = ""
+    if i == 0:
+        label = "4KB load misses"
+    elif i == 1:
+        label = "4KB store misses"
+    elif i == 2:
+        label = "2MB load misses"
+    else:
+        label = "2MB store misses"
+
+    ax.set_ylabel(label)
+    ax.set_ylim((1, maxy))
+    ax.set_yscale("symlog")
+
+    ax.set_xlim((minx, maxx - gap_offset))
+
+    ax.grid(True)
+
+plot_group(0)
+plot_group(1)
+plot_group(2)
+plot_group(3)
+
+def to_hex(x, pos):
+    x = int(x)
+
+    offset = 0
+
+    prevs, prevl = None, None
+    for ((s, l), (nexts, nextl)) in zip(gaps[:-1], gaps[1:]):
+        offset += l
+        if x < nexts - offset:
+            break
+
+        #print("x=%x s=%x l=%x o=%x" % (x, s, l, offset))
+    #print("%x\n" % (x + offset))
+    return '%x' % (x + offset)
+
+def compute_tick_spacing():
+    start, end = axs[3].get_xlim()
+    width = end - start
+
+    NTICKS = 40
+
+    raw_interval = int(width / (NTICKS - 1))
+    interval = (raw_interval >> 20) << 20
+
+    if interval == 0:
+        interval = (raw_interval >> 10) << 10
+
+    return interval
+
+axs[3].get_xaxis().set_major_formatter(ticker.FuncFormatter(to_hex))
+axs[3].get_xaxis().set_major_locator(ticker.MultipleLocator(compute_tick_spacing()))
+plt.setp(axs[3].xaxis.get_majorticklabels(), rotation=60)
+axs[3].set_xlabel("Address")
+
+plt.tight_layout()
+
+plt.savefig("%s.pdf" % FILE)
diff --git a/mm/kbadgerd/kbadgerd.c b/mm/kbadgerd/kbadgerd.c
new file mode 100644
index 000000000000..6f8238331121
--- /dev/null
+++ b/mm/kbadgerd/kbadgerd.c
@@ -0,0 +1,1184 @@
+/*
+ * Implementation of kbadgerd kthread.
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/mm_types.h>
+#include <linux/badger_trap.h>
+#include <linux/kthread.h>
+#include <linux/sched/task.h>
+#include <linux/sched/mm.h>
+#include <linux/delay.h>
+#include <linux/vmalloc.h>
+#include <linux/mm.h>
+#include <linux/sort.h>
+#include <linux/rbtree.h>
+#include <linux/mm_econ.h>
+
+#define KBADGERD_SLEEP_MS 100
+#define KBADGERD_NEW_VMA_CHECK_RATE 4
+#define RANGE_SIZE_THRESHOLD HPAGE_PMD_SIZE
+
+// The minimum number of tlb misses for us to consider looking into a range.
+// TODO: this probably should be in units of misses per page or something like
+// that -- that is, for larger regions, it probably makes sense to require more
+// misses. Also, it should depend on the cost of a TLB miss: the cost of the
+// TLB misses should exceed the CPU cost of the sampling.
+#define RANGE_IRRELEVANCE_THRESHOLD 0
+
+struct kbadgerd_range {
+	struct rb_node data_node;
+	struct rb_node range_node;
+
+	u64 start;
+	u64 end; // exclusive
+
+	// Has the range ever been explored?
+	bool explored;
+
+
+	// The number of times this range has been sampled (excluding any
+	// currently running sampling).
+	u64 nsamples;
+
+	// The results of the samples. `stats` is only the most recent/current
+	// sample, while `totals` is the sum of all samples so far.
+	//
+	// `stats`, not `totals` is used for prioritizing the next region to sample.
+	struct badger_trap_stats stats;
+	struct badger_trap_stats totals;
+};
+
+/* The internal state of kbadgerd. */
+struct kbadgerd_state {
+	/* The PID of the process to inspect. */
+	volatile pid_t pid;
+
+	/* The interval to sleep (in millisecs). */
+	volatile unsigned int sleep_interval;
+
+	/*
+	 * Is kbadgerd actively inspecting a process?
+	 *
+	 * If false and pid != 0, then we need to start inspecting a new process.
+	 * If false and pid == 0, there is nothing to do.
+	 * If true and pid == 0, a bug has occurred.
+	 * If true and pid != 0, we are actively inspecting the process.
+	 */
+	volatile bool active;
+	volatile bool pid_changed;
+	volatile bool current_range_removed;
+
+	/* The process and address space to inspect. */
+	struct task_struct *inspected_task;
+	struct mm_struct *mm;
+
+	/*
+	 * The data and range trees below should never have overlapping ranges,
+	 * but old_data may.
+	 * The data and range trees should have the same data except for
+	 * current_range, which is in range but not data.
+	 */
+	/* The data collected by inspection. */
+	struct rb_root_cached data;
+
+	/* List of old ranges, sorted by starting address. */
+	struct rb_root old_data;
+
+	/* List of the VMA ranges tracked to detect new ranges */
+	struct rb_root range;
+
+	/* Protects the three trees.
+	 *
+	 * Lock order: grab mmap_sem and badger_trap_page_table_sem before this lock.
+	 */
+	spinlock_t lock;
+
+	/* Current range. */
+	struct kbadgerd_range *current_range;
+
+	u64 iteration;
+	u64 iteration_time_left;
+};
+static struct kbadgerd_state state;
+
+/* The task_struct of the kthread. */
+static struct task_struct *kbadgerd_task = NULL;
+
+/*
+ * Flag that is set to true to halt the kthread. Only used when exiting the
+ * module.
+ */
+static volatile bool kbadgerd_should_stop = false;
+
+/* kbadgerd sysfs. */
+static struct kobject *kbadgerd_kobj = NULL;
+
+///////////////////////////////////////////////////////////////////////////////
+// Manipulate the range rb-tree.
+//
+// We actually mostly use it as a heap, sorting the ranges so that the maximum
+// range is the one we want to investigate next. We also want to have
+// relatively cheap insertion and deletion so we can remove the max,
+// investigate it, split it, and then insert the chunks.
+
+static bool addr_in_range(u64 addr, struct kbadgerd_range *range)
+{
+	if (addr < range->start)
+		return false;
+	if (addr >= range->end)
+		return false;
+	return true;
+}
+
+static u64 total_misses(const struct badger_trap_stats *stats) {
+	return atomic64_read(&stats->total_dtlb_2mb_load_misses)
+		+ atomic64_read(&stats->total_dtlb_2mb_store_misses)
+		+ atomic64_read(&stats->total_dtlb_4kb_load_misses)
+		+ atomic64_read(&stats->total_dtlb_4kb_store_misses);
+}
+
+/* Compares ranges by size/weight, not memory address. */
+static int kbadgerd_range_cmp(
+		const struct kbadgerd_range *ra,
+		const struct kbadgerd_range *rb)
+{
+	u64 sizea = ra->end - ra->start;
+	u64 sizeb = rb->end - rb->start;
+
+	// Prioritize unexplored regions, but if both are unexplored, keep
+	// comparing...
+	if ((!ra->explored || !rb->explored) && (ra->explored || rb->explored)) {
+		if (ra->explored) return 1;
+		else return -1;
+	}
+
+	// Prioritize regions with more misses, but if both have the same
+	// number, keep comparing...
+	if (total_misses(&ra->stats) > total_misses(&rb->stats)) {
+		return -1;
+	} else if (total_misses(&ra->stats) < total_misses(&rb->stats)) {
+		return 1;
+	}
+
+	// Otherwise, just pick the largest region.
+	if (sizea < sizeb)
+		return 1;
+	else if (sizea > sizeb)
+		return -1;
+	else
+		return 0;
+}
+
+// Inserts a new range into the data and range rb trees
+// If range_root == NULL, the range is not inserted into the range tree
+// If the range overlaps with an existing range, it will not be added to the
+// range tree.
+static noinline void kbadgerd_range_insert_by_weight(
+		struct rb_root_cached *data_root,
+		struct kbadgerd_range *new_range)
+{
+	struct rb_node **new = &(data_root->rb_root.rb_node), *parent = NULL;
+	bool is_leftmost = true;
+
+	/* Figure out where to put new node in the data rb tree */
+	while (*new) {
+		struct kbadgerd_range *this =
+			container_of(*new, struct kbadgerd_range, data_node);
+		int result = kbadgerd_range_cmp(new_range, this);
+
+		parent = *new;
+		if (result < 0) {
+			new = &((*new)->rb_left);
+		} else {
+			// NOTE: since we are sorting by weight, it is possible
+			// for two nodes to have the same weight.
+			new = &((*new)->rb_right);
+			is_leftmost = false;
+		}
+	}
+
+	/* Add new node and rebalance tree. */
+	rb_link_node(&new_range->data_node, parent, new);
+	rb_insert_color_cached(&new_range->data_node, data_root, is_leftmost);
+}
+
+static noinline void kbadgerd_range_insert_by_start(
+		struct rb_root *range_root,
+		struct kbadgerd_range *new_range,
+		bool allow_overlap)
+{
+	struct rb_node **new = &(range_root->rb_node), *parent = NULL;
+
+	while (*new) {
+		struct kbadgerd_range *this =
+			container_of(*new, struct kbadgerd_range, range_node);
+
+		/* The ranges should not overlap*/
+		if (!allow_overlap &&
+		    ((new_range->start <= this->start && this->start < new_range->end)
+		    || (this->start <= new_range->start && new_range->start < this->end)))
+		{
+			pr_err("kbadgerd: Attempted to insert overlapping range!\n");
+			pr_err("kbadgerd: old range=[%llx, %llx) new_range=[%llx, %llx)",
+					this->start, this->end,
+					new_range->start, new_range->end);
+		    	BUG();
+			return;
+		}
+
+		parent = *new;
+		if (new_range->start < this->start)
+			new = &((*new)->rb_left);
+		else
+			new = &((*new)->rb_right);
+	}
+
+	rb_link_node(&new_range->range_node, parent, new);
+	rb_insert_color(&new_range->range_node, range_root);
+}
+
+/*
+ * Finds the smallest range in the tree that contains the given address and
+ * returns it; or returns NULL if none was found.
+ */
+static noinline struct kbadgerd_range *
+kbadgerd_range_search_by_addr(
+	u64 addr,
+	struct rb_root *range_root,
+	bool allow_overlap)
+{
+	struct rb_node *node = rb_first(range_root);
+	struct kbadgerd_range *range, *best_range = NULL;
+
+	// Since there can be overlapping nodes in the tree, we need to start
+	// at the beginning and iterate until we are sure there can be no more
+	// matches.
+	while (node) {
+		range = container_of(node, struct kbadgerd_range, range_node);
+
+		// Have we reached the first range that is too high to contain
+		// our addr?
+		if (range->start > addr)
+			break;
+
+		// Try to find the smallest containing range.
+		if (addr_in_range(addr, range)
+			&& (!best_range
+			    || ((best_range->end - best_range->start) >
+				(range->end - range->start))))
+		{
+			best_range = range;
+		}
+
+		node = rb_next(node);
+	}
+
+	return best_range;
+}
+
+/* NOTE: max = first, so we can take advantage of rb_root_cached. */
+/* Only remove from the data rb tree to avoid accidentally adding the same range twice */
+static struct kbadgerd_range *
+kbadgerd_range_remove_max(struct rb_root_cached *root)
+{
+	struct rb_node *max_node = rb_first_cached(root);
+	struct kbadgerd_range *max;
+
+	if (!max_node)
+		return NULL;
+
+	max = container_of(max_node, struct kbadgerd_range, data_node);
+	rb_erase_cached(max_node, root);
+
+	return max;
+}
+
+///////////////////////////////////////////////////////////////////////////////
+// kbadgerd data collection.
+
+static void set_range_iterations(struct kbadgerd_range *range) {
+	u64 range_size_hp = (range->end - range->start) >> HPAGE_SHIFT;
+	state.iteration_time_left = range_size_hp > 0 ? range_size_hp : 1;
+}
+
+static void start_badger_trap(struct kbadgerd_range *range) {
+	BUG_ON(!range);
+
+	set_range_iterations(range);
+
+	// Turn on badger trap for the next range.
+	badger_trap_set_stats_loc(state.mm, &range->stats);
+	badger_trap_stats_clear(state.mm->bt_stats);
+	badger_trap_walk(state.mm, range->start, range->end - 1, true);
+}
+
+static void print_data(struct kbadgerd_range *range) {
+	u64 ld_4k = atomic64_read_acquire(&range->totals.total_dtlb_4kb_load_misses),
+	    st_4k = atomic64_read_acquire(&range->totals.total_dtlb_4kb_store_misses),
+	    ld_2m = atomic64_read_acquire(&range->totals.total_dtlb_2mb_load_misses),
+	    st_2m = atomic64_read_acquire(&range->totals.total_dtlb_2mb_store_misses),
+	    total = ld_4k + st_4k + ld_2m + st_2m;
+
+	pr_warn("kbadgerd: [%llx, %llx) (%lld bytes)", range->start, range->end,
+			range->end - range->start);
+
+	// We want to print values in units of misses/huge-page/LTU.
+	if (total) {
+		u64 size = (range->end - range->start) >> HPAGE_SHIFT;
+		u64 nsamples = range->nsamples;
+		if (nsamples == 0) {
+			pr_warn("kbadgerd: range has total %lld but 0 samples...\n", total);
+			nsamples += 1; // should only happen for active range...
+		}
+
+		// Scale time
+		ld_4k *= (MM_ECON_LTU / KBADGERD_SLEEP_MS) / nsamples;
+		st_4k *= (MM_ECON_LTU / KBADGERD_SLEEP_MS) / nsamples;
+		ld_2m *= (MM_ECON_LTU / KBADGERD_SLEEP_MS) / nsamples;
+		st_2m *= (MM_ECON_LTU / KBADGERD_SLEEP_MS) / nsamples;
+
+		// Scale size
+		ld_4k /= size > 0 ? size : 1;
+		st_4k /= size > 0 ? size : 1;
+		ld_2m /= size > 0 ? size : 1;
+		st_2m /= size > 0 ? size : 1;
+
+		pr_warn("kbadgerd: \t4KB load misses: %lld", ld_4k);
+		pr_warn("kbadgerd: \t4KB store misses: %lld", st_4k);
+		pr_warn("kbadgerd: \t2MB load misses: %lld", ld_2m);
+		pr_warn("kbadgerd: \t2MB store misses: %lld\n", st_2m);
+	} else {
+		pr_warn("kbadgerd: \tNo misses\n");
+	}
+}
+
+static void print_all_data(void) {
+	struct kbadgerd_range *range;
+	// The range tree is used here because it is sorted by the start address
+	// This relies on the invariant that the data and range trees have the
+	// same ranges
+	struct rb_node *node = rb_first(&state.range);
+
+	pr_warn("kbadgerd: Results of inspection for pid=%d\n", state.pid);
+
+	while (node) {
+		range = container_of(node, struct kbadgerd_range, range_node);
+		print_data(range);
+
+		node = rb_next(node);
+	}
+
+	pr_warn("kbadgerd: Discarded ranges for pid=%d\n", state.pid);
+	node = rb_first(&state.old_data);
+
+	while (node) {
+		range = container_of(node, struct kbadgerd_range, range_node);
+		print_data(range);
+
+		node = rb_next(node);
+	}
+
+	pr_warn("kbadgerd: END Results of inspection for pid=%d\n", state.pid);
+}
+
+static struct kbadgerd_range *
+kbadgerd_is_new_range(struct rb_root *root, struct vm_area_struct *vma) {
+	struct rb_node *node = root->rb_node;
+	struct kbadgerd_range *range;
+	struct kbadgerd_range *new_range;
+	u64 max_start = vma->vm_start;
+	u64 min_end = vma->vm_end;
+
+	while (node) {
+		range = container_of(node, struct kbadgerd_range, range_node);
+
+		// If the vma is the same or entirely in another range, stop.
+		if (max_start >= range->start && min_end <= range->end) {
+			return NULL;
+		}
+
+		// If the vma completely subsumes another range, shorten it to
+		// one side, and we'll get the other side in a future check.
+		if (max_start <= range->start && min_end >= range->end) {
+			max_start = range->end;
+			node = root->rb_node;
+			continue;
+		}
+
+		// If the start of the range is within an old range and the end is outside
+		// of the old range, we may need to create a new range at the end of the old
+		// range. This can happen if the VMA grows up from the end.
+		// If the range keeps growing, there can be multiple ranges between the VMA
+		// start and end, so make sure to get the largest one.
+		if (max_start < range->end && min_end > range->end) {
+			max_start = range->end;
+			node = root->rb_node;
+			continue;
+		}
+		// Same as above, but for if the VMA grows down from the start
+		if (max_start < range->start && min_end > range->start) {
+			min_end = range->start;
+			node = root->rb_node;
+			continue;
+		}
+
+		// Traverse the tree
+		if (max_start < range->start)
+			node = node->rb_left;
+		else
+			node = node->rb_right;
+	}
+
+	new_range =
+		(struct kbadgerd_range *)vzalloc(sizeof(struct kbadgerd_range));
+
+	if (!new_range) {
+		pr_err("kbadgerd: Unable to alloc new range! Skipping.");
+		return NULL;
+	}
+
+	badger_trap_stats_init(&new_range->stats);
+	badger_trap_stats_init(&new_range->totals);
+
+	new_range->start = max_start;
+	new_range->end = min_end;
+
+	pr_warn("kbadgerd: range extension detected. vma=[%lx, %lx) new_range=[%llx, %llx)\n",
+			vma->vm_start, vma->vm_end,
+			new_range->start, new_range->end);
+
+	return new_range;
+}
+
+static struct kbadgerd_range *
+kbadgerd_has_holes(
+	struct rb_root_cached *data_root,
+	struct rb_root *old_data_root,
+	struct rb_root *range_root,
+	struct vm_area_struct *vma)
+{
+	struct rb_node *node = range_root->rb_node;
+	struct rb_node **nodes_to_remove;
+	struct kbadgerd_range *range;
+	struct kbadgerd_range *first_range = NULL;
+	struct kbadgerd_range *last_range = NULL;
+	struct kbadgerd_range *new_range;
+	bool is_hole = false;
+	int num_ranges;
+	int i;
+
+	// First, find the first kbadgerd range in the vma. This is the first
+	// range whose end address is after the vma's start address
+	while (node) {
+		range = container_of(node, struct kbadgerd_range, range_node);
+
+		if (range->end > vma->vm_start && range->start < vma->vm_end) {
+			if (!first_range || range->start < first_range->start)
+				first_range = range;
+		}
+
+		if (vma->vm_start < range->start)
+			node = node->rb_left;
+		else if (vma->vm_start == range->start)
+			break;
+		else
+			node = node->rb_right;
+	}
+
+	if (!first_range || !node)
+		return NULL;
+
+	// Find the last kbadgerd range in the vma. This is the last range
+	// whose start address is before the vma's end address
+	last_range = first_range;
+	num_ranges = 1;
+	node = rb_next(node);
+	while (node) {
+		range = container_of(node, struct kbadgerd_range, range_node);
+
+		if (range->start >= vma->vm_end)
+			break;
+
+		// If there's a gap between ranges within a vma, there's a hole
+		if (last_range->end != range->start)
+			is_hole = true;
+
+		last_range = range;
+
+		num_ranges++;
+		node = rb_next(node);
+	}
+
+	if (!is_hole) {
+		return NULL;
+	}
+
+	nodes_to_remove =
+		(struct rb_node**)vzalloc(sizeof(struct rb_node*) * num_ranges);
+
+	if (!nodes_to_remove) {
+		pr_err("kbadgerd: Unable to alloc array of old ranges.");
+		return NULL;
+	}
+
+	new_range =
+		(struct kbadgerd_range *)vzalloc(sizeof(struct kbadgerd_range));
+
+	if (!new_range) {
+		pr_err("kbadgerd: Unable to alloc new range! Skipping.");
+		vfree(nodes_to_remove);
+		return NULL;
+	}
+
+	pr_warn("kbadgerd: detected new VMA that overlaps holes. vma=[%lx, %lx)\n",
+			vma->vm_start, vma->vm_end);
+
+	new_range->start = min((u64)vma->vm_start, first_range->start);
+	new_range->end = max((u64)vma->vm_end, last_range->end);
+
+	badger_trap_stats_init(&new_range->stats);
+	badger_trap_stats_init(&new_range->totals);
+
+	// We need to gather the nodes to remove here and actually remove them
+	// in a different loop because we need to avoid calling rb_erase when
+	// iterating with rb_next.
+	node = &first_range->range_node;
+	for (i = 0; i < num_ranges && node; i++) {
+		nodes_to_remove[i] = node;
+
+		node = rb_next(node);
+	}
+
+	// Put the old ranges inside the new range in the old data tree
+	for (i = 0; i < num_ranges; i++) {
+		range = container_of(nodes_to_remove[i], struct kbadgerd_range, range_node);
+
+		pr_warn("kbadgerd: VMA overlaps range=[%llx, %llx)\n",
+				range->start, range->end);
+
+		rb_erase(&range->range_node, range_root);
+		// If this range is current range, it has already been removed
+		// from the data tree.
+		if (range != state.current_range) {
+			rb_erase_cached(&range->data_node, data_root);
+		} else {
+			// This is needed to make sure this range isn't removed from the
+			// range tree twice
+			state.current_range_removed = true;
+			// This is needed to prevent spending more time on this range
+			state.iteration_time_left = 1;
+		}
+		kbadgerd_range_insert_by_start(old_data_root, range, true);
+	}
+
+	vfree(nodes_to_remove);
+
+	return new_range;
+}
+
+static noinline void check_for_new_vmas(void) {
+	struct vm_area_struct *vma = NULL;
+	struct kbadgerd_range *range;
+
+	down_read(&state.mm->mmap_sem);
+	spin_lock(&state.lock);
+
+	for (vma = state.mm->mmap; vma; vma = vma->vm_next) {
+		range = kbadgerd_has_holes(&state.data, &state.old_data,
+			&state.range, vma);
+
+		if (!range)
+			range = kbadgerd_is_new_range(&state.range, vma);
+
+		if (range) {
+			kbadgerd_range_insert_by_weight(&state.data, range);
+			kbadgerd_range_insert_by_start(&state.range, range, false);
+		}
+	}
+
+	spin_unlock(&state.lock);
+	up_read(&state.mm->mmap_sem);
+}
+
+static noinline void start_inspection(void) {
+	struct task_struct *target_task;
+	struct vm_area_struct *vma = NULL;
+	int i;
+
+	pr_warn("kbadgerd: start inspection of pid=%d\n", state.pid);
+
+	BUG_ON(state.pid == 0);
+
+	// Avoid extra printing from badger trap.
+	silence();
+
+	// Get the task_struct and mm_struct to be inspected.
+	target_task = get_pid_task(find_get_pid(state.pid), PIDTYPE_PID);
+	if (!target_task) {
+		pr_warn("kbadgerd: no such pid for promotion.\n");
+		return;
+	}
+
+	state.inspected_task = target_task;
+
+	mmgrab(target_task->mm);
+	state.mm = target_task->mm;
+
+	// Collect a list of address ranges. We collect this list rather than
+	// an array of vm_area_struct because there can be calls to mmap
+	// between iterations of kbadgerd. This would lead to annoyances in
+	// managing pointers.
+	down_read(&state.mm->mmap_sem);
+	spin_lock(&state.lock);
+
+	state.data = RB_ROOT_CACHED;
+	state.old_data = RB_ROOT;
+	state.range = RB_ROOT;
+
+	state.current_range = NULL;
+
+	for (i = 0, vma = state.mm->mmap; vma; vma = vma->vm_next) {
+		struct kbadgerd_range *new_range =
+			(struct kbadgerd_range *)vzalloc(sizeof(struct kbadgerd_range));
+
+		if (!new_range) {
+			pr_err("kbadgerd: Unable to alloc new range! Skipping.");
+			break;
+		}
+
+		badger_trap_stats_init(&new_range->stats);
+		badger_trap_stats_init(&new_range->totals);
+
+		new_range->start = vma->vm_start;
+		new_range->end = vma->vm_end;
+
+		kbadgerd_range_insert_by_weight(&state.data, new_range);
+		kbadgerd_range_insert_by_start(&state.range, new_range, false);
+
+		i += 1;
+
+		pr_warn("kbadgerd: region [%lx, %lx) anon=%d\n",
+				vma->vm_start, vma->vm_end,
+				vma_is_anonymous(vma));
+	}
+
+	spin_unlock(&state.lock);
+	up_read(&state.mm->mmap_sem);
+
+	// Start badger trap for the first range...
+	state.active = true;
+	state.current_range_removed = false;
+	state.current_range = kbadgerd_range_remove_max(&state.data);
+	if (!state.current_range) {
+		pr_err("kbadgerd: no range to act on.");
+		return;
+	}
+	start_badger_trap(state.current_range);
+
+	pr_warn("kbadgerd: inited with process %d, %lld iterations\n",
+			state.pid, state.iteration_time_left);
+
+	return;
+}
+
+static noinline void end_inspection(void) {
+	struct kbadgerd_range *range;
+	struct rb_node *node;
+
+	pr_warn("kbadgerd: Ending inspection.\n");
+
+	// Collect final stats...
+	if (state.current_range) {
+		// NOTE: don't hold state.lock here because mmap_sem is grabbed
+		// by badger_trap_walk.
+		badger_trap_walk(state.mm,
+				state.current_range->start,
+				state.current_range->end - 1,
+				false);
+
+		badger_trap_add_stats(&state.current_range->totals,
+				&state.current_range->stats);
+
+		spin_lock(&state.lock);
+
+		// Add it back to the tree for simplicity.
+		kbadgerd_range_insert_by_weight(&state.data, state.current_range);
+		state.current_range = NULL;
+		badger_trap_set_stats_loc(state.mm, NULL);
+	} else {
+		spin_lock(&state.lock);
+	}
+
+	// Print all stats.
+	print_all_data();
+
+	// Free the tree.
+	while ((range = kbadgerd_range_remove_max(&state.data))) { // NOTE: assignment
+		vfree(range);
+	}
+
+	while ((node = rb_first(&state.old_data))) { // NOTE: assignment
+		range = container_of(node, struct kbadgerd_range, range_node);
+		rb_erase(node, &state.old_data);
+		vfree(range);
+	}
+
+	state.range = RB_ROOT;
+
+	spin_unlock(&state.lock);
+
+	if (state.mm) {
+		mmdrop(state.mm);
+		state.mm = NULL;
+	}
+
+	if (state.inspected_task) {
+		put_task_struct(state.inspected_task);
+		state.inspected_task = NULL;
+	}
+
+	state.active = false;
+	state.pid = 0;
+}
+
+// NOTE: caller must acquire state.lock. This function does NOT release the lock.
+static noinline void process_and_insert_current_range(void) {
+	struct kbadgerd_range *current_range = state.current_range;
+	struct kbadgerd_range *new_left_range, *new_right_range;
+	u64 midpoint;
+
+	if (state.current_range_removed) {
+		state.current_range = NULL;
+		state.current_range_removed = false;
+		return;
+	}
+
+	// Mark the current range as explored. This decreases its priority in
+	// the next scan should we choose not to split it.
+	current_range->explored = true;
+	current_range->nsamples += 1;
+
+	// Remove the range from the range rb tree because it might be split
+	rb_erase(&current_range->range_node, &state.range);
+
+	// If the size of the current range is smaller than the threshold, we
+	// don't try to break it down further. Just insert it back to the tree.
+	if (current_range->end - current_range->start <= RANGE_SIZE_THRESHOLD) {
+		kbadgerd_range_insert_by_weight(&state.data, current_range);
+		kbadgerd_range_insert_by_start(&state.range, current_range, false);
+		return;
+	}
+
+	// If the region took no hits, then don't bother looking at it much more...
+	if (total_misses(&current_range->stats) <= RANGE_IRRELEVANCE_THRESHOLD) {
+		kbadgerd_range_insert_by_weight(&state.data, current_range);
+		kbadgerd_range_insert_by_start(&state.range, current_range, false);
+		return;
+	}
+
+	// We want the midpoint to end at a page-aligned boundary.
+	midpoint = (current_range->start
+		+ (current_range->end - current_range->start) / 2)
+		& PAGE_MASK;
+
+	// Range is too small to split further. If RANGE_SIZE_THRESHOLD is
+	// large enough, this should never happen.
+	if (current_range->start == midpoint || current_range->end == midpoint) {
+		kbadgerd_range_insert_by_weight(&state.data, current_range);
+		kbadgerd_range_insert_by_start(&state.range, current_range, false);
+		return;
+	}
+
+	// Otherwise, we try to split it in half. We give each half the weight
+	// of the whole so that we prioritize re-inspection.
+	new_left_range =
+		(struct kbadgerd_range *)vzalloc(sizeof(struct kbadgerd_range));
+
+	if (!new_left_range) {
+		pr_err("kbadgerd: Unable to alloc new range! Reusing old.");
+		kbadgerd_range_insert_by_weight(&state.data, current_range);
+		kbadgerd_range_insert_by_start(&state.range, current_range, false);
+		return;
+	}
+
+	new_right_range =
+		(struct kbadgerd_range *)vzalloc(sizeof(struct kbadgerd_range));
+
+	if (!new_right_range) {
+		pr_err("kbadgerd: Unable to alloc new range! Reusing old.");
+		kbadgerd_range_insert_by_weight(&state.data, current_range);
+		kbadgerd_range_insert_by_start(&state.range, current_range, false);
+		vfree(new_left_range);
+		return;
+	}
+
+	badger_trap_stats_init(&new_left_range->stats);
+	badger_trap_stats_init(&new_right_range->stats);
+	badger_trap_stats_init(&new_left_range->totals);
+	badger_trap_stats_init(&new_right_range->totals);
+
+	// TODO markm: bijan suggested taking stats/2 for each half...
+
+	new_left_range->start = current_range->start;
+	new_left_range->end = midpoint;
+	new_left_range->stats = current_range->stats;
+
+	new_right_range->start = midpoint;
+	new_right_range->end = current_range->end;
+	new_right_range->stats = current_range->stats;
+
+	kbadgerd_range_insert_by_weight(&state.data, new_left_range);
+	kbadgerd_range_insert_by_start(&state.range, new_left_range, false);
+	kbadgerd_range_insert_by_weight(&state.data, new_right_range);
+	kbadgerd_range_insert_by_start(&state.range, new_right_range, false);
+
+	state.current_range = NULL;
+	kbadgerd_range_insert_by_start(&state.old_data, current_range, true);
+}
+
+static noinline void continue_inspection(void) {
+	struct kbadgerd_range *range;
+
+	if (state.inspected_task->flags & (PF_EXITING | PF_SIGNALED)) {
+		pr_warn("kbadgerd: inspected process is exiting. Ending inspection.\n");
+		end_inspection();
+		return;
+	}
+
+	if (state.iteration_time_left == 1 || (state.iteration_time_left % 100 == 0))
+		pr_warn("kbadgerd: continuing inspection of pid=%d, %lld iterations left\n",
+				state.pid, state.iteration_time_left);
+
+	BUG_ON(state.pid == 0 || !state.active);
+	BUG_ON(!state.current_range);
+
+	// If we are not done with the current range, do nothing.
+	if (--state.iteration_time_left > 0) return;
+
+	range = state.current_range;
+
+	// Turn off bt for the outgoing range.
+	badger_trap_set_stats_loc(state.mm, NULL);
+	badger_trap_walk(state.mm, range->start, range->end - 1, false);
+
+	// Accumulate the changes from the last sampling period.
+	badger_trap_add_stats(&range->totals, &range->stats);
+
+	// Insert the current range back into the tree.
+	spin_lock(&state.lock);
+	process_and_insert_current_range();
+
+	// Move to the next range, if any.
+	state.current_range = kbadgerd_range_remove_max(&state.data);
+	spin_unlock(&state.lock);
+	if (!state.current_range) {
+		end_inspection();
+		return;
+	}
+
+	// TODO markm: want some terminating condition here.
+
+	// Reset the counters and start badger trap.
+	start_badger_trap(state.current_range);
+}
+
+/* The main loop of kbadgerd. */
+static int kbadgerd_do_work(void *data)
+{
+        u32 i = 0;
+
+	while (!kbadgerd_should_stop) {
+		if (state.active && state.pid_changed) {
+			pr_warn("kbadgerd: pid changed. Ending inspection.");
+			end_inspection();
+		}
+
+		if (state.active) {
+                        if (i % KBADGERD_NEW_VMA_CHECK_RATE == 0) {
+				check_for_new_vmas();
+                        }
+			continue_inspection();
+		} else if (state.pid != 0) {
+			start_inspection();
+		}
+
+                i++;
+		pr_warn_once("kbadgerd: Interval is %d ms.\n", state.sleep_interval);
+		msleep(state.sleep_interval);
+	}
+
+	pr_warn("kbadgerd: exiting.\n");
+
+	spin_lock(&state.lock);
+	end_inspection();
+	spin_unlock(&state.lock);
+
+	do_exit(0);
+	BUG(); // Should never get here.
+}
+
+/******************************************************************************/
+/* A mm_econ_tlb_miss_estimator_fn_t for use in mm_econ, of course! */
+
+// For now, we do something simple: we find out if we have any info on the
+// given range.  If so, we return (# misses in the range)/(# huge pages in the
+// range), effectively assuming that the range has uniformly distributed misses
+// across its pages.
+//
+// We check starting in state.data and then state.old_data.
+//
+// If we don't have any info, just return 0;
+static u64 tlb_miss_est_fn(const struct mm_action *action)
+{
+	u64 ret = 0;
+	const u64 addr = action->address;
+	struct kbadgerd_range *range = NULL;
+
+	// Do a quick check before hand. This is racy, but will be true for all
+	// processes that are not being inspected, so we want it to be fast.
+	//
+	// If this check succeeds, then we grab the lock and try again.
+	if (!state.inspected_task
+		|| current->pid != state.inspected_task->pid
+		|| !state.active
+		|| !state.current_range)
+	{
+		return 0;
+	}
+
+	spin_lock(&state.lock);
+
+	if (current->pid != state.inspected_task->pid
+		|| !state.active
+		|| !state.current_range)
+	{
+		spin_unlock(&state.lock);
+		return 0;
+	}
+
+	// Check current range.
+	if (addr_in_range(addr, state.current_range))
+		range = state.current_range;
+
+	// Check range tree.
+	else {
+		range = kbadgerd_range_search_by_addr(addr, &state.range, false);
+	}
+
+	// Check old_data if we don't have enough info yet...
+	if (!range || total_misses(&range->totals) == 0) {
+		range = kbadgerd_range_search_by_addr(addr, &state.range, true);
+	}
+
+	// If we found a range, compute the number of misses per page and return.
+	if (range) {
+		// Divide misses over the whole range.
+		//
+		// NOTE: it's possible that the range is smaller than 2MB in
+		// size if it derives from a VMA that was smaller than 2MB.
+		// It's not clear what the best way to scale it is, so we jsut
+		// do the most obvious thing here...
+		u64 npages = (range->end - range->start) >> HPAGE_SHIFT;
+		if (npages == 0) {
+			ret = total_misses(&range->totals);
+		} else {
+			ret = total_misses(&range->totals) / npages;
+		}
+
+		// Scale up to be in LTU, then divide by number of samples.
+		if (ret > 0) {
+			BUG_ON(range->nsamples == 0);
+			ret = ret * (MM_ECON_LTU / KBADGERD_SLEEP_MS) / range->nsamples;
+		}
+
+		//pr_warn("mm_econ: estimating page benefit: "
+		//	"misses=%llu size=%llu per-page=%llu\n",
+		//	ret,
+		//	(range->end - range->start) >> HPAGE_SHIFT,
+		//	ret);
+	}
+
+	spin_unlock(&state.lock);
+
+	return ret;
+}
+
+/******************************************************************************/
+/* Module init and deinit */
+
+static ssize_t enabled_show(struct kobject *kobj,
+			    struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "pid=%d\n", state.pid);
+}
+
+static ssize_t enabled_store(struct kobject *kobj,
+			     struct kobj_attribute *attr,
+			     const char *buf, size_t count)
+{
+	pid_t pid;
+	int ret;
+
+	ret = kstrtoint(buf, 0, &pid);
+
+	if (state.pid != 0) {
+		pr_warn("kbadgerd: pid changed while already running.");
+		state.pid_changed = true;
+	}
+
+	if (sysfs_streq(buf, "off")) {
+		return count;
+	}
+
+	if (ret != 0) {
+		state.pid = 0;
+		return ret;
+	}
+	// Check that this is an existing process.
+	else if (find_vpid(pid) != NULL) {
+		state.pid = pid;
+		return count;
+	}
+	// Not a valid PID.
+	else {
+		state.pid = 0;
+		return -EINVAL;
+	}
+}
+static struct kobj_attribute enabled_attr =
+	__ATTR(enabled, 0644, enabled_show, enabled_store);
+
+static ssize_t sleep_interval_show(struct kobject *kobj,
+			    struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", state.sleep_interval);
+}
+
+static ssize_t sleep_interval_store(struct kobject *kobj,
+			     struct kobj_attribute *attr,
+			     const char *buf, size_t count)
+{
+	unsigned int interval;
+	int ret;
+
+	ret = kstrtouint(buf, 0, &interval);
+	if (ret != 0) {
+		return ret;
+	}
+	else if (interval == 0) {
+		state.sleep_interval = KBADGERD_SLEEP_MS;
+		return count;
+	}
+	else {
+		state.sleep_interval = interval;
+		return count;
+	}
+}
+static struct kobj_attribute sleep_interval_attr =
+	__ATTR(sleep_interval, 0644, sleep_interval_show,
+			sleep_interval_store);
+
+static struct attribute *kbadgerd_attr[] = {
+	&enabled_attr.attr,
+	&sleep_interval_attr.attr,
+	NULL,
+};
+
+static const struct attribute_group kbadgerd_attr_group = {
+	.attrs = kbadgerd_attr,
+};
+
+static int kbadgerd_init_sysfs(struct kobject **kbadgerd_kobj)
+{
+	int err;
+
+	*kbadgerd_kobj = kobject_create_and_add("kbadgerd", mm_kobj);
+	if (unlikely(!*kbadgerd_kobj)) {
+		pr_err("failed to create kbadgerd kobject\n");
+		return -ENOMEM;
+	}
+
+	err = sysfs_create_group(*kbadgerd_kobj, &kbadgerd_attr_group);
+	if (err) {
+		pr_err("failed to register kbadgerd group\n");
+		kobject_put(*kbadgerd_kobj);
+		return err;
+	}
+
+	return 0;
+}
+
+#if IS_MODULE(CONFIG_KBADGERD)
+static void kbadgerd_exit_sysfs(struct kobject *kbadgerd_kobj)
+{
+	sysfs_remove_group(kbadgerd_kobj, &kbadgerd_attr_group);
+	kobject_put(kbadgerd_kobj);
+}
+#endif
+
+static int do_kbadgerd_init(void)
+{
+	int err;
+
+	BUG_ON(kbadgerd_task);
+
+	pr_warn("kbadgerd: Starting.\n");
+
+	// Init state by clearing it.
+	memset(&state, 0, sizeof(state));
+	state.sleep_interval = KBADGERD_SLEEP_MS;
+	spin_lock_init(&state.lock);
+
+	kbadgerd_should_stop = false;
+	kbadgerd_task = kthread_run(kbadgerd_do_work, NULL, "kbadgerd");
+
+	if (IS_ERR(kbadgerd_task)) {
+		err = PTR_ERR(kbadgerd_task);
+		kbadgerd_task = NULL;
+		return err;
+	}
+
+	err = kbadgerd_init_sysfs(&kbadgerd_kobj);
+	if (err)
+		return err;
+
+	register_mm_econ_tlb_miss_estimator(tlb_miss_est_fn);
+
+	return 0;
+}
+
+#if IS_MODULE(CONFIG_KBADGERD)
+static int __init init_kbadgerd(void)
+{
+	return do_kbadgerd_init();
+}
+module_init(init_kbadgerd);
+
+static void do_kbadgerd_exit(void)
+{
+	if (kbadgerd_task) {
+		kbadgerd_should_stop = true;
+		kthread_stop(kbadgerd_task);
+	}
+
+	kbadgerd_exit_sysfs(kbadgerd_kobj);
+}
+
+static void __exit exit_kbadgerd(void)
+{
+	do_kbadgerd_exit();
+}
+module_exit(exit_kbadgerd);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Mark Mansi <markm@cs.wisc.edu>");
+MODULE_DESCRIPTION("BadgerTrap kthread.");
+#elif IS_BUILTIN(CONFIG_KBADGERD)
+static int __init init_kbadgerd(void)
+{
+	return do_kbadgerd_init();
+}
+subsys_initcall(init_kbadgerd);
+#endif
diff --git a/mm/khugepaged.c b/mm/khugepaged.c
index b679908743cb..e3aa4d030b73 100644
--- a/mm/khugepaged.c
+++ b/mm/khugepaged.c
@@ -18,39 +18,15 @@
 #include <linux/page_idle.h>
 #include <linux/swapops.h>
 #include <linux/shmem_fs.h>
+#include <linux/mm_econ.h>
+#include <linux/mm_stats.h>
+#include <linux/huge_mm.h>
+#include <linux/badger_trap.h>
 
 #include <asm/tlb.h>
 #include <asm/pgalloc.h>
 #include "internal.h"
 
-enum scan_result {
-	SCAN_FAIL,
-	SCAN_SUCCEED,
-	SCAN_PMD_NULL,
-	SCAN_EXCEED_NONE_PTE,
-	SCAN_PTE_NON_PRESENT,
-	SCAN_PAGE_RO,
-	SCAN_LACK_REFERENCED_PAGE,
-	SCAN_PAGE_NULL,
-	SCAN_SCAN_ABORT,
-	SCAN_PAGE_COUNT,
-	SCAN_PAGE_LRU,
-	SCAN_PAGE_LOCK,
-	SCAN_PAGE_ANON,
-	SCAN_PAGE_COMPOUND,
-	SCAN_ANY_PROCESS,
-	SCAN_VMA_NULL,
-	SCAN_VMA_CHECK,
-	SCAN_ADDRESS_RANGE,
-	SCAN_SWAP_CACHE_PAGE,
-	SCAN_DEL_PAGE_LRU,
-	SCAN_ALLOC_HUGE_PAGE_FAIL,
-	SCAN_CGROUP_CHARGE_FAIL,
-	SCAN_EXCEED_SWAP_PTE,
-	SCAN_TRUNCATED,
-	SCAN_PAGE_HAS_PRIVATE,
-};
-
 #define CREATE_TRACE_POINTS
 #include <trace/events/huge_memory.h>
 
@@ -646,8 +622,10 @@ static int __collapse_huge_page_isolate(struct vm_area_struct *vma,
 static void __collapse_huge_page_copy(pte_t *pte, struct page *page,
 				      struct vm_area_struct *vma,
 				      unsigned long address,
-				      spinlock_t *ptl)
+				      spinlock_t *ptl,
+				      struct mm_stats_pftrace *pftrace)
 {
+	u64 start = rdtsc();
 	pte_t *_pte;
 	for (_pte = pte; _pte < pte + HPAGE_PMD_NR;
 				_pte++, page++, address += PAGE_SIZE) {
@@ -655,6 +633,7 @@ static void __collapse_huge_page_copy(pte_t *pte, struct page *page,
 		struct page *src_page;
 
 		if (pte_none(pteval) || is_zero_pfn(pte_pfn(pteval))) {
+			mm_stats_set_flag(pftrace, MM_STATS_PF_CLEARED_MEM);
 			clear_user_highpage(page, address);
 			add_mm_counter(vma->vm_mm, MM_ANONPAGES, 1);
 			if (is_zero_pfn(pte_pfn(pteval))) {
@@ -670,6 +649,7 @@ static void __collapse_huge_page_copy(pte_t *pte, struct page *page,
 				spin_unlock(ptl);
 			}
 		} else {
+			mm_stats_set_flag(pftrace, MM_STATS_PF_HUGE_COPY);
 			src_page = pte_page(pteval);
 			copy_user_highpage(page, src_page, address, vma);
 			VM_BUG_ON_PAGE(page_mapcount(src_page) != 1, src_page);
@@ -690,6 +670,8 @@ static void __collapse_huge_page_copy(pte_t *pte, struct page *page,
 			free_page_and_swap_cache(src_page);
 		}
 	}
+
+	mm_stats_hist_measure(&mm_huge_page_promotion_copy_pages_cycles, rdtsc() - start);
 }
 
 static void khugepaged_alloc_sleep(void)
@@ -858,7 +840,7 @@ khugepaged_alloc_page(struct page **hpage, gfp_t gfp, int node)
  */
 
 static int hugepage_vma_revalidate(struct mm_struct *mm, unsigned long address,
-		struct vm_area_struct **vmap)
+		struct vm_area_struct **vmap, bool force)
 {
 	struct vm_area_struct *vma;
 	unsigned long hstart, hend;
@@ -870,6 +852,8 @@ static int hugepage_vma_revalidate(struct mm_struct *mm, unsigned long address,
 	if (!vma)
 		return SCAN_VMA_NULL;
 
+	if (force) return 0;
+
 	hstart = (vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;
 	hend = vma->vm_end & HPAGE_PMD_MASK;
 	if (address < hstart || address + HPAGE_PMD_SIZE > hend)
@@ -890,7 +874,8 @@ static int hugepage_vma_revalidate(struct mm_struct *mm, unsigned long address,
 static bool __collapse_huge_page_swapin(struct mm_struct *mm,
 					struct vm_area_struct *vma,
 					unsigned long address, pmd_t *pmd,
-					int referenced)
+					int referenced, bool force,
+					struct mm_stats_pftrace *pftrace)
 {
 	int swapped_in = 0;
 	vm_fault_t ret = 0;
@@ -914,12 +899,22 @@ static bool __collapse_huge_page_swapin(struct mm_struct *mm,
 		if (!is_swap_pte(vmf.orig_pte))
 			continue;
 		swapped_in++;
-		ret = do_swap_page(&vmf);
+
+		// NOTE(markm): releasing here technically allows a race
+		// between bt and do_swap_page, but do_swap_page is a huge
+		// rat's nest that I don't want to crawl through and handle
+		// properly.
+		up_read(&mm->badger_trap_page_table_sem);
+
+		ret = do_swap_page(&vmf, pftrace);
+
+		mm_stats_set_flag(pftrace, MM_STATS_PF_SWAP);
 
 		/* do_swap_page returns VM_FAULT_RETRY with released mmap_sem */
 		if (ret & VM_FAULT_RETRY) {
 			down_read(&mm->mmap_sem);
-			if (hugepage_vma_revalidate(mm, address, &vmf.vma)) {
+			down_read(&mm->badger_trap_page_table_sem);
+			if (hugepage_vma_revalidate(mm, address, &vmf.vma, force)) {
 				/* vma is no longer available, don't continue to swapin */
 				trace_mm_collapse_huge_page_swapin(mm, swapped_in, referenced, 0);
 				return false;
@@ -943,10 +938,12 @@ static bool __collapse_huge_page_swapin(struct mm_struct *mm,
 	return true;
 }
 
-static void collapse_huge_page(struct mm_struct *mm,
+static int collapse_huge_page(struct mm_struct *mm,
 				   unsigned long address,
 				   struct page **hpage,
-				   int node, int referenced)
+				   int node, int referenced,
+				   bool force,
+				   struct mm_stats_pftrace *pftrace)
 {
 	pmd_t *pmd, _pmd;
 	pte_t *pte;
@@ -958,6 +955,7 @@ static void collapse_huge_page(struct mm_struct *mm,
 	struct vm_area_struct *vma;
 	struct mmu_notifier_range range;
 	gfp_t gfp;
+	u64 start = rdtsc();
 
 	VM_BUG_ON(address & ~HPAGE_PMD_MASK);
 
@@ -970,9 +968,11 @@ static void collapse_huge_page(struct mm_struct *mm,
 	 * sync compaction, and we do not need to hold the mmap_sem during
 	 * that. We will recheck the vma after taking it again in write mode.
 	 */
+	up_read(&mm->badger_trap_page_table_sem);
 	up_read(&mm->mmap_sem);
 	new_page = khugepaged_alloc_page(hpage, gfp, node);
 	if (!new_page) {
+		mm_stats_set_flag(pftrace, MM_STATS_PF_HUGE_ALLOC_FAILED);
 		result = SCAN_ALLOC_HUGE_PAGE_FAIL;
 		goto out_nolock;
 	}
@@ -983,17 +983,22 @@ static void collapse_huge_page(struct mm_struct *mm,
 	}
 
 	down_read(&mm->mmap_sem);
-	result = hugepage_vma_revalidate(mm, address, &vma);
+	result = hugepage_vma_revalidate(mm, address, &vma, force);
 	if (result) {
 		mem_cgroup_cancel_charge(new_page, memcg, true);
 		up_read(&mm->mmap_sem);
 		goto out_nolock;
 	}
 
+	pr_info("promoting: vma=%p", vma);
+
+	down_read(&mm->badger_trap_page_table_sem);
+
 	pmd = mm_find_pmd(mm, address);
 	if (!pmd) {
 		result = SCAN_PMD_NULL;
 		mem_cgroup_cancel_charge(new_page, memcg, true);
+		up_read(&mm->badger_trap_page_table_sem);
 		up_read(&mm->mmap_sem);
 		goto out_nolock;
 	}
@@ -1003,12 +1008,14 @@ static void collapse_huge_page(struct mm_struct *mm,
 	 * If it fails, we release mmap_sem and jump out_nolock.
 	 * Continuing to collapse causes inconsistency.
 	 */
-	if (!__collapse_huge_page_swapin(mm, vma, address, pmd, referenced)) {
+	if (!__collapse_huge_page_swapin(mm, vma, address, pmd, referenced, force, pftrace)) {
 		mem_cgroup_cancel_charge(new_page, memcg, true);
+		up_read(&mm->badger_trap_page_table_sem);
 		up_read(&mm->mmap_sem);
 		goto out_nolock;
 	}
 
+	up_read(&mm->badger_trap_page_table_sem);
 	up_read(&mm->mmap_sem);
 	/*
 	 * Prevent all access to pagetables with the exception of
@@ -1016,16 +1023,18 @@ static void collapse_huge_page(struct mm_struct *mm,
 	 * handled by the anon_vma lock + PG_lock.
 	 */
 	down_write(&mm->mmap_sem);
+	down_read(&mm->badger_trap_page_table_sem);
 	result = SCAN_ANY_PROCESS;
 	if (!mmget_still_valid(mm))
 		goto out;
-	result = hugepage_vma_revalidate(mm, address, &vma);
+	result = hugepage_vma_revalidate(mm, address, &vma, force);
 	if (result)
 		goto out;
 	/* check if the pmd is still valid */
 	if (mm_find_pmd(mm, address) != pmd)
 		goto out;
 
+	pr_info("promoting 2: vma=%p vaddr=%lx", vma, address);
 	anon_vma_lock_write(vma->anon_vma);
 
 	mmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, NULL, mm,
@@ -1072,7 +1081,9 @@ static void collapse_huge_page(struct mm_struct *mm,
 	 */
 	anon_vma_unlock_write(vma->anon_vma);
 
-	__collapse_huge_page_copy(pte, new_page, vma, address, pte_ptl);
+	pftrace->prep_start_tsc = rdtsc();
+	__collapse_huge_page_copy(pte, new_page, vma, address, pte_ptl, pftrace);
+	pftrace->prep_end_tsc = rdtsc();
 	pte_unmap(pte);
 	__SetPageUptodate(new_page);
 	pgtable = pmd_pgtable(_pmd);
@@ -1094,24 +1105,41 @@ static void collapse_huge_page(struct mm_struct *mm,
 	count_memcg_events(memcg, THP_COLLAPSE_ALLOC, 1);
 	lru_cache_add_active_or_unevictable(new_page, vma);
 	pgtable_trans_huge_deposit(mm, pmd, pgtable);
+	/* Make the page table entry as reserved for TLB miss tracking */
+	if(is_badger_trap_enabled(mm, address)) {
+		if (vma && !(vma->vm_flags & VM_EXEC)) {
+			// TODO markm currently handle anon pages only
+			if (vma_is_anonymous(vma)) {
+				_pmd = pmd_mkreserve(_pmd);
+			}
+		}
+	}
 	set_pmd_at(mm, address, pmd, _pmd);
 	update_mmu_cache_pmd(vma, address, pmd);
 	spin_unlock(pmd_ptl);
 
 	*hpage = NULL;
 
+	// The rest is cheap.
+	mm_stats_hist_measure(&mm_huge_page_promotion_work_cycles, rdtsc() - start);
+
 	khugepaged_pages_collapsed++;
 	result = SCAN_SUCCEED;
 out_up_write:
+	up_read(&mm->badger_trap_page_table_sem);
 	up_write(&mm->mmap_sem);
 out_nolock:
 	trace_mm_collapse_huge_page(mm, isolated, result);
-	return;
+	pr_info("results = %d", result);
+	return result;
 out:
 	mem_cgroup_cancel_charge(new_page, memcg, true);
 	goto out_up_write;
 }
 
+/* badger_trap_page_table_sem must be held when entering. It will always be
+ * released before returning.
+ */
 static int khugepaged_scan_pmd(struct mm_struct *mm,
 			       struct vm_area_struct *vma,
 			       unsigned long address,
@@ -1125,12 +1153,18 @@ static int khugepaged_scan_pmd(struct mm_struct *mm,
 	spinlock_t *ptl;
 	int node = NUMA_NO_NODE, unmapped = 0;
 	bool writable = false;
+	struct mm_cost_delta mm_cost_delta;
+	struct mm_action mm_action;
+	bool should_do;
+	struct mm_stats_pftrace pftrace; // dummy, not used
+	mm_stats_pftrace_init(&pftrace);
 
 	VM_BUG_ON(address & ~HPAGE_PMD_MASK);
 
 	pmd = mm_find_pmd(mm, address);
 	if (!pmd) {
 		result = SCAN_PMD_NULL;
+		up_read(&mm->badger_trap_page_table_sem);
 		goto out;
 	}
 
@@ -1227,9 +1261,25 @@ static int khugepaged_scan_pmd(struct mm_struct *mm,
 out_unmap:
 	pte_unmap_unlock(pte, ptl);
 	if (ret) {
-		node = khugepaged_find_target_node();
-		/* collapse_huge_page will return with the mmap_sem released */
-		collapse_huge_page(mm, address, hpage, node, referenced);
+		// (markm) run the estimator to check if we should create a 2MB page.
+		mm_action.address = address;
+		mm_action.action = MM_ACTION_PROMOTE_HUGE;
+		mm_action.huge_page_order = HPAGE_PMD_ORDER;
+		mm_estimate_changes(&mm_action, &mm_cost_delta);
+		should_do = mm_decide(&mm_cost_delta);
+
+		if (should_do) {
+			node = khugepaged_find_target_node();
+			/* collapse_huge_page will return with the mmap_sem released */
+			collapse_huge_page(mm, address, hpage, node, referenced,
+					/* force */ false, &pftrace);
+		} else {
+			up_read(&mm->badger_trap_page_table_sem);
+			up_read(&mm->mmap_sem);
+			ret = SCAN_MM_ECON_CANCEL;
+		}
+	} else {
+		up_read(&mm->badger_trap_page_table_sem);
 	}
 out:
 	trace_mm_khugepaged_scan_pmd(mm, page, writable, referenced,
@@ -1237,6 +1287,50 @@ static int khugepaged_scan_pmd(struct mm_struct *mm,
 	return ret;
 }
 
+int
+promote_to_huge(struct mm_struct *mm,
+		struct vm_area_struct *vma,
+		unsigned long address,
+		struct mm_stats_pftrace *pftrace)
+{
+	int node;
+	int result;
+	struct page *hpage = NULL;
+
+	pr_info("Attempting to promote %lx (vma=%p mm=%p)", address, vma, mm);
+
+	// Linux doesn't support huge pages for file-backed memory, which
+	// unfortunately rules out huge-page mappings for the text section.
+	if (vma->vm_file) {
+		pr_warn("Not promoting %lx: file-backed memory.", address);
+		return SCAN_FAIL;
+	}
+
+	down_read(&mm->mmap_sem);
+	down_read(&mm->badger_trap_page_table_sem);
+
+	node = khugepaged_find_target_node();
+	result = collapse_huge_page(mm, address, &hpage, node, 512,
+			/* force */ true, pftrace);
+
+	if (IS_ERR_OR_NULL(hpage)) {
+		pr_info("hpage is null or error");
+	} else {
+		put_page(hpage);
+	}
+
+	pr_info("Attempted to promote %lx: result=%d hpage=%p",
+			address, result, hpage);
+
+	if (result == SCAN_SUCCEED) {
+		mm_stats_set_flag(pftrace, MM_STATS_PF_HUGE_PROMOTION);
+	} else {
+		mm_stats_set_flag(pftrace, MM_STATS_PF_HUGE_PROMOTION_FAILED);
+	}
+
+	return result;
+}
+
 static void collect_mm_slot(struct mm_slot *mm_slot)
 {
 	struct mm_struct *mm = mm_slot->mm;
@@ -1999,9 +2093,12 @@ static unsigned int khugepaged_scan_mm_slot(unsigned int pages,
 				khugepaged_scan_file(mm, file, pgoff, hpage);
 				fput(file);
 			} else {
+				down_read(&mm->badger_trap_page_table_sem);
 				ret = khugepaged_scan_pmd(mm, vma,
 						khugepaged_scan.address,
 						hpage);
+				/* khugepaged_scan_pmd always releases the
+				 * badger_trap_page_table_sem. */
 			}
 			/* move to next address */
 			khugepaged_scan.address += HPAGE_PMD_SIZE;
@@ -2053,8 +2150,18 @@ static int khugepaged_has_work(void)
 
 static int khugepaged_wait_event(void)
 {
-	return !list_empty(&khugepaged_scan.mm_head) ||
-		kthread_should_stop();
+    struct mm_cost_delta mm_cost_delta;
+    struct mm_action mm_action = {
+        .action = MM_ACTION_RUN_PROMOTION,
+        .unused = 0,
+    };
+    bool should_run;
+    mm_estimate_changes(&mm_action, &mm_cost_delta);
+    should_run = mm_decide(&mm_cost_delta);
+
+	return should_run &&
+        (!list_empty(&khugepaged_scan.mm_head) ||
+		kthread_should_stop());
 }
 
 static void khugepaged_do_scan(void)
@@ -2063,6 +2170,7 @@ static void khugepaged_do_scan(void)
 	unsigned int progress = 0, pass_through_head = 0;
 	unsigned int pages = khugepaged_pages_to_scan;
 	bool wait = true;
+	u64 start = rdtsc();
 
 	barrier(); /* write khugepaged_pages_to_scan to local stack */
 
@@ -2078,10 +2186,8 @@ static void khugepaged_do_scan(void)
 		spin_lock(&khugepaged_mm_lock);
 		if (!khugepaged_scan.mm_slot)
 			pass_through_head++;
-		if (khugepaged_has_work() &&
-		    pass_through_head < 2)
-			progress += khugepaged_scan_mm_slot(pages - progress,
-							    &hpage);
+		if (khugepaged_has_work() && pass_through_head < 2)
+			progress += khugepaged_scan_mm_slot(pages - progress, &hpage);
 		else
 			progress = pages;
 		spin_unlock(&khugepaged_mm_lock);
@@ -2089,6 +2195,8 @@ static void khugepaged_do_scan(void)
 
 	if (!IS_ERR_OR_NULL(hpage))
 		put_page(hpage);
+
+	mm_stats_hist_measure(&mm_huge_page_promotion_scanning_cycles, rdtsc() - start);
 }
 
 static bool khugepaged_should_wakeup(void)
@@ -2114,7 +2222,7 @@ static void khugepaged_wait_work(void)
 	}
 
 	if (khugepaged_enabled())
-		wait_event_freezable(khugepaged_wait, khugepaged_wait_event());
+        wait_event_freezable(khugepaged_wait, khugepaged_wait_event());
 }
 
 static int khugepaged(void *none)
diff --git a/mm/ksm.c b/mm/ksm.c
index d17c7d57d0d8..2f909077fe2c 100644
--- a/mm/ksm.c
+++ b/mm/ksm.c
@@ -38,6 +38,7 @@
 #include <linux/freezer.h>
 #include <linux/oom.h>
 #include <linux/numa.h>
+#include <linux/mm_stats.h>
 
 #include <asm/tlbflush.h>
 #include "internal.h"
@@ -471,6 +472,8 @@ static int break_ksm(struct vm_area_struct *vma, unsigned long addr)
 {
 	struct page *page;
 	vm_fault_t ret = 0;
+	struct mm_stats_pftrace pftrace; // dummy, not used
+	mm_stats_pftrace_init(&pftrace);
 
 	do {
 		cond_resched();
@@ -480,7 +483,8 @@ static int break_ksm(struct vm_area_struct *vma, unsigned long addr)
 			break;
 		if (PageKsm(page))
 			ret = handle_mm_fault(vma, addr,
-					FAULT_FLAG_WRITE | FAULT_FLAG_REMOTE);
+					FAULT_FLAG_WRITE | FAULT_FLAG_REMOTE,
+					&pftrace);
 		else
 			ret = VM_FAULT_WRITE;
 		put_page(page);
diff --git a/mm/madvise.c b/mm/madvise.c
index bcdb6a042787..6e7b748b75ae 100644
--- a/mm/madvise.c
+++ b/mm/madvise.c
@@ -1044,7 +1044,8 @@ madvise_behavior_valid(int behavior)
  *  -EBADF  - map exists, but area maps something that isn't a file.
  *  -EAGAIN - a kernel resource was temporarily unavailable.
  */
-SYSCALL_DEFINE3(madvise, unsigned long, start, size_t, len_in, int, behavior)
+int do_madvise(struct mm_struct *mm, unsigned long start, size_t len_in,
+	       int behavior)
 {
 	unsigned long end, tmp;
 	struct vm_area_struct *vma, *prev;
@@ -1082,10 +1083,10 @@ SYSCALL_DEFINE3(madvise, unsigned long, start, size_t, len_in, int, behavior)
 
 	write = madvise_need_mmap_write(behavior);
 	if (write) {
-		if (down_write_killable(&current->mm->mmap_sem))
+		if (down_write_killable(&mm->mmap_sem))
 			return -EINTR;
 	} else {
-		down_read(&current->mm->mmap_sem);
+		down_read(&mm->mmap_sem);
 	}
 
 	/*
@@ -1093,7 +1094,7 @@ SYSCALL_DEFINE3(madvise, unsigned long, start, size_t, len_in, int, behavior)
 	 * ranges, just ignore them, but return -ENOMEM at the end.
 	 * - different from the way of handling in mlock etc.
 	 */
-	vma = find_vma_prev(current->mm, start, &prev);
+	vma = find_vma_prev(mm, start, &prev);
 	if (vma && start > vma->vm_start)
 		prev = vma;
 
@@ -1130,14 +1131,19 @@ SYSCALL_DEFINE3(madvise, unsigned long, start, size_t, len_in, int, behavior)
 		if (prev)
 			vma = prev->vm_next;
 		else	/* madvise_remove dropped mmap_sem */
-			vma = find_vma(current->mm, start);
+			vma = find_vma(mm, start);
 	}
 out:
 	blk_finish_plug(&plug);
 	if (write)
-		up_write(&current->mm->mmap_sem);
+		up_write(&mm->mmap_sem);
 	else
-		up_read(&current->mm->mmap_sem);
+		up_read(&mm->mmap_sem);
 
 	return error;
 }
+
+SYSCALL_DEFINE3(madvise, unsigned long, start, size_t, len_in, int, behavior)
+{
+	return do_madvise(current->mm, start, len_in, behavior);
+}
diff --git a/mm/memory.c b/mm/memory.c
index 45442d9a4f52..ab453be3d635 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -71,6 +71,11 @@
 #include <linux/dax.h>
 #include <linux/oom.h>
 #include <linux/numa.h>
+#include <linux/mm_econ.h>
+#include <linux/mm_stats.h>
+#include <linux/proc_fs.h>
+#include <linux/memory.h>
+#include <linux/badger_trap.h>
 
 #include <trace/events/kmem.h>
 
@@ -107,6 +112,80 @@ EXPORT_SYMBOL(mem_map);
 void *high_memory;
 EXPORT_SYMBOL(high_memory);
 
+static unsigned long dump_mapping;
+static struct proc_dir_entry *dump_mapping_ent;
+
+static ssize_t dump_mapping_read_cb(
+		struct file *file, char __user *ubuf,
+		size_t count, loff_t *ppos)
+{
+	char buf[100];
+	int len=0;
+	unsigned long pfn = 0;
+	struct page *page = NULL;
+	bool is_huge = false;
+
+	if(*ppos > 0)
+		return 0;
+
+	get_page_mapping(dump_mapping, &pfn, &page, &is_huge);
+
+	if (page) {
+		len += sprintf(buf, "0x%lx pfn=0x%lx sp=%p %s\n",
+				dump_mapping, pfn, page, is_huge ? "huge" : "base");
+	} else {
+		len += sprintf(buf, "0x%lx is not mapped\n", dump_mapping);
+	}
+
+	if(count < len)
+		return 0;
+
+	if(copy_to_user(ubuf, buf, len))
+		return -EFAULT;
+
+	*ppos = len;
+	return len;
+}
+
+static ssize_t dump_mapping_write_cb(
+		struct file *file, const char __user *ubuf,
+		size_t len, loff_t *offset)
+{
+	int num;
+	unsigned long val;
+	char input[20];
+
+	if(*offset > 0 || len > 20) {
+		return -EFAULT;
+	}
+
+	if(copy_from_user(input, ubuf, len)) {
+		return -EFAULT;
+	}
+
+	num = sscanf(input, "%lx", &val);
+	if(num != 1) {
+		return -EINVAL;
+	}
+
+	dump_mapping = val;
+
+	pr_warn("dump_mapping: 0x%lx\n", dump_mapping);
+
+	return len;
+}
+
+static struct file_operations dump_mapping_ops =
+{
+	.write = dump_mapping_write_cb,
+	.read = dump_mapping_read_cb,
+};
+
+void init_dump_mapping(void)
+{
+    dump_mapping_ent = proc_create("dump_mapping", 0444, NULL, &dump_mapping_ops);
+}
+
 /*
  * Randomize the address space (stacks, mmaps, brk, etc.).
  *
@@ -592,7 +671,13 @@ static void print_bad_pte(struct vm_area_struct *vma, unsigned long addr,
 struct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,
 			    pte_t pte)
 {
-	unsigned long pfn = pte_pfn(pte);
+	unsigned long pfn;
+
+	if (vma->vm_mm && vma->vm_mm->badger_trap_was_enabled
+		&& is_pte_reserved(pte))
+	    pte = pte_unreserve(pte);
+
+	pfn = pte_pfn(pte);
 
 	if (IS_ENABLED(CONFIG_ARCH_HAS_PTE_SPECIAL)) {
 		if (likely(!pte_special(pte)))
@@ -606,6 +691,7 @@ struct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,
 		if (pte_devmap(pte))
 			return NULL;
 
+		pr_warn("Bad PTE: marked special unexpectedly\n");
 		print_bad_pte(vma, addr, pte, NULL);
 		return NULL;
 	}
@@ -632,6 +718,8 @@ struct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,
 
 check_pfn:
 	if (unlikely(pfn > highest_memmap_pfn)) {
+		pr_warn("Bad PTE: PFN is too high! %lx > %lx\n",
+			pfn, highest_memmap_pfn);
 		print_bad_pte(vma, addr, pte, NULL);
 		return NULL;
 	}
@@ -1040,7 +1128,16 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 	flush_tlb_batched_pending(mm);
 	arch_enter_lazy_mmu_mode();
 	do {
-		pte_t ptent = *pte;
+		pte_t ptent;
+
+		if (vma->vm_mm && vma->vm_mm->badger_trap_was_enabled
+			&& is_pte_reserved(ptent))
+		{
+			*pte = pte_unreserve(*pte);
+		}
+
+		ptent = *pte;
+
 		if (pte_none(ptent))
 			continue;
 
@@ -1078,8 +1175,9 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 			}
 			rss[mm_counter(page)]--;
 			page_remove_rmap(page, false);
-			if (unlikely(page_mapcount(page) < 0))
+			if (unlikely(page_mapcount(page) < 0)) {
 				print_bad_pte(vma, addr, ptent, page);
+			}
 			if (unlikely(__tlb_remove_page(tlb, page))) {
 				force_flush = 1;
 				addr += PAGE_SIZE;
@@ -1122,8 +1220,9 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 			page = migration_entry_to_page(entry);
 			rss[mm_counter(page)]--;
 		}
-		if (unlikely(!free_swap_and_cache(entry)))
+		if (unlikely(!free_swap_and_cache(entry))) {
 			print_bad_pte(vma, addr, ptent, NULL);
+		}
 		pte_clear_not_present_full(mm, addr, pte, tlb->fullmm);
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 
@@ -2438,7 +2537,8 @@ static inline void wp_page_reuse(struct vm_fault *vmf)
  *   held to the old page, as well as updating the rmap.
  * - In any case, unlock the PTL and drop the reference we took to the old page.
  */
-static vm_fault_t wp_page_copy(struct vm_fault *vmf)
+static vm_fault_t wp_page_copy(struct vm_fault *vmf,
+			       struct mm_stats_pftrace *pftrace)
 {
 	struct vm_area_struct *vma = vmf->vma;
 	struct mm_struct *mm = vma->vm_mm;
@@ -2453,13 +2553,21 @@ static vm_fault_t wp_page_copy(struct vm_fault *vmf)
 		goto oom;
 
 	if (is_zero_pfn(pte_pfn(vmf->orig_pte))) {
+		pftrace->alloc_start_tsc = rdtsc();
 		new_page = alloc_zeroed_user_highpage_movable(vma,
 							      vmf->address);
+		pftrace->alloc_end_tsc = rdtsc();
+		mm_stats_check_alloc_fallback(pftrace);
+		mm_stats_check_alloc_zeroing(pftrace);
 		if (!new_page)
 			goto oom;
 	} else {
+		pftrace->alloc_start_tsc = rdtsc();
 		new_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma,
 				vmf->address);
+		pftrace->alloc_end_tsc = rdtsc();
+		mm_stats_check_alloc_fallback(pftrace);
+		mm_stats_check_alloc_zeroing(pftrace);
 		if (!new_page)
 			goto oom;
 
@@ -2504,6 +2612,14 @@ static vm_fault_t wp_page_copy(struct vm_fault *vmf)
 		flush_cache_page(vma, vmf->address, pte_pfn(vmf->orig_pte));
 		entry = mk_pte(new_page, vma->vm_page_prot);
 		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
+
+		/* Make the page table entry as reserved for TLB miss tracking */
+		if(is_badger_trap_enabled(mm, vmf->address)
+			&& !(vmf->flags & FAULT_FLAG_INSTRUCTION))
+		{
+			entry = pte_mkreserve(entry);
+		}
+
 		/*
 		 * Clear the pte entry and flush it first, before updating the
 		 * pte with the new entry. This will avoid a race condition
@@ -2692,7 +2808,8 @@ static vm_fault_t wp_page_shared(struct vm_fault *vmf)
  * but allow concurrent faults), with pte both mapped and locked.
  * We return with mmap_sem still held, but pte unmapped and unlocked.
  */
-static vm_fault_t do_wp_page(struct vm_fault *vmf)
+static vm_fault_t do_wp_page(struct vm_fault *vmf,
+			     struct mm_stats_pftrace *pftrace)
 	__releases(vmf->ptl)
 {
 	struct vm_area_struct *vma = vmf->vma;
@@ -2711,7 +2828,7 @@ static vm_fault_t do_wp_page(struct vm_fault *vmf)
 			return wp_pfn_shared(vmf);
 
 		pte_unmap_unlock(vmf->pte, vmf->ptl);
-		return wp_page_copy(vmf);
+		return wp_page_copy(vmf, pftrace);
 	}
 
 	/*
@@ -2773,7 +2890,7 @@ static vm_fault_t do_wp_page(struct vm_fault *vmf)
 	get_page(vmf->page);
 
 	pte_unmap_unlock(vmf->pte, vmf->ptl);
-	return wp_page_copy(vmf);
+	return wp_page_copy(vmf, pftrace);
 }
 
 static void unmap_mapping_range_vma(struct vm_area_struct *vma,
@@ -2880,7 +2997,8 @@ EXPORT_SYMBOL(unmap_mapping_range);
  * We return with the mmap_sem locked or unlocked in the same cases
  * as does filemap_fault().
  */
-vm_fault_t do_swap_page(struct vm_fault *vmf)
+vm_fault_t do_swap_page(struct vm_fault *vmf,
+			struct mm_stats_pftrace *pftrace)
 {
 	struct vm_area_struct *vma = vmf->vma;
 	struct page *page = NULL, *swapcache;
@@ -3030,6 +3148,17 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 	flush_icache_page(vma, page);
 	if (pte_swp_soft_dirty(vmf->orig_pte))
 		pte = pte_mksoft_dirty(pte);
+
+	/* Make the page table entry as reserved for TLB miss tracking */
+	if(is_badger_trap_enabled(vma->vm_mm, vmf->address)
+		&& !(vmf->flags & FAULT_FLAG_INSTRUCTION))
+	{
+		// TODO markm: currently only handle anon memory
+		if (vma_is_anonymous(vma)) {
+			// pte = pte_mkreserve(pte); // TODO markm uncomment -- this one seems to cause a panic/pt corruption somehow
+		}
+	}
+
 	set_pte_at(vma->vm_mm, vmf->address, vmf->pte, pte);
 	arch_do_swap_page(vma->vm_mm, vma, vmf->address, pte, vmf->orig_pte);
 	vmf->orig_pte = pte;
@@ -3064,7 +3193,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 	}
 
 	if (vmf->flags & FAULT_FLAG_WRITE) {
-		ret |= do_wp_page(vmf);
+		ret |= do_wp_page(vmf, pftrace);
 		if (ret & VM_FAULT_ERROR)
 			ret &= VM_FAULT_ERROR;
 		goto out;
@@ -3095,7 +3224,8 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
  * but allow concurrent faults), and pte mapped but not yet locked.
  * We return with mmap_sem still held, but pte unmapped and unlocked.
  */
-static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
+static vm_fault_t do_anonymous_page(struct vm_fault *vmf,
+				    struct mm_stats_pftrace *pftrace)
 {
 	struct vm_area_struct *vma = vmf->vma;
 	struct mem_cgroup *memcg;
@@ -3127,6 +3257,7 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
 	/* Use the zero-page for reads */
 	if (!(vmf->flags & FAULT_FLAG_WRITE) &&
 			!mm_forbids_zeropage(vma->vm_mm)) {
+		mm_stats_set_flag(pftrace, MM_STATS_PF_ZERO);
 		entry = pte_mkspecial(pfn_pte(my_zero_pfn(vmf->address),
 						vma->vm_page_prot));
 		vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,
@@ -3147,7 +3278,11 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
 	/* Allocate our own private page. */
 	if (unlikely(anon_vma_prepare(vma)))
 		goto oom;
+	pftrace->alloc_start_tsc = rdtsc();
 	page = alloc_zeroed_user_highpage_movable(vma, vmf->address);
+	pftrace->alloc_end_tsc = rdtsc();
+	mm_stats_check_alloc_fallback(pftrace);
+	mm_stats_check_alloc_zeroing(pftrace);
 	if (!page)
 		goto oom;
 
@@ -3188,6 +3323,13 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
 	mem_cgroup_commit_charge(page, memcg, false, false);
 	lru_cache_add_active_or_unevictable(page, vma);
 setpte:
+	/* Make the page table entry as reserved for TLB miss tracking */
+	if(is_badger_trap_enabled(vma->vm_mm, vmf->address)
+		&& !(vmf->flags & FAULT_FLAG_INSTRUCTION))
+	{
+		entry = pte_mkreserve(entry);
+	}
+
 	set_pte_at(vma->vm_mm, vmf->address, vmf->pte, entry);
 
 	/* No need to invalidate - it was non-present before */
@@ -3454,6 +3596,14 @@ vm_fault_t alloc_set_pte(struct vm_fault *vmf, struct mem_cgroup *memcg,
 		inc_mm_counter_fast(vma->vm_mm, mm_counter_file(page));
 		page_add_file_rmap(page, false);
 	}
+
+	/* Make the page table entry as reserved for TLB miss tracking */
+	if(is_badger_trap_enabled(vma->vm_mm, vmf->address)
+		&& !(vmf->flags & FAULT_FLAG_INSTRUCTION))
+	{
+		entry = pte_mkreserve(entry);
+	}
+
 	set_pte_at(vma->vm_mm, vmf->address, vmf->pte, entry);
 
 	/* no need to invalidate: a not-present page won't be cached */
@@ -3534,6 +3684,7 @@ static int __init fault_around_debugfs(void)
 {
 	debugfs_create_file_unsafe("fault_around_bytes", 0644, NULL, NULL,
 				   &fault_around_bytes_fops);
+	init_dump_mapping();
 	return 0;
 }
 late_initcall(fault_around_debugfs);
@@ -3618,11 +3769,13 @@ static vm_fault_t do_fault_around(struct vm_fault *vmf)
 	return ret;
 }
 
-static vm_fault_t do_read_fault(struct vm_fault *vmf)
+static vm_fault_t do_read_fault(struct vm_fault *vmf, struct mm_stats_pftrace *pftrace)
 {
 	struct vm_area_struct *vma = vmf->vma;
 	vm_fault_t ret = 0;
 
+	mm_stats_set_flag(pftrace, MM_STATS_PF_NOT_ANON_READ);
+
 	/*
 	 * Let's call ->map_pages() first and use ->fault() as fallback
 	 * if page by the offset is not ready to be mapped (cold cache or
@@ -3645,15 +3798,21 @@ static vm_fault_t do_read_fault(struct vm_fault *vmf)
 	return ret;
 }
 
-static vm_fault_t do_cow_fault(struct vm_fault *vmf)
+static vm_fault_t do_cow_fault(struct vm_fault *vmf, struct mm_stats_pftrace *pftrace)
 {
 	struct vm_area_struct *vma = vmf->vma;
 	vm_fault_t ret;
 
+	mm_stats_set_flag(pftrace, MM_STATS_PF_NOT_ANON_COW);
+
 	if (unlikely(anon_vma_prepare(vma)))
 		return VM_FAULT_OOM;
 
+	pftrace->alloc_start_tsc = rdtsc();
 	vmf->cow_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, vmf->address);
+	pftrace->alloc_end_tsc = rdtsc();
+	mm_stats_check_alloc_fallback(pftrace);
+	mm_stats_check_alloc_zeroing(pftrace);
 	if (!vmf->cow_page)
 		return VM_FAULT_OOM;
 
@@ -3669,8 +3828,10 @@ static vm_fault_t do_cow_fault(struct vm_fault *vmf)
 	if (ret & VM_FAULT_DONE_COW)
 		return ret;
 
+	pftrace->prep_start_tsc = rdtsc();
 	copy_user_highpage(vmf->cow_page, vmf->page, vmf->address, vma);
 	__SetPageUptodate(vmf->cow_page);
+	pftrace->prep_end_tsc = rdtsc();
 
 	ret |= finish_fault(vmf);
 	unlock_page(vmf->page);
@@ -3684,11 +3845,13 @@ static vm_fault_t do_cow_fault(struct vm_fault *vmf)
 	return ret;
 }
 
-static vm_fault_t do_shared_fault(struct vm_fault *vmf)
+static vm_fault_t do_shared_fault(struct vm_fault *vmf, struct mm_stats_pftrace *pftrace)
 {
 	struct vm_area_struct *vma = vmf->vma;
 	vm_fault_t ret, tmp;
 
+	mm_stats_set_flag(pftrace, MM_STATS_PF_NOT_ANON_SHARED);
+
 	ret = __do_fault(vmf);
 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
 		return ret;
@@ -3727,7 +3890,7 @@ static vm_fault_t do_shared_fault(struct vm_fault *vmf)
  * If mmap_sem is released, vma may become invalid (for example
  * by other thread calling munmap()).
  */
-static vm_fault_t do_fault(struct vm_fault *vmf)
+static vm_fault_t do_fault(struct vm_fault *vmf, struct mm_stats_pftrace *pftrace)
 {
 	struct vm_area_struct *vma = vmf->vma;
 	struct mm_struct *vm_mm = vma->vm_mm;
@@ -3763,11 +3926,11 @@ static vm_fault_t do_fault(struct vm_fault *vmf)
 			pte_unmap_unlock(vmf->pte, vmf->ptl);
 		}
 	} else if (!(vmf->flags & FAULT_FLAG_WRITE))
-		ret = do_read_fault(vmf);
+		ret = do_read_fault(vmf, pftrace);
 	else if (!(vma->vm_flags & VM_SHARED))
-		ret = do_cow_fault(vmf);
+		ret = do_cow_fault(vmf, pftrace);
 	else
-		ret = do_shared_fault(vmf);
+		ret = do_shared_fault(vmf, pftrace);
 
 	/* preallocated pagetable is unused: free it */
 	if (vmf->prealloc_pte) {
@@ -3882,26 +4045,30 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)
 	return 0;
 }
 
-static inline vm_fault_t create_huge_pmd(struct vm_fault *vmf)
+static inline vm_fault_t create_huge_pmd(struct vm_fault *vmf,
+					 struct mm_stats_pftrace *pftrace,
+					 bool require_prezeroed)
 {
 	if (vma_is_anonymous(vmf->vma))
-		return do_huge_pmd_anonymous_page(vmf);
+		return do_huge_pmd_anonymous_page(vmf, pftrace, require_prezeroed);
 	if (vmf->vma->vm_ops->huge_fault)
 		return vmf->vma->vm_ops->huge_fault(vmf, PE_SIZE_PMD);
 	return VM_FAULT_FALLBACK;
 }
 
 /* `inline' is required to avoid gcc 4.1.2 build error */
-static inline vm_fault_t wp_huge_pmd(struct vm_fault *vmf, pmd_t orig_pmd)
+static inline vm_fault_t wp_huge_pmd(struct vm_fault *vmf, pmd_t orig_pmd,
+				     struct mm_stats_pftrace *pftrace)
 {
 	if (vma_is_anonymous(vmf->vma))
-		return do_huge_pmd_wp_page(vmf, orig_pmd);
+		return do_huge_pmd_wp_page(vmf, orig_pmd, pftrace);
 	if (vmf->vma->vm_ops->huge_fault)
 		return vmf->vma->vm_ops->huge_fault(vmf, PE_SIZE_PMD);
 
 	/* COW handled on pte level: split pmd */
 	VM_BUG_ON_VMA(vmf->vma->vm_flags & VM_SHARED, vmf->vma);
 	__split_huge_pmd(vmf->vma, vmf->pmd, vmf->address, false, NULL);
+	mm_stats_set_flag(pftrace, MM_STATS_PF_HUGE_SPLIT);
 
 	return VM_FAULT_FALLBACK;
 }
@@ -3935,6 +4102,76 @@ static vm_fault_t wp_huge_pud(struct vm_fault *vmf, pud_t orig_pud)
 	return VM_FAULT_FALLBACK;
 }
 
+/*
+ * This function handles the fake page fault introduced to perform TLB miss
+ * studies. We can perform our work in this fuction on the page table entries.
+ */
+static int do_fake_page_fault(struct mm_struct *mm, struct vm_area_struct *vma,
+		unsigned long address, pte_t *page_table, pmd_t *pmd,
+		unsigned int flags, spinlock_t *ptl)
+{
+        unsigned long *touch_page_addr;
+        unsigned long touched;
+	unsigned long ret;
+	/*
+	static unsigned int consecutive = 0;
+	static unsigned long prev_address = 0;
+
+	if(address == prev_address)
+		consecutive++;
+	else
+	{
+		consecutive = 0;
+		prev_address = address;
+	}
+
+	if(consecutive > 1)
+	{
+		*page_table = pte_unreserve(*page_table);
+		pte_unmap_unlock(page_table, ptl);
+		return 0;
+	}
+	*/
+
+	if(flags & FAULT_FLAG_WRITE)
+		*page_table = pte_mkdirty(*page_table);
+
+	*page_table = pte_mkyoung(*page_table);
+	*page_table = pte_unreserve(*page_table);
+
+	pte_unmap_unlock(page_table, ptl);
+
+	touch_page_addr = (void *)(address & PAGE_MASK);
+	ret = copy_from_user(&touched,
+		(__force const void __user *)touch_page_addr,
+		sizeof(unsigned long));
+
+	if(ret) {
+		return VM_FAULT_SIGBUS;
+	}
+
+	/* Here where we do all our analysis */
+	if (flags & FAULT_FLAG_WRITE)
+	    atomic64_inc(&mm->bt_stats->total_dtlb_4kb_store_misses);
+	else
+	    atomic64_inc(&mm->bt_stats->total_dtlb_4kb_load_misses);
+
+	/*
+	if (vma) {
+	    if (flags & FAULT_FLAG_WRITE)
+		vma->bt_stats.total_dtlb_4kb_store_misses++;
+	    else
+		vma->bt_stats.total_dtlb_4kb_load_misses++;
+	}
+	*/
+
+	pte_offset_map_lock(mm, pmd, address, &ptl);
+	*page_table = pte_mkreserve(*page_table);
+	pte_unmap_unlock(page_table, ptl);
+
+	return 0;
+}
+
 /*
  * These routines also need to handle stuff like marking pages dirty
  * and/or accessed for architectures that don't do it in hardware (most
@@ -3950,9 +4187,101 @@ static vm_fault_t wp_huge_pud(struct vm_fault *vmf, pud_t orig_pud)
  * The mmap_sem may have been released depending on flags and our return value.
  * See filemap_fault() and __lock_page_or_retry().
  */
-static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
+static vm_fault_t handle_pte_fault(struct vm_fault *vmf,
+				   struct mm_stats_pftrace *pftrace)
 {
 	pte_t entry;
+	vm_fault_t ret;
+
+	if (vmf->vma->vm_mm && vmf->vma->vm_mm->badger_trap_was_enabled)
+	{
+	    /*
+	     * I'm not sure when this would happen, but just in case: if we ever
+	     * encounter a page reserved (by badger trap) when we have taken an
+	     * instruction fault, unreserve it. I think maybe this can happen for
+	     * JIT-compiled languages where code might be allocated and run off of
+	     * the heap (markm).
+	     */
+	    if((vmf->flags & FAULT_FLAG_INSTRUCTION)
+		    || !is_badger_trap_enabled(vmf->vma->vm_mm, vmf->address))
+	    {
+		    vmf->pte = pte_offset_map(vmf->pmd, vmf->address);
+		    if (vmf->pte) {
+			vmf->ptl = pte_lockptr(vmf->vma->vm_mm, vmf->pmd);
+			spin_lock(vmf->ptl);
+			if (is_pte_reserved(*vmf->pte))
+				*vmf->pte = pte_unreserve(*vmf->pte);
+			spin_unlock(vmf->ptl);
+		    }
+		    pte_unmap(vmf->pte);
+	    }
+
+	    /* We need to figure out if the page fault is a fake page fault or not.
+	     * If it is a fake page fault, we need to handle it specially. It has to
+	     * be made sure that the special page fault is not on instruction fault.
+	     * Our technique cannot not handle instruction page fault yet.
+	     *
+	     * We can have two cases when we have a fake page fault:
+	     * 1. We have taken a fake page fault on a COW page. A
+	     * 	fake page fault on a COW page if for reading only
+	     * 	has to be considered a normal fake page fault. But
+	     * 	for writing purposes need to be handled correctly.
+	     * 2. We have taken a fake page fault on a normal page.
+	     */
+	    if(!(vmf->flags & FAULT_FLAG_INSTRUCTION)
+		    && likely(!pmd_none(*vmf->pmd))
+		    && (vmf->pte = pte_offset_map(vmf->pmd, vmf->address)) // NOTE: assignment + side_effects
+		    && pte_present(*vmf->pte))
+	    {
+		entry = *vmf->pte;
+
+		if (is_pte_reserved(entry) && !pte_protnone(entry))
+		{
+		    if((vmf->flags & FAULT_FLAG_WRITE)
+			    && !pte_write(entry))
+		    {
+			mm_stats_set_flag(pftrace, MM_STATS_PF_WP);
+
+			// We want to do this here because we want
+			// do_wp_page not to see the magical reserved bit.
+			vmf->ptl = pte_lockptr(vmf->vma->vm_mm, vmf->pmd);
+			spin_lock(vmf->ptl);
+			*vmf->pte = pte_unreserve(*vmf->pte);
+			vmf->orig_pte = *vmf->pte;
+
+			ret = do_wp_page(vmf, pftrace);
+			// Returns with pte unmapped, unlocked.
+
+			pte_offset_map_lock(vmf->vma->vm_mm, vmf->pmd, vmf->address, &vmf->ptl);
+			if (pte_present(*vmf->pte))
+			    *vmf->pte = pte_mkreserve(*vmf->pte);
+			pte_unmap_unlock(vmf->pte, vmf->ptl);
+
+			return ret;
+		    }
+		    else
+		    {
+			mm_stats_set_flag(pftrace, MM_STATS_PF_BADGER_TRAP);
+			vmf->ptl = pte_lockptr(vmf->vma->vm_mm, vmf->pmd);
+			spin_lock(vmf->ptl);
+			return do_fake_page_fault(vmf->vma->vm_mm, vmf->vma,
+				vmf->address, vmf->pte, vmf->pmd,
+				vmf->flags, vmf->ptl);
+		    }
+		}
+
+		vmf->ptl = pte_lockptr(vmf->vma->vm_mm, vmf->pmd);
+		spin_lock(vmf->ptl);
+		if (is_badger_trap_enabled(vmf->vma->vm_mm, vmf->address)
+			&& !pte_protnone(*vmf->pte))
+		{
+		    *vmf->pte = pte_mkreserve(*vmf->pte);
+		} else {
+		    *vmf->pte = pte_unreserve(*vmf->pte);
+		}
+		pte_unmap_unlock(vmf->pte, vmf->ptl);
+	    }
+	}
 
 	if (unlikely(pmd_none(*vmf->pmd))) {
 		/*
@@ -3992,16 +4321,22 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
 
 	if (!vmf->pte) {
 		if (vma_is_anonymous(vmf->vma))
-			return do_anonymous_page(vmf);
-		else
-			return do_fault(vmf);
+			return do_anonymous_page(vmf, pftrace);
+		else {
+			mm_stats_set_flag(pftrace, MM_STATS_PF_NOT_ANON);
+			return do_fault(vmf, pftrace);
+		}
 	}
 
-	if (!pte_present(vmf->orig_pte))
-		return do_swap_page(vmf);
+	if (!pte_present(vmf->orig_pte)) {
+		mm_stats_set_flag(pftrace, MM_STATS_PF_SWAP);
+		return do_swap_page(vmf, pftrace);
+	}
 
-	if (pte_protnone(vmf->orig_pte) && vma_is_accessible(vmf->vma))
+	if (pte_protnone(vmf->orig_pte) && vma_is_accessible(vmf->vma)) {
+		mm_stats_set_flag(pftrace, MM_STATS_PF_NUMA);
 		return do_numa_page(vmf);
+	}
 
 	vmf->ptl = pte_lockptr(vmf->vma->vm_mm, vmf->pmd);
 	spin_lock(vmf->ptl);
@@ -4009,8 +4344,10 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
 	if (unlikely(!pte_same(*vmf->pte, entry)))
 		goto unlock;
 	if (vmf->flags & FAULT_FLAG_WRITE) {
-		if (!pte_write(entry))
-			return do_wp_page(vmf);
+		if (!pte_write(entry)) {
+			mm_stats_set_flag(pftrace, MM_STATS_PF_WP);
+			return do_wp_page(vmf, pftrace);
+		}
 		entry = pte_mkdirty(entry);
 	}
 	entry = pte_mkyoung(entry);
@@ -4027,11 +4364,69 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
 		if (vmf->flags & FAULT_FLAG_WRITE)
 			flush_tlb_fix_spurious_fault(vmf->vma, vmf->address);
 	}
+
 unlock:
 	pte_unmap_unlock(vmf->pte, vmf->ptl);
 	return 0;
 }
 
+static int transparent_fake_fault(struct vm_fault *vmf)
+{
+        unsigned long *touch_page_addr;
+        unsigned long touched;
+        unsigned long ret;
+	/*
+        static unsigned int consecutive = 0;
+        static unsigned long prev_address = 0;
+
+        if(vmf->address == prev_address)
+                consecutive++;
+        else
+        {
+                consecutive = 0;
+                prev_address = vmf->address;
+        }
+
+        if(consecutive > 1)
+        {
+                *vmf->pmd = pmd_unreserve(*vmf->pmd);
+                return 0;
+        }
+	*/
+
+        if(vmf->flags & FAULT_FLAG_WRITE)
+                *vmf->pmd = pmd_mkdirty(*vmf->pmd);
+
+        *vmf->pmd = pmd_mkyoung(*vmf->pmd);
+        *vmf->pmd = pmd_unreserve(*vmf->pmd);
+
+        touch_page_addr = (void *)(vmf->address & PAGE_MASK);
+        ret = copy_from_user(&touched,
+		(__force const void __user *)touch_page_addr,
+		sizeof(unsigned long));
+
+        if(ret)
+                return VM_FAULT_SIGBUS;
+
+        /* Here where we do all our analysis */
+	if (vmf->flags & FAULT_FLAG_WRITE)
+	    atomic64_inc(&vmf->vma->vm_mm->bt_stats->total_dtlb_2mb_store_misses);
+	else
+	    atomic64_inc(&vmf->vma->vm_mm->bt_stats->total_dtlb_2mb_load_misses);
+
+	/*
+	if (vmf->vma) {
+	    if (vmf->flags & FAULT_FLAG_WRITE)
+		vmf->vma->bt_stats.total_dtlb_2mb_store_misses++;
+	    else
+		vmf->vma->bt_stats.total_dtlb_2mb_load_misses++;
+	}
+	*/
+
+        *vmf->pmd = pmd_mkreserve(*vmf->pmd);
+        return 0;
+}
+
 /*
  * By the time we get here, we already hold the mm semaphore
  *
@@ -4039,7 +4434,8 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
  * return value.  See filemap_fault() and __lock_page_or_retry().
  */
 static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
-		unsigned long address, unsigned int flags)
+		unsigned long address, unsigned int flags,
+		struct mm_stats_pftrace *pftrace)
 {
 	struct vm_fault vmf = {
 		.vma = vma,
@@ -4052,22 +4448,89 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 	struct mm_struct *mm = vma->vm_mm;
 	pgd_t *pgd;
 	p4d_t *p4d;
+	spinlock_t *ptl;
 	vm_fault_t ret;
+	struct mm_cost_delta mm_cost_delta;
+	struct mm_action mm_action;
+	bool should_do;
+
+	// (markm) cr3->pgd->p4d->pud->pmd->pt->page
 
 	pgd = pgd_offset(mm, address);
 	p4d = p4d_alloc(mm, pgd, address);
 	if (!p4d)
 		return VM_FAULT_OOM;
 
+	vmf.pud = pud_offset(p4d, address);
+
+	/* Check for transparent 1GB huge pages that are marked reserved. */
+	if (mm && mm->badger_trap_was_enabled && !(vmf.flags & FAULT_FLAG_INSTRUCTION)
+		&& vmf.pud && pud_trans_huge(*vmf.pud))
+	{
+	    pud_t orig_pud = *vmf.pud;
+
+	    if ((vmf.flags & FAULT_FLAG_WRITE) && is_pud_reserved(orig_pud)
+		    && !pud_write(orig_pud) && pud_present(orig_pud))
+	    {
+		mm_stats_set_flag(pftrace, MM_STATS_PF_VERY_HUGE_PAGE);
+		mm_stats_set_flag(pftrace, MM_STATS_PF_WP);
+		// NOTE: This is pseudocode... Linux doesn't support
+		// transparent 1GB huge pages yet...
+		//
+		// return do_huge_pud_wp_page();
+		goto escape_pud;
+	    }
+
+	    if (is_pud_reserved(orig_pud) && pud_present(orig_pud))
+	    {
+		mm_stats_set_flag(pftrace, MM_STATS_PF_VERY_HUGE_PAGE);
+		mm_stats_set_flag(pftrace, MM_STATS_PF_BADGER_TRAP);
+		// NOTE: This is pseudocode... Linux doesn't support
+		// transparent 1GB huge pages yet...
+		//
+		// ret = transparent_1gb_fake_fault();
+		// return ret;
+		goto escape_pud;
+	    }
+
+	    ptl = pud_lock(mm, vmf.pud);
+	    // We use the "live" pud here.
+	    if (pud_present(*vmf.pud)
+		    && is_badger_trap_enabled(vmf.vma->vm_mm, vmf.address))
+	    {
+		// *vmf.pud = pud_mkreserve(*vmf.pud); // TODO markm uncomment
+	    } else if (pud_present(*vmf.pud))
+	    {
+		*vmf.pud = pud_unreserve(*vmf.pud);
+	    }
+	    spin_unlock(ptl);
+	}
+
+escape_pud:
 	vmf.pud = pud_alloc(mm, p4d, address);
 	if (!vmf.pud)
 		return VM_FAULT_OOM;
+
 retry_pud:
-	if (pud_none(*vmf.pud) && __transparent_hugepage_enabled(vma)) {
-		ret = create_huge_pud(&vmf);
-		if (!(ret & VM_FAULT_FALLBACK))
-			return ret;
+	if (pud_none(*vmf.pud) && __transparent_hugepage_enabled(vma, address)) {
+		// (markm) No entry present.
+
+		// (markm) run the estimator to check if we should create a 1GB page.
+		mm_action.address = address;
+		mm_action.action = MM_ACTION_PROMOTE_HUGE;
+		mm_action.huge_page_order = HPAGE_PUD_SHIFT-PAGE_SHIFT;
+		mm_estimate_changes(&mm_action, &mm_cost_delta);
+		should_do = mm_decide(&mm_cost_delta);
+
+		if (should_do) {
+			ret = create_huge_pud(&vmf);
+			if (!(ret & VM_FAULT_FALLBACK)) {
+				mm_stats_set_flag(pftrace, MM_STATS_PF_VERY_HUGE_PAGE);
+				return ret;
+			}
+		}
 	} else {
+		// (markm) Entry is already present.
 		pud_t orig_pud = *vmf.pud;
 
 		barrier();
@@ -4077,8 +4540,11 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 
 			if (dirty && !pud_write(orig_pud)) {
 				ret = wp_huge_pud(&vmf, orig_pud);
-				if (!(ret & VM_FAULT_FALLBACK))
+				if (!(ret & VM_FAULT_FALLBACK)) {
+					mm_stats_set_flag(pftrace, MM_STATS_PF_VERY_HUGE_PAGE);
+					mm_stats_set_flag(pftrace, MM_STATS_PF_WP);
 					return ret;
+				}
 			} else {
 				huge_pud_set_accessed(&vmf, orig_pud);
 				return 0;
@@ -4086,6 +4552,43 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 		}
 	}
 
+	vmf.pmd = pmd_offset(vmf.pud, address);
+
+        /*
+	 * Here we check for transparent huge page that are marked as reserved
+         */
+        if(mm && mm->badger_trap_was_enabled && !(vmf.flags & FAULT_FLAG_INSTRUCTION)
+		&& vmf.pmd && pmd_trans_huge(*vmf.pmd))
+        {
+                pmd_t orig_pmd = *vmf.pmd;
+
+                if ((vmf.flags & FAULT_FLAG_WRITE) && is_pmd_reserved(orig_pmd)
+			&& !pmd_write(orig_pmd) && pmd_present(orig_pmd))
+                {
+			mm_stats_set_flag(pftrace, MM_STATS_PF_HUGE_PAGE);
+			mm_stats_set_flag(pftrace, MM_STATS_PF_WP);
+			return do_huge_pmd_wp_page(&vmf, orig_pmd, pftrace);
+                }
+                if (is_pmd_reserved(orig_pmd) && pmd_present(orig_pmd))
+                {
+			mm_stats_set_flag(pftrace, MM_STATS_PF_HUGE_PAGE);
+			mm_stats_set_flag(pftrace, MM_STATS_PF_BADGER_TRAP);
+                        ret = transparent_fake_fault(&vmf);
+			return ret;
+                }
+
+		ptl = pmd_lock(mm, vmf.pmd);
+		// We use the "live" pmd here.
+                if (pmd_present(*vmf.pmd)
+			&& is_badger_trap_enabled(vmf.vma->vm_mm, vmf.address))
+		{
+			// *vmf.pmd = pmd_mkreserve(*vmf.pmd); // TODO markm uncomment
+                } else if (pmd_present(*vmf.pmd)) {
+			*vmf.pmd = pmd_unreserve(*vmf.pmd);
+		}
+		spin_unlock(ptl);
+	}
+
 	vmf.pmd = pmd_alloc(mm, vmf.pud, address);
 	if (!vmf.pmd)
 		return VM_FAULT_OOM;
@@ -4094,27 +4597,45 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 	if (pud_trans_unstable(vmf.pud))
 		goto retry_pud;
 
-	if (pmd_none(*vmf.pmd) && __transparent_hugepage_enabled(vma)) {
-		ret = create_huge_pmd(&vmf);
-		if (!(ret & VM_FAULT_FALLBACK))
-			return ret;
+	if (pmd_none(*vmf.pmd) && __transparent_hugepage_enabled(vma, address)) {
+		// (markm) No entry present.
+
+		// (markm) run the estimator to check if we should create a 2MB page.
+		mm_action.address = address;
+		mm_action.action = MM_ACTION_PROMOTE_HUGE;
+		mm_action.huge_page_order = HPAGE_PMD_ORDER;
+		mm_estimate_changes(&mm_action, &mm_cost_delta);
+		should_do = mm_decide(&mm_cost_delta);
+
+		if (should_do) {
+			ret = create_huge_pmd(&vmf, pftrace, mm_cost_delta.extra);
+			if (!(ret & VM_FAULT_FALLBACK))
+				return ret;
+		}
 	} else {
+		// (markm) Entry is already present.
+
 		pmd_t orig_pmd = *vmf.pmd;
 
 		barrier();
 		if (unlikely(is_swap_pmd(orig_pmd))) {
 			VM_BUG_ON(thp_migration_supported() &&
-					  !is_pmd_migration_entry(orig_pmd));
+					!is_pmd_migration_entry(orig_pmd));
 			if (is_pmd_migration_entry(orig_pmd))
 				pmd_migration_entry_wait(mm, vmf.pmd);
 			return 0;
 		}
 		if (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {
-			if (pmd_protnone(orig_pmd) && vma_is_accessible(vma))
+			if (pmd_protnone(orig_pmd) && vma_is_accessible(vma)) {
+				mm_stats_set_flag(pftrace, MM_STATS_PF_NUMA);
 				return do_huge_pmd_numa_page(&vmf, orig_pmd);
+			}
 
+			// TODO(markm): wp_huge_pmd/pud are for huge COW
+			// faults. Should add mm-econ logic here too.
 			if (dirty && !pmd_write(orig_pmd)) {
-				ret = wp_huge_pmd(&vmf, orig_pmd);
+				ret = wp_huge_pmd(&vmf, orig_pmd, pftrace);
+				mm_stats_set_flag(pftrace, MM_STATS_PF_WP);
 				if (!(ret & VM_FAULT_FALLBACK))
 					return ret;
 			} else {
@@ -4124,7 +4645,7 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 		}
 	}
 
-	return handle_pte_fault(&vmf);
+	return handle_pte_fault(&vmf, pftrace) | VM_FAULT_BASE_PAGE;
 }
 
 /*
@@ -4134,7 +4655,7 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
  * return value.  See filemap_fault() and __lock_page_or_retry().
  */
 vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
-		unsigned int flags)
+		unsigned int flags, struct mm_stats_pftrace *pftrace)
 {
 	vm_fault_t ret;
 
@@ -4158,10 +4679,20 @@ vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 	if (flags & FAULT_FLAG_USER)
 		mem_cgroup_enter_user_fault();
 
+	// If any other thread is in `badger_trap_walk`, we need to wait here.
+	if (vma->vm_mm) {
+		down_read(&vma->vm_mm->badger_trap_page_table_sem);
+	}
+
 	if (unlikely(is_vm_hugetlb_page(vma)))
+		// TODO(markm): maybe eventually instrument this with pftrace?
 		ret = hugetlb_fault(vma->vm_mm, vma, address, flags);
 	else
-		ret = __handle_mm_fault(vma, address, flags);
+		ret = __handle_mm_fault(vma, address, flags, pftrace);
+
+	if (vma->vm_mm) {
+		up_read(&vma->vm_mm->badger_trap_page_table_sem);
+	}
 
 	if (flags & FAULT_FLAG_USER) {
 		mem_cgroup_exit_user_fault();
@@ -4614,6 +5145,9 @@ static inline void process_huge_page(
 	unsigned long addr = addr_hint &
 		~(((unsigned long)pages_per_huge_page << PAGE_SHIFT) - 1);
 
+	u64 start = rdtsc();
+	u64 start_single;
+
 	/* Process target subpage last to keep its cache lines hot */
 	might_sleep();
 	n = (addr_hint - addr) / PAGE_SIZE;
@@ -4624,7 +5158,11 @@ static inline void process_huge_page(
 		/* Process subpages at the end of huge page */
 		for (i = pages_per_huge_page - 1; i >= 2 * n; i--) {
 			cond_resched();
+			start_single = rdtsc();
 			process_subpage(addr + i * PAGE_SIZE, i, arg);
+			mm_stats_hist_measure(
+				&mm_process_huge_page_single_page_cycles,
+				rdtsc() - start_single);
 		}
 	} else {
 		/* If target subpage in second half of huge page */
@@ -4633,7 +5171,11 @@ static inline void process_huge_page(
 		/* Process subpages at the begin of huge page */
 		for (i = 0; i < base; i++) {
 			cond_resched();
+			start_single = rdtsc();
 			process_subpage(addr + i * PAGE_SIZE, i, arg);
+			mm_stats_hist_measure(
+				&mm_process_huge_page_single_page_cycles,
+				rdtsc() - start_single);
 		}
 	}
 	/*
@@ -4645,10 +5187,21 @@ static inline void process_huge_page(
 		int right_idx = base + 2 * l - 1 - i;
 
 		cond_resched();
+		start_single = rdtsc();
 		process_subpage(addr + left_idx * PAGE_SIZE, left_idx, arg);
+		mm_stats_hist_measure(
+			&mm_process_huge_page_single_page_cycles,
+			rdtsc() - start_single);
+
 		cond_resched();
+		start_single = rdtsc();
 		process_subpage(addr + right_idx * PAGE_SIZE, right_idx, arg);
+		mm_stats_hist_measure(
+			&mm_process_huge_page_single_page_cycles,
+			rdtsc() - start_single);
 	}
+
+	mm_stats_hist_measure(&mm_process_huge_page_cycles, rdtsc() - start);
 }
 
 static void clear_gigantic_page(struct page *page,
@@ -4670,6 +5223,8 @@ static void clear_subpage(unsigned long addr, int idx, void *arg)
 {
 	struct page *page = arg;
 
+	if (PageZeroed(page + idx)) return;
+
 	clear_user_highpage(page + idx, addr);
 }
 
@@ -4678,6 +5233,7 @@ void clear_huge_page(struct page *page,
 {
 	unsigned long addr = addr_hint &
 		~(((unsigned long)pages_per_huge_page << PAGE_SHIFT) - 1);
+	u64 start = rdtsc();
 
 	if (unlikely(pages_per_huge_page > MAX_ORDER_NR_PAGES)) {
 		clear_gigantic_page(page, addr, pages_per_huge_page);
@@ -4685,6 +5241,8 @@ void clear_huge_page(struct page *page,
 	}
 
 	process_huge_page(addr_hint, pages_per_huge_page, clear_subpage, page);
+
+	mm_stats_hist_measure(&mm_huge_page_fault_clear_cycles, rdtsc() - start);
 }
 
 static void copy_user_gigantic_page(struct page *dst, struct page *src,
@@ -4731,6 +5289,7 @@ void copy_user_huge_page(struct page *dst, struct page *src,
 		.src = src,
 		.vma = vma,
 	};
+	u64 start = rdtsc();
 
 	if (unlikely(pages_per_huge_page > MAX_ORDER_NR_PAGES)) {
 		copy_user_gigantic_page(dst, src, addr, vma,
@@ -4739,6 +5298,8 @@ void copy_user_huge_page(struct page *dst, struct page *src,
 	}
 
 	process_huge_page(addr_hint, pages_per_huge_page, copy_subpage, &arg);
+
+	mm_stats_hist_measure(&mm_huge_page_fault_cow_copy_huge_cycles, rdtsc() - start);
 }
 
 long copy_huge_page_from_user(struct page *dst_page,
diff --git a/mm/mempolicy.c b/mm/mempolicy.c
index 977c641f78cf..5c40989ea94b 100644
--- a/mm/mempolicy.c
+++ b/mm/mempolicy.c
@@ -1083,6 +1083,7 @@ int do_migrate_pages(struct mm_struct *mm, const nodemask_t *from,
 		return err;
 
 	down_read(&mm->mmap_sem);
+	down_read(&mm->badger_trap_page_table_sem);
 
 	/*
 	 * Find a 'source' bit set in 'tmp' whose corresponding 'dest'
@@ -1163,6 +1164,7 @@ int do_migrate_pages(struct mm_struct *mm, const nodemask_t *from,
 		if (err < 0)
 			break;
 	}
+	up_read(&mm->badger_trap_page_table_sem);
 	up_read(&mm->mmap_sem);
 	if (err < 0)
 		return err;
diff --git a/mm/migrate.c b/mm/migrate.c
index b3b5d3bf0aab..bd50117c8625 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -48,6 +48,7 @@
 #include <linux/page_owner.h>
 #include <linux/sched/mm.h>
 #include <linux/ptrace.h>
+#include <linux/badger_trap.h>
 
 #include <asm/tlbflush.h>
 
@@ -254,7 +255,7 @@ static bool remove_migration_pte(struct page *page, struct vm_area_struct *vma,
 		if (PageHuge(new)) {
 			pte = pte_mkhuge(pte);
 			pte = arch_make_huge_pte(pte, vma, new, 0);
-			set_huge_pte_at(vma->vm_mm, pvmw.address, pvmw.pte, pte);
+			set_huge_pte_at(vma->vm_mm, pvmw.address, pvmw.pte, pte); // TODO markm is anything needed here?
 			if (PageAnon(new))
 				hugepage_add_anon_rmap(new, vma, pvmw.address);
 			else
@@ -262,7 +263,7 @@ static bool remove_migration_pte(struct page *page, struct vm_area_struct *vma,
 		} else
 #endif
 		{
-			set_pte_at(vma->vm_mm, pvmw.address, pvmw.pte, pte);
+			set_pte_at(vma->vm_mm, pvmw.address, pvmw.pte, pte); // TODO markm is anything needed here?
 
 			if (PageAnon(new))
 				page_add_anon_rmap(new, vma, pvmw.address, false);
@@ -2029,6 +2030,7 @@ int migrate_misplaced_transhuge_page(struct mm_struct *mm,
 	struct page *new_page = NULL;
 	int page_lru = page_is_file_cache(page);
 	unsigned long start = address & HPAGE_PMD_MASK;
+	//bool is_old_reserved = is_pmd_reserved(*pmd);
 
 	new_page = alloc_pages_node(node,
 		(GFP_TRANSHUGE_LIGHT | __GFP_THISNODE),
@@ -2091,6 +2093,14 @@ int migrate_misplaced_transhuge_page(struct mm_struct *mm,
 	 * visible before the pagetable update.
 	 */
 	page_add_anon_rmap(new_page, vma, start, true);
+
+	/*
+	 * Make the page table entry as reserved for TLB miss tracking
+	 * if the PMD was marked as reserved.
+	 */
+	//if(is_old_reserved && mm && mm->badger_trap_was_enabled) // TODO markm uncomment
+	//	entry = pmd_mkreserve(entry);
+
 	/*
 	 * At this point the pmd is numa/protnone (i.e. non present) and the TLB
 	 * has already been flushed globally.  So no TLB can be currently
@@ -2134,7 +2144,7 @@ int migrate_misplaced_transhuge_page(struct mm_struct *mm,
 	ptl = pmd_lock(mm, pmd);
 	if (pmd_same(*pmd, entry)) {
 		entry = pmd_modify(entry, vma->vm_page_prot);
-		set_pmd_at(mm, start, pmd, entry);
+		set_pmd_at(mm, start, pmd, entry); // TODO markm i don't think anything is needed here
 		update_mmu_cache_pmd(vma, address, &entry);
 	}
 	spin_unlock(ptl);
@@ -2317,7 +2327,7 @@ static int migrate_vma_collect_pmd(pmd_t *pmdp,
 			swp_pte = swp_entry_to_pte(entry);
 			if (pte_soft_dirty(pte))
 				swp_pte = pte_swp_mksoft_dirty(swp_pte);
-			set_pte_at(mm, addr, ptep, swp_pte);
+			set_pte_at(mm, addr, ptep, swp_pte); // TODO markm do we need something here?
 
 			/*
 			 * This is like regular unmap: we remove the rmap and
@@ -2811,11 +2821,11 @@ static void migrate_vma_insert_page(struct migrate_vma *migrate,
 	if (flush) {
 		flush_cache_page(vma, addr, pte_pfn(*ptep));
 		ptep_clear_flush_notify(vma, addr, ptep);
-		set_pte_at_notify(mm, addr, ptep, entry);
+		set_pte_at_notify(mm, addr, ptep, entry); // TODO markm do we need something here?
 		update_mmu_cache(vma, addr, ptep);
 	} else {
 		/* No need to invalidate - it was non-present before */
-		set_pte_at(mm, addr, ptep, entry);
+		set_pte_at(mm, addr, ptep, entry); // TODO markm do we need something here?
 		update_mmu_cache(vma, addr, ptep);
 	}
 
diff --git a/mm/mm_stats.c b/mm/mm_stats.c
new file mode 100644
index 000000000000..2c3a61f19326
--- /dev/null
+++ b/mm/mm_stats.c
@@ -0,0 +1,700 @@
+/*
+ * Stats about the memory management subsystem.
+ */
+
+#include <linux/mm_stats.h>
+#include <linux/uaccess.h>
+#include <linux/vmalloc.h>
+#include <linux/slab.h>
+#include <linux/fs.h>
+#include <linux/hashtable.h>
+#include <linux/cred.h>
+
+#define MM_STATS_INSTR_BUFSIZE 24
+
+struct mm_hist;
+
+static int hist_sprintf(struct file *file, char __user *ubuf,
+        size_t count, loff_t *ppos, struct mm_hist *hist);
+
+#define MM_STATS_PROC_CREATE_INT_INNER(type, name, default_val, fmt, extra_code) \
+    type name = default_val; \
+    static struct proc_dir_entry *name##_ent; \
+    \
+    static ssize_t name##_read_cb( \
+            struct file *file, char __user *ubuf,size_t count, loff_t *ppos) \
+    { \
+        char buf[MM_STATS_INSTR_BUFSIZE]; \
+        int len=0; \
+ \
+        if(*ppos > 0) \
+            return 0; \
+ \
+        len += sprintf(buf, fmt "\n", name); \
+ \
+        if(count < len) \
+            return 0; \
+ \
+        if(copy_to_user(ubuf, buf, len)) \
+            return -EFAULT; \
+ \
+        *ppos = len; \
+        return len; \
+    } \
+    \
+    static ssize_t name##_write_cb( \
+            struct file *file, const char __user *ubuf, size_t len, loff_t *offset) \
+    { \
+        int num; \
+        type val; \
+        char input[MM_STATS_INSTR_BUFSIZE]; \
+ \
+        if(*offset > 0 || len > MM_STATS_INSTR_BUFSIZE) { \
+            return -EFAULT; \
+        } \
+ \
+        if(copy_from_user(input, ubuf, len)) { \
+            return -EFAULT; \
+        } \
+ \
+        num = sscanf(input, fmt, &val); \
+        if(num != 1) { \
+            return -EINVAL; \
+        } \
+ \
+        name = val; \
+        extra_code \
+ \
+        printk(KERN_WARNING "mm-econ: %s = " fmt "\n", #name, name); \
+ \
+        return len; \
+    } \
+    \
+    static struct file_operations name##_ops = \
+    { \
+        .write = name##_write_cb, \
+        .read = name##_read_cb, \
+    };
+
+#define MM_STATS_PROC_CREATE_INT(type, name, default_val, fmt) \
+    MM_STATS_PROC_CREATE_INT_INNER(type, name, default_val, fmt, { /* nothing */ })
+
+#define MM_STATS_INIT_INT(name) \
+    name##_ent = proc_create(#name, 0444, NULL, &name##_ops);
+
+#define MM_STATS_PROC_CREATE_HIST_OUTPUT(name) \
+    static struct proc_dir_entry *name##_ent; \
+    \
+    static ssize_t name##_read_cb( \
+            struct file *file, char __user *ubuf,size_t count, loff_t *ppos) \
+    { \
+        return hist_sprintf(file, ubuf, count, ppos, &name); \
+    } \
+    \
+    static struct file_operations name##_ops = \
+    { \
+        .read = name##_read_cb, \
+    };
+
+#define MM_STATS_INIT_MM_HIST(name) \
+    reinit_mm_hist(&name, name##_nbins, name##_min, \
+            name##_width, name##_is_exp)
+
+#define MM_STATS_PROC_CREATE_HIST(name) \
+    unsigned int name##_nbins; \
+    u64 name##_min; \
+    u64 name##_width; \
+    int name##_is_exp; \
+    struct mm_hist name; \
+    MM_STATS_PROC_CREATE_INT_INNER(unsigned int, name##_nbins, 20, "%u", \
+        { MM_STATS_INIT_MM_HIST(name); }); \
+    MM_STATS_PROC_CREATE_INT_INNER(u64, name##_min, 0, "%llu", \
+        { MM_STATS_INIT_MM_HIST(name); }); \
+    MM_STATS_PROC_CREATE_INT_INNER(u64, name##_width, 1000, "%llu", \
+        { MM_STATS_INIT_MM_HIST(name); }); \
+    MM_STATS_PROC_CREATE_INT_INNER(int, name##_is_exp, 1, "%d", \
+        { MM_STATS_INIT_MM_HIST(name); }); \
+    MM_STATS_PROC_CREATE_HIST_OUTPUT(name);
+
+#define MM_STATS_INIT_HIST(name) \
+    MM_STATS_INIT_INT(name##_nbins); \
+    MM_STATS_INIT_INT(name##_min); \
+    MM_STATS_INIT_INT(name##_width); \
+    MM_STATS_INIT_INT(name##_is_exp); \
+    name##_ent = proc_create(#name, 0444, NULL, &name##_ops); \
+    name.bins = NULL; \
+    if (MM_STATS_INIT_MM_HIST(name)) { \
+        pr_warn("mm-econ: unable to init histogram " #name); \
+    }
+
+/*
+ * A histogram of u64 values. The minimum and number of bins are adjustable,
+ * and there are counters for the number of values that fall outside of the
+ * [min, max) range of the histogram. The histogram can also be set to be
+ * linear or exponential.
+ *
+ * If the bins are linear, then bin i contains the frequency of range
+ * [min + i*width, min + (i+1)*width).
+ *
+ * If the bins are exponential, the ith bin contains the frequency of range
+ * [min + (2^i)*width, min + (2^(i+1))*width).
+ */
+struct mm_hist {
+    // The number of bins.
+    unsigned int n;
+    // The smallest value.
+    u64 min;
+    // The width of a bin.
+    u64 width;
+    // Are the bin-widths exponentially increasing or linearly increasing?
+    bool is_exp;
+
+    // An array of u64 for the data.
+    u64 *bins;
+
+    // Frequency of values lower than min or higher than max.
+    u64 too_lo_count;
+    u64 too_hi_count;
+};
+
+static int reinit_mm_hist(struct mm_hist *hist, unsigned int n,
+        u64 min, u64 width, bool is_exp) {
+    pr_warn("mm-econ: reset mm_hist n=%u min=%llu width=%llu isexp=%d",
+            n, min, width, is_exp);
+
+    hist->n = n;
+    hist->min = min;
+    hist->width = width;
+    hist->is_exp = is_exp;
+    hist->too_hi_count = 0;
+    hist->too_lo_count = 0;
+
+    if (hist->bins) {
+        vfree(hist->bins);
+    }
+
+    if (hist-> n < 1) {
+        pr_warn("mm-econ: adjusting nbins to 1.");
+        hist->n = 1;
+    }
+
+    hist->bins = (u64 *)vzalloc(n * sizeof(u64));
+
+    if (hist->bins) {
+        return 0;
+    } else {
+        pr_warn("mm-econ: unable to allocate histogram");
+        return -ENOMEM;
+    }
+}
+
+// Returns the bit index of the most-significant bit in val (starting from 0).
+static inline int highest_bit(u64 val)
+{
+    int msb = 0;
+
+    if (val == 0) {
+        // There was no set bit. This is an error.
+        pr_warn("mm-econ: no MSB, but expected one.");
+        return -1;
+    }
+
+    if (val & 0xFFFFFFFF00000000) {
+        msb += 32;
+        val >>= 32;
+    }
+
+    if (val & 0xFFFF0000) {
+        msb += 16;
+        val >>= 16;
+    }
+
+    if (val & 0xFF00) {
+        msb += 8;
+        val >>= 8;
+    }
+
+    if (val & 0xF0) {
+        msb += 4;
+        val >>= 4;
+    }
+
+    if (val & 0xC) {
+        msb += 2;
+        val >>= 2;
+    }
+
+    if (val & 0x2) {
+        msb += 1;
+        val >>= 1;
+    }
+
+    return msb;
+}
+
+void mm_stats_hist_measure(struct mm_hist *hist, u64 val)
+{
+    unsigned int bin_idx;
+
+    // Some sanity checking.
+    BUG_ON(hist->n == 0);
+
+    if (!hist->bins) {
+        pr_warn("mm-econ: no bins allocated.");
+        return;
+    }
+
+    // Find out if it is too low or too high.
+    if (val < hist->min) {
+        hist->too_lo_count++;
+        return;
+    }
+    if (hist->is_exp) {
+        if (val >= (hist->min + ((1 << (hist->n - 1)) * hist->width))) {
+            hist->too_hi_count++;
+            return;
+        }
+    } else {
+        if (val >= (hist->min + (hist->n - 1) * hist->width)) {
+            hist->too_hi_count++;
+            return;
+        }
+    }
+
+    // If we get here, we know there is a bin for the data. Find out which one.
+    if (hist->is_exp) {
+        bin_idx = (val - hist->min) / hist->width;
+        if (bin_idx > 0) {
+            bin_idx = highest_bit(bin_idx) + 1;
+        }
+    } else {
+        bin_idx = (val - hist->min) / hist->width;
+    }
+
+    BUG_ON(bin_idx >= hist->n);
+    hist->bins[bin_idx]++;
+}
+
+/*
+ * Reads a histogram and creates the output string to be reported to the user
+ */
+static int hist_sprintf(struct file *file, char __user *ubuf,
+        size_t count, loff_t *ppos, struct mm_hist *hist)
+{
+        size_t buf_size = min(count, (size_t)((hist->n + 2) * 16 + 1));
+        char *buf;
+        int len=0;
+        int i;
+
+        if(*ppos > 0)
+            return 0;
+
+        buf = (char *)vmalloc(buf_size);
+        if (!buf) {
+            pr_warn("mm-econ: Unable to allocate results string buffer.");
+            return -ENOMEM;
+        }
+
+        len += sprintf(buf, "%llu %llu ", hist->too_lo_count, hist->too_hi_count);
+        for (i = 0; i < hist->n; ++i) {
+            len += sprintf(&buf[len], "%llu ", hist->bins[i]);
+        }
+        buf[len++] = '\0';
+
+        if(count < len) {
+            vfree(buf);
+            return 0;
+        }
+
+        if(copy_to_user(ubuf, buf, len)) {
+            vfree(buf);
+            return -EFAULT;
+        }
+
+        vfree(buf);
+
+        *ppos = len;
+        return len;
+}
+
+///////////////////////////////////////////////////////////////////////////////
+// Implement the pftrace stuff.
+
+// Convert flags to text names for the sake of debugging/printing.
+char *mm_stats_pf_flags_names[MM_STATS_NUM_FLAGS] = {
+	[MM_STATS_PF_HUGE_PAGE] = "MM_STATS_PF_HUGE_PAGE",
+	[MM_STATS_PF_VERY_HUGE_PAGE] = "MM_STATS_PF_VERY_HUGE_PAGE",
+	[MM_STATS_PF_BADGER_TRAP] = "MM_STATS_PF_BADGER_TRAP",
+        [MM_STATS_PF_WP] = "MM_STATS_PF_WP",
+	[MM_STATS_PF_EXEC] = "MM_STATS_PF_EXEC",
+	[MM_STATS_PF_NUMA] = "MM_STATS_PF_NUMA",
+	[MM_STATS_PF_SWAP] = "MM_STATS_PF_SWAP",
+        [MM_STATS_PF_NOT_ANON] = "MM_STATS_PF_NOT_ANON",
+	[MM_STATS_PF_NOT_ANON_READ] = "MM_STATS_PF_NOT_ANON_READ",
+	[MM_STATS_PF_NOT_ANON_COW] = "MM_STATS_PF_NOT_ANON_COW",
+	[MM_STATS_PF_NOT_ANON_SHARED] = "MM_STATS_PF_NOT_ANON_SHARED",
+        [MM_STATS_PF_ZERO] = "MM_STATS_PF_ZERO",
+	[MM_STATS_PF_HUGE_ALLOC_FAILED] = "MM_STATS_PF_HUGE_ALLOC_FAILED",
+        [MM_STATS_PF_HUGE_SPLIT] = "MM_STATS_PF_HUGE_SPLIT",
+        [MM_STATS_PF_HUGE_PROMOTION] = "MM_STATS_PF_HUGE_PROMOTION",
+	[MM_STATS_PF_HUGE_PROMOTION_FAILED] = "MM_STATS_PF_HUGE_PROMOTION_FAILED",
+	[MM_STATS_PF_HUGE_COPY] = "MM_STATS_PF_HUGE_COPY",
+	[MM_STATS_PF_CLEARED_MEM] = "MM_STATS_PF_CLEARED_MEM",
+	[MM_STATS_PF_ALLOC_FALLBACK] = "MM_STATS_PF_ALLOC_FALLBACK",
+        [MM_STATS_PF_ALLOC_PREZEROED] = "MM_STATS_PF_ALLOC_PREZEROED",
+        [MM_STATS_PF_ALLOC_NODE_RECLAIM] = "MM_STATS_PF_ALLOC_NODE_RECLAIM",
+};
+
+// This is the pftrace file, found at "/pftrace". We also keep track of the
+// file offset for writes and the number of writes so far so we can batch
+// fsyncing.
+#define MM_STATS_PFTRACE_FNAME "/pftrace"
+static struct file *pftrace_file = NULL;
+static loff_t pftrace_pos = 0;
+static long pftrace_nwrites = 0;
+
+// Create /proc/pftrace_enable which enables/disables pftrace.
+// 0: off
+// !0: on
+//
+// At the same time, it attempts to open the pftrace file when written to.
+static inline int open_pftrace_file(bool);
+MM_STATS_PROC_CREATE_INT_INNER(int, pftrace_enable, 0, "%d", {
+    long err;
+
+    if (pftrace_enable) {
+        err = open_pftrace_file(true);
+        if (err) return err;
+    } else if (pftrace_file) {
+        filp_close(pftrace_file, NULL);
+        file = NULL;
+    }
+})
+// Create /proc/pftrace_threshold - the rejection threshold for samples (in cycles).
+#define MM_STATS_PFTRACE_DEFAULT_THRESHOLD (100 * 1000)
+MM_STATS_PROC_CREATE_INT(u64, pftrace_threshold,
+        MM_STATS_PFTRACE_DEFAULT_THRESHOLD, "%llu")
+// Keeps count of samples dropped because they occurred in an interrupt context.
+MM_STATS_PROC_CREATE_INT(u64, pftrace_discarded_from_interrupt, 0, "%llu")
+// Keeps count of samples dropped due to FS errors.
+MM_STATS_PROC_CREATE_INT(u64, pftrace_discarded_from_error, 0, "%llu")
+
+// Keep counts of the number of rejected sample for different sets of bitflags.
+// This helps us figure out what part of the tail our samples are.
+#define REJECTED_HASH_BITS 5
+static DEFINE_HASHTABLE(pftrace_rejected_samples, REJECTED_HASH_BITS);
+
+struct rejected_hash_node {
+    struct hlist_node node;
+    mm_stats_bitflags_t bitflags;
+    u64 count;
+};
+
+// Output the rejected sample count to a procfs file.
+static struct proc_dir_entry *rejected_hash_ent;
+static ssize_t rejected_hash_read_cb(
+        struct file *file, char __user *ubuf,size_t count, loff_t *ppos)
+{
+    ssize_t len = 0;
+    int bkt;
+    struct rejected_hash_node *node;
+    char *buf;
+
+    if(*ppos > 0)
+        return 0;
+
+    buf = (char *)vmalloc(count);
+    if (!buf) {
+        pr_warn("mm_stats: Unable to allocate rejected string buffer.");
+        return -ENOMEM;
+    }
+
+    hash_for_each(pftrace_rejected_samples, bkt, node, node)
+    {
+        len += sprintf(&buf[len], "%llx:%llu ",
+                node->bitflags, node->count);
+    }
+    buf[len++] = '\0';
+
+    if(count < len) {
+        vfree(buf);
+        return 0;
+    }
+
+    if(copy_to_user(ubuf, buf, len)) {
+        vfree(buf);
+        return -EFAULT;
+    }
+
+    vfree(buf);
+
+    *ppos = len;
+    return len;
+}
+
+static struct file_operations rejected_hash_ops =
+{
+    .read = rejected_hash_read_cb,
+};
+
+
+static inline void rejected_sample(struct mm_stats_pftrace *trace)
+{
+    struct rejected_hash_node *node = NULL;
+    bool found = false;
+
+    // Look for existing entry.
+    hash_for_each_possible(pftrace_rejected_samples, node, node, trace->bitflags)
+    {
+        if (trace->bitflags == node->bitflags) {
+            found = true;
+            break;
+        }
+    }
+
+    // If not found, insert.
+    if (!found) {
+        node = (struct rejected_hash_node *)kmalloc(
+                sizeof(struct rejected_hash_node),
+                // We pass __GFP_ATOMIC because this may occur in an interrupt
+                // context, which doesn't allow sleeping.
+                GFP_KERNEL | __GFP_ZERO | __GFP_ATOMIC);
+        if (!node) {
+            pr_err("mm_stats: unable to alloc rejected_hash_node. skipping.");
+            return;
+        }
+
+        node->bitflags = trace->bitflags;
+        node->count = 0;
+
+        hash_add(pftrace_rejected_samples, &node->node, node->bitflags);
+    }
+
+    // Increase the count;
+    node->count += 1;
+}
+
+static inline int open_pftrace_file(bool reopen) {
+    struct file *file;
+    long err;
+
+    // Don't open multiple times.
+    if (likely(pftrace_file != NULL && !reopen)) return 0;
+
+    // If we need to reopen the file, try closing first.
+    if (pftrace_file && reopen) {
+        filp_close(pftrace_file, NULL);
+        file = NULL;
+    }
+
+    // Open the file.
+    file = filp_open(MM_STATS_PFTRACE_FNAME,
+            O_WRONLY | O_CREAT | O_TRUNC, 0444);
+    if (IS_ERR(file)) {
+        err = PTR_ERR(file);
+        pr_err("mm_stats: Failed to open pftrace file. "
+               "errno=%ld current->uid=%u\n",
+               err, __kuid_val(current_cred()->uid));
+        return err;
+    }
+
+    // Successfully opened the file!
+
+    pftrace_file = file;
+
+    pr_warn("mm_stats: Successfully opened %s current->uid=%u\n",
+            MM_STATS_PFTRACE_FNAME, __kuid_val(current_cred()->uid));
+    return 0;
+}
+
+void mm_stats_pftrace_init(struct mm_stats_pftrace *trace)
+{
+    memset(trace, 0, sizeof(struct mm_stats_pftrace));
+}
+
+void mm_stats_pftrace_submit(struct mm_stats_pftrace *trace, struct pt_regs *regs)
+{
+    long err, i;
+    ssize_t total_written = 0, written;
+
+    // Check if pftrace is on.
+    if (!pftrace_enable) return;
+
+    // Filter out some events.
+    // TODO: more complex filter...
+    if (trace->end_tsc - trace->start_tsc < pftrace_threshold) {
+        rejected_sample(trace);
+        return;
+    }
+
+    // If we are in an interrupt context, we can't write to a file.
+    // TODO: if it turns out to be important we change this to buffer the trace
+    //       for latter...
+    if (in_interrupt()) {
+        pftrace_discarded_from_interrupt += 1;
+        return;
+    }
+
+    // HACK: for some reason if we shuffle page lists and turn on pftrace with
+    // a threshold of 10000, sshd gets an XFSZ (signal 25).
+    if (strncmp(current->comm, "sshd", 4) == 0) {
+        pr_err("mm_stats: discarding sample from sshd. pos=%lld\n",
+                pftrace_pos);
+        pftrace_discarded_from_error += 1;
+
+        pr_warn("mm_stats: total=%10llu bits=%llx",
+                trace->end_tsc - trace->start_tsc,
+                trace->bitflags);
+
+        for (i = 0; i < MM_STATS_NUM_FLAGS; ++i) {
+            if (mm_stats_test_flag(trace, i)) {
+                pr_cont(" %s", mm_stats_pf_flags_names[i]);
+            }
+        }
+
+        pr_warn("process=%d (%s) ip=%lx\n",
+                current->pid, current->comm, regs->ip);
+
+        return;
+    }
+
+    // Make sure the trace file is open.
+    err = open_pftrace_file(false);
+    if (err) {
+        pr_err_once("mm_stats: pftrace file not open. "
+                    "Dropping unrejected samples!");
+        pftrace_discarded_from_error += 1;
+        return;
+    }
+
+    /* for debugging...
+    pr_warn("mm_stats: total=%10llu bits=%llx",
+            trace->end_tsc - trace->start_tsc,
+            trace->bitflags);
+
+    for (i = 0; i < MM_STATS_NUM_FLAGS; ++i) {
+        if (mm_stats_test_flag(trace, i)) {
+            pr_cont(" %s", mm_stats_pf_flags_names[i]);
+        }
+    }
+    */
+
+    // Write the trace directly to the end of the file.
+    while (total_written < sizeof(struct mm_stats_pftrace)) {
+        written = kernel_write(pftrace_file, trace,
+                sizeof(struct mm_stats_pftrace), &pftrace_pos);
+        if (written < 0) {
+            pr_err("mm_stats: error writing pftrace: w=%ld pos=%lld\n",
+                    written, pftrace_pos);
+            pftrace_discarded_from_error += 1;
+
+            pr_warn("mm_stats: total=%10llu bits=%llx",
+                    trace->end_tsc - trace->start_tsc,
+                    trace->bitflags);
+
+            for (i = 0; i < MM_STATS_NUM_FLAGS; ++i) {
+                if (mm_stats_test_flag(trace, i)) {
+                    pr_cont(" %s", mm_stats_pf_flags_names[i]);
+                }
+            }
+
+            pr_warn("process=%d (%s) ip=%lx\n",
+                    current->pid, current->comm, regs->ip);
+
+            return;
+        }
+
+        total_written += written;
+    }
+
+    // Every 1000 writes, flush.
+    if (++pftrace_nwrites % 1000 == 0) {
+        vfs_fsync(pftrace_file, 0);
+    }
+}
+
+///////////////////////////////////////////////////////////////////////////////
+// Define various stats below.
+
+// Histograms of page fault latency (base page and huge page).
+MM_STATS_PROC_CREATE_HIST(mm_base_page_fault_cycles);
+MM_STATS_PROC_CREATE_HIST(mm_huge_page_fault_cycles);
+// Cycles to allocate a new huge page in the pf handler (pages are never
+// promoted in pf handler). This is a subset of the previous histogram. This
+// only includes successful operations. Huge zero-page is not included.
+MM_STATS_PROC_CREATE_HIST(mm_huge_page_fault_create_new_cycles);
+// Time to clear a new huge page. This is a subset of the previous histogram.
+MM_STATS_PROC_CREATE_HIST(mm_huge_page_fault_clear_cycles);
+// Create a new huge zero page.
+MM_STATS_PROC_CREATE_HIST(mm_huge_page_fault_zero_page_cycles);
+// Time for a page fault touching a write-protected anon huge page. This
+// usually means a COW. There are a few things that can happen here:
+// - If nobody else is using the page, we can just make it writable.
+// - If there is no backing page, we need to allocate a huge page. Then, we
+//   need to clear it.
+// - Otherwise, we try to make a private copy of the huge page (COW), which
+//   involves copying the entire old huge page.
+// - Otherwise, we fall back to splitting the page into base pages.
+//
+// For now we don't split out all of these possibilities, but it's not hard to
+// do so (just a bit of work).
+MM_STATS_PROC_CREATE_HIST(mm_huge_page_fault_wp_cycles);
+// Time to copy the huge page during a COW clone. This is a subset of the
+// previous histogram.
+MM_STATS_PROC_CREATE_HIST(mm_huge_page_fault_cow_copy_huge_cycles);
+
+// Histograms of compaction events.
+MM_STATS_PROC_CREATE_HIST(mm_direct_compaction_cycles);
+MM_STATS_PROC_CREATE_HIST(mm_indirect_compaction_cycles);
+
+// Histograms of reclamation events.
+MM_STATS_PROC_CREATE_HIST(mm_direct_reclamation_cycles);
+
+// Histograms of huge page promotion/demotion events.
+// Cycles spent by khugepaged to find and promote.
+MM_STATS_PROC_CREATE_HIST(mm_huge_page_promotion_scanning_cycles);
+// Subset of the above, specifically the work to promote once we have found a
+// page. Only includes successful operations.
+MM_STATS_PROC_CREATE_HIST(mm_huge_page_promotion_work_cycles);
+// Subset of the above, specifically the copying of contents to the new huge
+// page. Only includes successful operations.
+MM_STATS_PROC_CREATE_HIST(mm_huge_page_promotion_copy_pages_cycles);
+
+// Histograms of process_huge_page, which is used for copying or clearing whole
+// huge pages.
+MM_STATS_PROC_CREATE_HIST(mm_process_huge_page_cycles);
+MM_STATS_PROC_CREATE_HIST(mm_process_huge_page_single_page_cycles);
+
+// Histograms of estimated costs and benefits for mm_econ.
+MM_STATS_PROC_CREATE_HIST(mm_econ_cost);
+MM_STATS_PROC_CREATE_HIST(mm_econ_benefit);
+
+void mm_stats_init(void)
+{
+    MM_STATS_INIT_INT(pftrace_enable);
+    MM_STATS_INIT_INT(pftrace_threshold);
+    MM_STATS_INIT_INT(pftrace_discarded_from_interrupt);
+    MM_STATS_INIT_INT(pftrace_discarded_from_error);
+    hash_init(pftrace_rejected_samples);
+    rejected_hash_ent = proc_create("pftrace_rejected",
+            0444, NULL, &rejected_hash_ops);
+
+    MM_STATS_INIT_HIST(mm_base_page_fault_cycles);
+    MM_STATS_INIT_HIST(mm_huge_page_fault_cycles);
+    MM_STATS_INIT_HIST(mm_huge_page_fault_create_new_cycles);
+    MM_STATS_INIT_HIST(mm_huge_page_fault_clear_cycles);
+    MM_STATS_INIT_HIST(mm_huge_page_fault_zero_page_cycles);
+    MM_STATS_INIT_HIST(mm_huge_page_fault_wp_cycles);
+    MM_STATS_INIT_HIST(mm_huge_page_fault_cow_copy_huge_cycles);
+    MM_STATS_INIT_HIST(mm_direct_compaction_cycles);
+    MM_STATS_INIT_HIST(mm_indirect_compaction_cycles);
+    MM_STATS_INIT_HIST(mm_direct_reclamation_cycles);
+    MM_STATS_INIT_HIST(mm_huge_page_promotion_scanning_cycles);
+    MM_STATS_INIT_HIST(mm_huge_page_promotion_work_cycles);
+    MM_STATS_INIT_HIST(mm_huge_page_promotion_copy_pages_cycles);
+    MM_STATS_INIT_HIST(mm_process_huge_page_cycles);
+    MM_STATS_INIT_HIST(mm_process_huge_page_single_page_cycles);
+
+    MM_STATS_INIT_HIST(mm_econ_cost);
+    MM_STATS_INIT_HIST(mm_econ_benefit);
+}
diff --git a/mm/mmap.c b/mm/mmap.c
index cb2c79a3e914..a32ae4327f7a 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -47,6 +47,7 @@
 #include <linux/pkeys.h>
 #include <linux/oom.h>
 #include <linux/sched/mm.h>
+#include <linux/mm_econ.h>
 
 #include <linux/uaccess.h>
 #include <asm/cacheflush.h>
@@ -226,8 +227,14 @@ SYSCALL_DEFINE1(brk, unsigned long, brk)
 			      mm->end_data, mm->start_data))
 		goto out;
 
-	newbrk = PAGE_ALIGN(brk);
-	oldbrk = PAGE_ALIGN(mm->brk);
+	if (mm_process_is_using_cbmm(current->tgid)) {
+		newbrk = ALIGN(brk, HPAGE_SIZE);
+		oldbrk = ALIGN(mm->brk, HPAGE_SIZE);
+	} else {
+		newbrk = PAGE_ALIGN(brk);
+		oldbrk = PAGE_ALIGN(mm->brk);
+	}
+
 	if (oldbrk == newbrk) {
 		mm->brk = brk;
 		goto success;
@@ -275,6 +282,29 @@ SYSCALL_DEFINE1(brk, unsigned long, brk)
 	userfaultfd_unmap_complete(mm, &uf);
 	if (populate)
 		mm_populate(oldbrk, newbrk - oldbrk);
+	else if (newbrk > oldbrk && mm_econ_is_on() && mm_process_is_using_cbmm(current->tgid)) {
+		struct mm_cost_delta mm_cost_delta;
+		struct mm_action mm_action;
+		struct range *ranges = NULL;
+		int i = 0;
+		bool should_do;
+
+		mm_action.address = oldbrk;
+		mm_action.len = newbrk - oldbrk;
+		mm_action.action = MM_ACTION_EAGER_PAGING;
+		mm_estimate_changes(&mm_action, &mm_cost_delta);
+		should_do = mm_decide(&mm_cost_delta);
+
+		if (should_do) {
+			ranges = (struct range*)mm_cost_delta.extra;
+			while(ranges[i].start != -1 && ranges[i].end != -1) {
+				mm_populate(ranges[i].start, ranges[i].end - ranges[i].start);
+				i++;
+			}
+		}
+
+		if (ranges) vfree(ranges);
+	}
 	return brk;
 
 out:
@@ -1363,6 +1393,8 @@ static inline bool file_mmap_ok(struct file *file, struct inode *inode,
 
 /*
  * The caller must hold down_write(&current->mm->mmap_sem).
+ *
+ * This function acquires and releases down_read(&current->mm->badger_trap_page_table_sem).
  */
 unsigned long do_mmap(struct file *file, unsigned long addr,
 			unsigned long len, unsigned long prot,
@@ -1372,11 +1404,16 @@ unsigned long do_mmap(struct file *file, unsigned long addr,
 {
 	struct mm_struct *mm = current->mm;
 	int pkey = 0;
+	unsigned long ret = 0;
+
+	//down_read(&mm->badger_trap_page_table_sem);
 
 	*populate = 0;
 
-	if (!len)
-		return -EINVAL;
+	if (!len) {
+		ret = -EINVAL;
+		goto out;
+	}
 
 	/*
 	 * Does the application expect PROT_READ to imply PROT_EXEC?
@@ -1397,29 +1434,39 @@ unsigned long do_mmap(struct file *file, unsigned long addr,
 
 	/* Careful about overflows.. */
 	len = PAGE_ALIGN(len);
-	if (!len)
-		return -ENOMEM;
+	if (!len) {
+		ret = -ENOMEM;
+		goto out;
+	}
 
 	/* offset overflow? */
-	if ((pgoff + (len >> PAGE_SHIFT)) < pgoff)
-		return -EOVERFLOW;
+	if ((pgoff + (len >> PAGE_SHIFT)) < pgoff) {
+		ret = -EOVERFLOW;
+		goto out;
+	}
 
 	/* Too many mappings? */
-	if (mm->map_count > sysctl_max_map_count)
-		return -ENOMEM;
+	if (mm->map_count > sysctl_max_map_count) {
+		ret = -ENOMEM;
+		goto out;
+	}
 
 	/* Obtain the address to map to. we verify (or select) it and ensure
 	 * that it represents a valid section of the address space.
 	 */
 	addr = get_unmapped_area(file, addr, len, pgoff, flags);
-	if (IS_ERR_VALUE(addr))
-		return addr;
+	if (IS_ERR_VALUE(addr)) {
+		ret = addr;
+		goto out;
+	}
 
 	if (flags & MAP_FIXED_NOREPLACE) {
 		struct vm_area_struct *vma = find_vma(mm, addr);
 
-		if (vma && vma->vm_start < addr + len)
-			return -EEXIST;
+		if (vma && vma->vm_start < addr + len) {
+			ret = -EEXIST;
+			goto out;
+		}
 	}
 
 	if (prot == PROT_EXEC) {
@@ -1436,18 +1483,24 @@ unsigned long do_mmap(struct file *file, unsigned long addr,
 			mm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;
 
 	if (flags & MAP_LOCKED)
-		if (!can_do_mlock())
-			return -EPERM;
+		if (!can_do_mlock()) {
+			ret = -EPERM;
+			goto out;
+		}
 
-	if (mlock_future_check(mm, vm_flags, len))
-		return -EAGAIN;
+	if (mlock_future_check(mm, vm_flags, len)) {
+		ret = -EAGAIN;
+		goto out;
+	}
 
 	if (file) {
 		struct inode *inode = file_inode(file);
 		unsigned long flags_mask;
 
-		if (!file_mmap_ok(file, inode, pgoff, len))
-			return -EOVERFLOW;
+		if (!file_mmap_ok(file, inode, pgoff, len)) {
+			ret = -EOVERFLOW;
+			goto out;
+		}
 
 		flags_mask = LEGACY_MAP_MASK | file->f_op->mmap_supported_flags;
 
@@ -1463,27 +1516,37 @@ unsigned long do_mmap(struct file *file, unsigned long addr,
 			flags &= LEGACY_MAP_MASK;
 			/* fall through */
 		case MAP_SHARED_VALIDATE:
-			if (flags & ~flags_mask)
-				return -EOPNOTSUPP;
+			if (flags & ~flags_mask) {
+				ret = -EOPNOTSUPP;
+				goto out;
+			}
 			if (prot & PROT_WRITE) {
-				if (!(file->f_mode & FMODE_WRITE))
-					return -EACCES;
-				if (IS_SWAPFILE(file->f_mapping->host))
-					return -ETXTBSY;
+				if (!(file->f_mode & FMODE_WRITE)) {
+					ret = -EACCES;
+					goto out;
+				}
+				if (IS_SWAPFILE(file->f_mapping->host)) {
+					ret = -ETXTBSY;
+					goto out;
+				}
 			}
 
 			/*
 			 * Make sure we don't allow writing to an append-only
 			 * file..
 			 */
-			if (IS_APPEND(inode) && (file->f_mode & FMODE_WRITE))
-				return -EACCES;
+			if (IS_APPEND(inode) && (file->f_mode & FMODE_WRITE)) {
+				ret = -EACCES;
+				goto out;
+			}
 
 			/*
 			 * Make sure there are no mandatory locks on the file.
 			 */
-			if (locks_verify_locked(file))
-				return -EAGAIN;
+			if (locks_verify_locked(file)) {
+				ret = -EAGAIN;
+				goto out;
+			}
 
 			vm_flags |= VM_SHARED | VM_MAYSHARE;
 			if (!(file->f_mode & FMODE_WRITE))
@@ -1491,28 +1554,39 @@ unsigned long do_mmap(struct file *file, unsigned long addr,
 
 			/* fall through */
 		case MAP_PRIVATE:
-			if (!(file->f_mode & FMODE_READ))
-				return -EACCES;
+			if (!(file->f_mode & FMODE_READ)) {
+				ret = -EACCES;
+				goto out;
+			}
 			if (path_noexec(&file->f_path)) {
-				if (vm_flags & VM_EXEC)
-					return -EPERM;
+				if (vm_flags & VM_EXEC) {
+					ret = -EPERM;
+					goto out;
+				}
 				vm_flags &= ~VM_MAYEXEC;
 			}
 
-			if (!file->f_op->mmap)
-				return -ENODEV;
-			if (vm_flags & (VM_GROWSDOWN|VM_GROWSUP))
-				return -EINVAL;
+			if (!file->f_op->mmap) {
+				ret = -ENODEV;
+				goto out;
+			}
+			if (vm_flags & (VM_GROWSDOWN|VM_GROWSUP)) {
+				ret = -EINVAL;
+				goto out;
+			}
 			break;
 
 		default:
-			return -EINVAL;
+			ret = -EINVAL;
+			goto out;
 		}
 	} else {
 		switch (flags & MAP_TYPE) {
 		case MAP_SHARED:
-			if (vm_flags & (VM_GROWSDOWN|VM_GROWSUP))
-				return -EINVAL;
+			if (vm_flags & (VM_GROWSDOWN|VM_GROWSUP)) {
+				ret = -EINVAL;
+				goto out;
+			}
 			/*
 			 * Ignore pgoff.
 			 */
@@ -1526,7 +1600,8 @@ unsigned long do_mmap(struct file *file, unsigned long addr,
 			pgoff = addr >> PAGE_SHIFT;
 			break;
 		default:
-			return -EINVAL;
+			ret = -EINVAL;
+			goto out;
 		}
 	}
 
@@ -1549,7 +1624,11 @@ unsigned long do_mmap(struct file *file, unsigned long addr,
 	    ((vm_flags & VM_LOCKED) ||
 	     (flags & (MAP_POPULATE | MAP_NONBLOCK)) == MAP_POPULATE))
 		*populate = len;
-	return addr;
+	ret = addr;
+
+out:
+	//up_read(&mm->badger_trap_page_table_sem);
+	return ret;
 }
 
 unsigned long ksys_mmap_pgoff(unsigned long addr, unsigned long len,
@@ -1595,6 +1674,39 @@ unsigned long ksys_mmap_pgoff(unsigned long addr, unsigned long len,
 	flags &= ~(MAP_EXECUTABLE | MAP_DENYWRITE);
 
 	retval = vm_mmap_pgoff(file, addr, len, prot, flags, pgoff);
+#ifdef CONFIG_MM_ECON
+	if (!IS_ERR((void*)retval)) {
+		struct mm_cost_delta mm_cost_delta;
+		struct mm_action mm_action;
+		struct range *ranges = NULL;
+		int i = 0;
+		bool should_do;
+		// Bijan: Potentially add this mmap to the tracked process's profile
+		u64 section_off = current->mm->mmap_base - retval;
+		mm_add_memory_range(current->tgid, SectionMmap, retval, section_off,
+				addr, len, prot, flags, fd, pgoff);
+
+		// Determine if we want to eagerly allocate parts of this mmap
+		if ( (flags & MAP_ANONYMOUS) && mm_econ_is_on() && mm_process_is_using_cbmm(current->tgid)) {
+			mm_action.address = retval;
+			mm_action.len = len;
+			mm_action.action = MM_ACTION_EAGER_PAGING;
+			mm_estimate_changes(&mm_action, &mm_cost_delta);
+			should_do = mm_decide(&mm_cost_delta);
+
+			if (should_do) {
+				ranges = (struct range*)mm_cost_delta.extra;
+				while(ranges[i].start != -1 && ranges[i].end != -1) {
+					mm_populate(ranges[i].start, ranges[i].end - ranges[i].start);
+					i++;
+				}
+			}
+
+			if (ranges) vfree(ranges);
+		}
+
+	}
+#endif
 out_fput:
 	if (file)
 		fput(file);
@@ -2975,6 +3087,7 @@ static int do_brk_flags(unsigned long addr, unsigned long len, unsigned long fla
 	pgoff_t pgoff = addr >> PAGE_SHIFT;
 	int error;
 	unsigned long mapped_addr;
+	u64 section_off;
 
 	/* Until we need other flags, refuse anything except VM_EXEC. */
 	if ((flags & (~VM_EXEC)) != 0)
@@ -3037,6 +3150,15 @@ static int do_brk_flags(unsigned long addr, unsigned long len, unsigned long fla
 	if (flags & VM_LOCKED)
 		mm->locked_vm += (len >> PAGE_SHIFT);
 	vma->vm_flags |= VM_SOFTDIRTY;
+
+#ifdef CONFIG_MM_ECON
+	// Bijan: If we expand the heap, add the new section to the tracked
+	// process's profile
+	section_off = addr - current->mm->start_brk;
+	mm_add_memory_range(current->tgid, SectionHeap, addr, section_off, 0, len,
+		0, 0, 0, 0);
+#endif
+
 	return 0;
 }
 
diff --git a/mm/mmzone.c b/mm/mmzone.c
index 4686fdc23bb9..a8d4b114fccb 100644
--- a/mm/mmzone.c
+++ b/mm/mmzone.c
@@ -14,6 +14,7 @@ struct pglist_data *first_online_pgdat(void)
 {
 	return NODE_DATA(first_online_node);
 }
+EXPORT_SYMBOL(first_online_pgdat);
 
 struct pglist_data *next_online_pgdat(struct pglist_data *pgdat)
 {
@@ -42,6 +43,7 @@ struct zone *next_zone(struct zone *zone)
 	}
 	return zone;
 }
+EXPORT_SYMBOL(next_zone);
 
 static inline int zref_in_nodemask(struct zoneref *zref, nodemask_t *nodes)
 {
diff --git a/mm/mprotect.c b/mm/mprotect.c
index 7a8e84f86831..ed2142d56753 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -481,6 +481,8 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
 	if (down_write_killable(&current->mm->mmap_sem))
 		return -EINTR;
 
+	//down_read(&current->mm->badger_trap_page_table_sem);
+
 	/*
 	 * If userspace did not allocate the pkey, do not let
 	 * them use it here.
@@ -568,6 +570,7 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
 		prot = reqprot;
 	}
 out:
+	//up_read(&current->mm->badger_trap_page_table_sem);
 	up_write(&current->mm->mmap_sem);
 	return error;
 }
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 627f1eba6df7..69ceaa6b93d3 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -68,6 +68,10 @@
 #include <linux/lockdep.h>
 #include <linux/nmi.h>
 #include <linux/psi.h>
+#include <linux/mm_stats.h>
+#include <linux/cpufreq.h>
+#include <linux/mm_econ.h>
+#include <linux/cpufreq.h>
 
 #include <asm/sections.h>
 #include <asm/tlbflush.h>
@@ -98,6 +102,19 @@ EXPORT_PER_CPU_SYMBOL(_numa_mem_);
 int _node_numa_mem_[MAX_NUMNODES];
 #endif
 
+// markm: a hacky way to test for alloc fast_path failure on the most recent
+// allocation. This is inherenetly a bit racy, but it's probably good enough
+// because the fast path is fast.
+DEFINE_PER_CPU(bool, pftrace_alloc_fallback);
+DEFINE_PER_CPU(bool, pftrace_alloc_fallback_retry);
+DEFINE_PER_CPU(bool, pftrace_alloc_fallback_reclaim);
+DEFINE_PER_CPU(bool, pftrace_alloc_fallback_compact);
+DEFINE_PER_CPU(bool, pftrace_alloc_node_reclaim);
+// markm: ditto but to check if we zeroed a page and how long it took.
+DEFINE_PER_CPU(bool, pftrace_alloc_zeroed_page);
+DEFINE_PER_CPU(u64, pftrace_alloc_zeroing_duration);
+DEFINE_PER_CPU(bool, pftrace_alloc_prezeroed);
+
 /* work_structs for global per-cpu drains */
 struct pcpu_drain {
 	struct zone *zone;
@@ -335,6 +352,58 @@ static unsigned long nr_kernel_pages __initdata;
 static unsigned long nr_all_pages __initdata;
 static unsigned long dma_reserve __initdata;
 
+#define LFPA_N 5
+static u64 last_few_prezeroed_allocs[LFPA_N] = { 0, 0, 0, 0, 0 };
+static atomic_t lfpa_head = ATOMIC_INIT(0);
+
+// Update the lfpa array by adding the given timestamp counter value at the
+// end atomically and incrementing the head index.
+inline void lfpa_update(u64 ts) {
+	while (true) {
+		int i = atomic_read(&lfpa_head);
+		int inew = (i == (LFPA_N - 1)) ? 0 : (i + 1);
+
+		int old = atomic_cmpxchg(&lfpa_head, i, inew);
+		if (old == i) {
+			last_few_prezeroed_allocs[inew] = ts;
+			return;
+		}
+	}
+}
+
+// Use the LFPA array to compute an estimate of number of prezeroed pages
+// allocated per LTU.
+u64 mm_estimated_prezeroed_used(void)
+{
+	u64 max = 0, min = 0, x, diff;
+	int i;
+	u64 freq;
+
+	// Non-atomically read all, and compute the difference between max and
+	// min, being careful of unused entries.
+	u64 last_few_prezeroed_allocs_copy[LFPA_N];
+	memcpy(last_few_prezeroed_allocs_copy,
+			last_few_prezeroed_allocs,
+			LFPA_N * sizeof(u64));
+
+	for (i = 0; i < LFPA_N; ++i) {
+		x = last_few_prezeroed_allocs_copy[i];
+		if (x != 0) {
+			if (x > max) max = x;
+			if (x < min || min == 0) min = x;
+		}
+	}
+
+	if (min == 0 || min == max) return 0;
+
+	// This is the time to allocate (LFPA_N - 1) pages.
+	diff = max - min;
+	freq = (u64) arch_freq_get_on_cpu(smp_processor_id()); // khz
+
+	// We can now compute allocs per LTU.
+	return (LFPA_N - 1) * freq * MM_ECON_LTU / diff;
+}
+
 #ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
 static unsigned long arch_zone_lowest_possible_pfn[MAX_NR_ZONES] __initdata;
 static unsigned long arch_zone_highest_possible_pfn[MAX_NR_ZONES] __initdata;
@@ -902,7 +971,7 @@ static inline void __free_one_page(struct page *page,
 	unsigned long combined_pfn;
 	unsigned long uninitialized_var(buddy_pfn);
 	struct page *buddy;
-	unsigned int max_order;
+	unsigned int max_order, i;
 	struct capture_control *capc = task_capc(zone);
 
 	max_order = min_t(unsigned int, MAX_ORDER, pageblock_order + 1);
@@ -918,6 +987,9 @@ static inline void __free_one_page(struct page *page,
 	VM_BUG_ON_PAGE(bad_range(zone, page), page);
 
 continue_merging:
+	for (i = 0; i < (1 << order); i++)
+		ClearPageZeroed(page + i);
+
 	while (order < max_order - 1) {
 		if (compaction_capture(capc, page, order, migratetype)) {
 			__mod_zone_freepage_state(zone, -(1 << order),
@@ -1113,9 +1185,18 @@ static int free_tail_pages_check(struct page *head_page, struct page *page)
 static void kernel_init_free_pages(struct page *page, int numpages)
 {
 	int i;
+	u64 start = rdtsc();
+	bool prezeroed = false;
 
 	for (i = 0; i < numpages; i++)
-		clear_highpage(page + i);
+		if (PageZeroed(page + i))
+			prezeroed = true;
+		else
+			clear_highpage(page + i);
+
+	get_cpu_var(pftrace_alloc_zeroed_page) = true;
+	get_cpu_var(pftrace_alloc_zeroing_duration) = rdtsc() - start;
+	get_cpu_var(pftrace_alloc_prezeroed) = prezeroed;
 }
 
 static __always_inline bool free_pages_prepare(struct page *page,
@@ -2025,6 +2106,10 @@ static inline void expand(struct zone *zone, struct page *page,
 		size >>= 1;
 		VM_BUG_ON_PAGE(bad_range(zone, &page[size]), &page[size]);
 
+		// propagate the zeroed flag to subdivisions.
+		if (PageZeroed(page))
+			SetPageZeroed(&page[size]);
+
 		/*
 		 * Mark as guard pages (or page), that will allow to
 		 * merge back to allocator when buddy will be freed.
@@ -2180,7 +2265,7 @@ static void prep_new_page(struct page *page, unsigned int order, gfp_t gfp_flags
  */
 static __always_inline
 struct page *__rmqueue_smallest(struct zone *zone, unsigned int order,
-						int migratetype)
+					int migratetype, bool front)
 {
 	unsigned int current_order;
 	struct free_area *area;
@@ -2189,7 +2274,7 @@ struct page *__rmqueue_smallest(struct zone *zone, unsigned int order,
 	/* Find a page of the appropriate size in the preferred list */
 	for (current_order = order; current_order < MAX_ORDER; ++current_order) {
 		area = &(zone->free_area[current_order]);
-		page = get_page_from_free_area(area, migratetype);
+		page = get_page_from_free_area(area, migratetype, front);
 		if (!page)
 			continue;
 		del_page_from_free_area(page, area);
@@ -2222,7 +2307,7 @@ static int fallbacks[MIGRATE_TYPES][4] = {
 static __always_inline struct page *__rmqueue_cma_fallback(struct zone *zone,
 					unsigned int order)
 {
-	return __rmqueue_smallest(zone, order, MIGRATE_CMA);
+	return __rmqueue_smallest(zone, order, MIGRATE_CMA, true);
 }
 #else
 static inline struct page *__rmqueue_cma_fallback(struct zone *zone,
@@ -2566,7 +2651,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,
 		for (order = 0; order < MAX_ORDER; order++) {
 			struct free_area *area = &(zone->free_area[order]);
 
-			page = get_page_from_free_area(area, MIGRATE_HIGHATOMIC);
+			page = get_page_from_free_area(area, MIGRATE_HIGHATOMIC, true);
 			if (!page)
 				continue;
 
@@ -2689,7 +2774,7 @@ __rmqueue_fallback(struct zone *zone, int order, int start_migratetype,
 	VM_BUG_ON(current_order == MAX_ORDER);
 
 do_steal:
-	page = get_page_from_free_area(area, fallback_mt);
+	page = get_page_from_free_area(area, fallback_mt, true);
 
 	steal_suitable_fallback(zone, page, alloc_flags, start_migratetype,
 								can_steal);
@@ -2707,12 +2792,12 @@ __rmqueue_fallback(struct zone *zone, int order, int start_migratetype,
  */
 static __always_inline struct page *
 __rmqueue(struct zone *zone, unsigned int order, int migratetype,
-						unsigned int alloc_flags)
+			unsigned int alloc_flags, bool front)
 {
 	struct page *page;
 
 retry:
-	page = __rmqueue_smallest(zone, order, migratetype);
+	page = __rmqueue_smallest(zone, order, migratetype, front);
 	if (unlikely(!page)) {
 		if (migratetype == MIGRATE_MOVABLE)
 			page = __rmqueue_cma_fallback(zone, order);
@@ -2733,14 +2818,15 @@ __rmqueue(struct zone *zone, unsigned int order, int migratetype,
  */
 static int rmqueue_bulk(struct zone *zone, unsigned int order,
 			unsigned long count, struct list_head *list,
-			int migratetype, unsigned int alloc_flags)
+			int migratetype, unsigned int alloc_flags,
+			bool front)
 {
 	int i, alloced = 0;
 
 	spin_lock(&zone->lock);
 	for (i = 0; i < count; ++i) {
 		struct page *page = __rmqueue(zone, order, migratetype,
-								alloc_flags);
+						alloc_flags, front);
 		if (unlikely(page == NULL))
 			break;
 
@@ -3040,6 +3126,7 @@ static void free_unref_page_commit(struct page *page, unsigned long pfn)
 		migratetype = MIGRATE_MOVABLE;
 	}
 
+	ClearPageZeroed(page);
 	pcp = &this_cpu_ptr(zone->pageset)->pcp;
 	list_add(&page->lru, &pcp->lists[migratetype]);
 	pcp->count++;
@@ -3204,7 +3291,7 @@ static inline void zone_statistics(struct zone *preferred_zone, struct zone *z)
 static struct page *__rmqueue_pcplist(struct zone *zone, int migratetype,
 			unsigned int alloc_flags,
 			struct per_cpu_pages *pcp,
-			struct list_head *list)
+			struct list_head *list, bool front)
 {
 	struct page *page;
 
@@ -3212,7 +3299,7 @@ static struct page *__rmqueue_pcplist(struct zone *zone, int migratetype,
 		if (list_empty(list)) {
 			pcp->count += rmqueue_bulk(zone, 0,
 					pcp->batch, list,
-					migratetype, alloc_flags);
+					migratetype, alloc_flags, front);
 			if (unlikely(list_empty(list)))
 				return NULL;
 		}
@@ -3228,7 +3315,8 @@ static struct page *__rmqueue_pcplist(struct zone *zone, int migratetype,
 /* Lock and remove page from the per-cpu list */
 static struct page *rmqueue_pcplist(struct zone *preferred_zone,
 			struct zone *zone, gfp_t gfp_flags,
-			int migratetype, unsigned int alloc_flags)
+			int migratetype, unsigned int alloc_flags,
+			bool front)
 {
 	struct per_cpu_pages *pcp;
 	struct list_head *list;
@@ -3238,7 +3326,7 @@ static struct page *rmqueue_pcplist(struct zone *preferred_zone,
 	local_irq_save(flags);
 	pcp = &this_cpu_ptr(zone->pageset)->pcp;
 	list = &pcp->lists[migratetype];
-	page = __rmqueue_pcplist(zone,  migratetype, alloc_flags, pcp, list);
+	page = __rmqueue_pcplist(zone,  migratetype, alloc_flags, pcp, list, front);
 	if (page) {
 		__count_zid_vm_events(PGALLOC, page_zonenum(page), 1);
 		zone_statistics(preferred_zone, zone);
@@ -3254,14 +3342,14 @@ static inline
 struct page *rmqueue(struct zone *preferred_zone,
 			struct zone *zone, unsigned int order,
 			gfp_t gfp_flags, unsigned int alloc_flags,
-			int migratetype)
+			int migratetype, bool front)
 {
 	unsigned long flags;
 	struct page *page;
 
 	if (likely(order == 0)) {
 		page = rmqueue_pcplist(preferred_zone, zone, gfp_flags,
-					migratetype, alloc_flags);
+					migratetype, alloc_flags, front);
 		goto out;
 	}
 
@@ -3275,12 +3363,12 @@ struct page *rmqueue(struct zone *preferred_zone,
 	do {
 		page = NULL;
 		if (alloc_flags & ALLOC_HARDER) {
-			page = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);
+			page = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC, true);
 			if (page)
 				trace_mm_page_alloc_zone_locked(page, order, migratetype);
 		}
 		if (!page)
-			page = __rmqueue(zone, order, migratetype, alloc_flags);
+			page = __rmqueue(zone, order, migratetype, alloc_flags, front);
 	} while (page && check_new_pages(page, order));
 	spin_unlock(&zone->lock);
 	if (!page)
@@ -3299,6 +3387,15 @@ struct page *rmqueue(struct zone *preferred_zone,
 		wakeup_kswapd(zone, 0, 0, zone_idx(zone));
 	}
 
+	if (order >= 9 && mm_econ_debugging_mode) {
+		pr_warn("alloc_pages: page %p node %d "
+			"zone %p (%s) order %d "
+			"migratetype %d front %d\n",
+			page, zone->zone_pgdat->node_id,
+			zone, zone->name, order,
+			migratetype, front);
+	}
+
 	VM_BUG_ON_PAGE(page && bad_range(zone, page), page);
 	return page;
 
@@ -3574,6 +3671,11 @@ get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,
 	struct pglist_data *last_pgdat_dirty_limit = NULL;
 	bool no_fallback;
 
+	// markm: if the user requested a zeroed page of order > 0, we try
+	// removing a page from the back of the freelist, as that is where the
+	// prezeroed pages are likely to be.
+	bool try_back = order >= HPAGE_PMD_ORDER;
+
 retry:
 	/*
 	 * Scan zonelist, looking for a zone with enough free.
@@ -3659,6 +3761,8 @@ get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,
 			    !zone_allows_reclaim(ac->preferred_zoneref->zone, zone))
 				continue;
 
+			get_cpu_var(pftrace_alloc_node_reclaim) = true;
+
 			ret = node_reclaim(zone->zone_pgdat, gfp_mask, order);
 			switch (ret) {
 			case NODE_RECLAIM_NOSCAN:
@@ -3679,7 +3783,7 @@ get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,
 
 try_this_zone:
 		page = rmqueue(ac->preferred_zoneref->zone, zone, order,
-				gfp_mask, alloc_flags, ac->migratetype);
+				gfp_mask, alloc_flags, ac->migratetype, !try_back);
 		if (page) {
 			prep_new_page(page, order, gfp_mask, alloc_flags);
 
@@ -3880,10 +3984,15 @@ __alloc_pages_direct_compact(gfp_t gfp_mask, unsigned int order,
 	struct page *page = NULL;
 	unsigned long pflags;
 	unsigned int noreclaim_flag;
+    u64 start;
 
 	if (!order)
 		return NULL;
 
+	get_cpu_var(pftrace_alloc_fallback_compact) = true;
+
+    start = rdtsc();
+
 	psi_memstall_enter(&pflags);
 	noreclaim_flag = memalloc_noreclaim_save();
 
@@ -3907,6 +4016,9 @@ __alloc_pages_direct_compact(gfp_t gfp_mask, unsigned int order,
 	if (!page)
 		page = get_page_from_freelist(gfp_mask, order, alloc_flags, ac);
 
+    // The rest is fairly cheap
+    mm_stats_hist_measure(&mm_direct_compaction_cycles, rdtsc() - start);
+
 	if (page) {
 		struct zone *zone = page_zone(page);
 
@@ -4130,10 +4242,15 @@ __alloc_pages_direct_reclaim(gfp_t gfp_mask, unsigned int order,
 {
 	struct page *page = NULL;
 	bool drained = false;
+	u64 start = rdtsc();
+
+	get_cpu_var(pftrace_alloc_fallback_reclaim) = true;
 
 	*did_some_progress = __perform_reclaim(gfp_mask, order, ac);
-	if (unlikely(!(*did_some_progress)))
+	if (unlikely(!(*did_some_progress))) {
+		mm_stats_hist_measure(&mm_direct_reclamation_cycles, rdtsc() - start);
 		return NULL;
+	}
 
 retry:
 	page = get_page_from_freelist(gfp_mask, order, alloc_flags, ac);
@@ -4150,6 +4267,8 @@ __alloc_pages_direct_reclaim(gfp_t gfp_mask, unsigned int order,
 		goto retry;
 	}
 
+	mm_stats_hist_measure(&mm_direct_reclamation_cycles, rdtsc() - start);
+
 	return page;
 }
 
@@ -4401,6 +4520,7 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,
 	int no_progress_loops;
 	unsigned int cpuset_mems_cookie;
 	int reserve_flags;
+	int retry_count = 0;
 
 	/*
 	 * We also sanity check to catch abuse of atomic reserves being used by
@@ -4501,6 +4621,11 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,
 	}
 
 retry:
+	if (retry_count) {
+		get_cpu_var(pftrace_alloc_fallback_retry) = true;
+	}
+	retry_count += 1;
+
 	/* Ensure kswapd doesn't accidentally go to sleep as long as we loop */
 	if (alloc_flags & ALLOC_KSWAPD)
 		wake_all_kswapds(order, gfp_mask, ac);
@@ -4705,6 +4830,14 @@ __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid,
 	gfp_t alloc_mask; /* The gfp_t that was actually used for allocation */
 	struct alloc_context ac = { };
 
+	get_cpu_var(pftrace_alloc_fallback) = false;
+	get_cpu_var(pftrace_alloc_fallback_retry) = false;
+	get_cpu_var(pftrace_alloc_fallback_reclaim) = false;
+	get_cpu_var(pftrace_alloc_fallback_compact) = false;
+	get_cpu_var(pftrace_alloc_node_reclaim) = false;
+	get_cpu_var(pftrace_alloc_zeroed_page) = false;
+	get_cpu_var(pftrace_alloc_prezeroed) = false;
+
 	/*
 	 * There are several places where we assume that the order value is sane
 	 * so bail out early if the request is out of bound.
@@ -4732,6 +4865,8 @@ __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid,
 	if (likely(page))
 		goto out;
 
+	get_cpu_var(pftrace_alloc_fallback) = true;
+
 	/*
 	 * Apply scoped allocation constraints. This is mainly about GFP_NOFS
 	 * resp. GFP_NOIO which has to be inherited for all allocation requests
@@ -4757,6 +4892,12 @@ __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid,
 		page = NULL;
 	}
 
+	// markm: update if we allocated a zeroed large allocation.
+	// For now, we mostly care about huge pages.
+	if (page && (gfp_mask & GFP_TRANSHUGE_LIGHT) && (order >= 9)) {
+		lfpa_update(rdtsc());
+	}
+
 	trace_mm_page_alloc(page, order, alloc_mask, ac.migratetype);
 
 	return page;
diff --git a/mm/read-pftrace/Cargo.lock b/mm/read-pftrace/Cargo.lock
new file mode 100644
index 000000000000..2f19494afb8f
--- /dev/null
+++ b/mm/read-pftrace/Cargo.lock
@@ -0,0 +1,486 @@
+# This file is automatically @generated by Cargo.
+# It is not intended for manual editing.
+[[package]]
+name = "adler"
+version = "0.2.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "ee2a4ec343196209d6594e19543ae87a39f96d5534d7174822a3ad825dd6ed7e"
+
+[[package]]
+name = "ansi_term"
+version = "0.11.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "ee49baf6cb617b853aa8d93bf420db2383fab46d314482ca2803b40d5fde979b"
+dependencies = [
+ "winapi",
+]
+
+[[package]]
+name = "arrayvec"
+version = "0.5.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "23b62fc65de8e4e7f52534fb52b0f3ed04746ae267519eef2a83941e8085068b"
+
+[[package]]
+name = "atty"
+version = "0.2.14"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "d9b39be18770d11421cdb1b9947a45dd3f37e93092cbf377614828a319d5fee8"
+dependencies = [
+ "hermit-abi",
+ "libc",
+ "winapi",
+]
+
+[[package]]
+name = "autocfg"
+version = "1.0.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "cdb031dd78e28731d87d56cc8ffef4a8f36ca26c38fe2de700543e627f8a464a"
+
+[[package]]
+name = "base64"
+version = "0.13.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "904dfeac50f3cdaba28fc6f57fdcddb75f49ed61346676a78c4ffe55877802fd"
+
+[[package]]
+name = "bitflags"
+version = "1.2.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "cf1de2fe8c75bc145a2f577add951f8134889b4795d47466a54a5c846d691693"
+
+[[package]]
+name = "bitvec"
+version = "0.19.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "a7ba35e9565969edb811639dbebfe34edc0368e472c5018474c8eb2543397f81"
+dependencies = [
+ "funty",
+ "radium",
+ "tap",
+ "wyz",
+]
+
+[[package]]
+name = "byteorder"
+version = "1.4.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "ae44d1a3d5a19df61dd0c8beb138458ac2a53a7ac09eba97d55592540004306b"
+
+[[package]]
+name = "cc"
+version = "1.0.67"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "e3c69b077ad434294d3ce9f1f6143a2a4b89a8a2d54ef813d85003a4fd1137fd"
+
+[[package]]
+name = "cfg-if"
+version = "1.0.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "baf1de4339761588bc0619e3cbc0120ee582ebb74b53b4efbf79117bd2da40fd"
+
+[[package]]
+name = "clap"
+version = "2.33.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "37e58ac78573c40708d45522f0d80fa2f01cc4f9b4e2bf749807255454312002"
+dependencies = [
+ "ansi_term",
+ "atty",
+ "bitflags",
+ "strsim",
+ "textwrap",
+ "unicode-width",
+ "vec_map",
+]
+
+[[package]]
+name = "crc32fast"
+version = "1.2.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "81156fece84ab6a9f2afdb109ce3ae577e42b1228441eded99bd77f627953b1a"
+dependencies = [
+ "cfg-if",
+]
+
+[[package]]
+name = "crossbeam-channel"
+version = "0.5.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "dca26ee1f8d361640700bde38b2c37d8c22b3ce2d360e1fc1c74ea4b0aa7d775"
+dependencies = [
+ "cfg-if",
+ "crossbeam-utils",
+]
+
+[[package]]
+name = "crossbeam-utils"
+version = "0.8.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "bae8f328835f8f5a6ceb6a7842a7f2d0c03692adb5c889347235d59194731fe3"
+dependencies = [
+ "autocfg",
+ "cfg-if",
+ "lazy_static",
+ "loom",
+]
+
+[[package]]
+name = "flate2"
+version = "1.0.20"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "cd3aec53de10fe96d7d8c565eb17f2c687bb5518a2ec453b5b1252964526abe0"
+dependencies = [
+ "cfg-if",
+ "crc32fast",
+ "libc",
+ "miniz_oxide",
+]
+
+[[package]]
+name = "funty"
+version = "1.1.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "fed34cd105917e91daa4da6b3728c47b068749d6a62c59811f06ed2ac71d9da7"
+
+[[package]]
+name = "generator"
+version = "0.6.23"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "8cdc09201b2e8ca1b19290cf7e65de2246b8e91fb6874279722189c4de7b94dc"
+dependencies = [
+ "cc",
+ "libc",
+ "log",
+ "rustc_version",
+ "winapi",
+]
+
+[[package]]
+name = "hdrhistogram"
+version = "7.2.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "af54a13c410de2b5e2d611475072834b86d7b2f835a2a8f7b1b1248d3e9bfbd8"
+dependencies = [
+ "base64",
+ "byteorder",
+ "crossbeam-channel",
+ "flate2",
+ "nom",
+ "num-traits",
+]
+
+[[package]]
+name = "heck"
+version = "0.3.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "87cbf45460356b7deeb5e3415b5563308c0a9b057c85e12b06ad551f98d0a6ac"
+dependencies = [
+ "unicode-segmentation",
+]
+
+[[package]]
+name = "hermit-abi"
+version = "0.1.18"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "322f4de77956e22ed0e5032c359a0f1273f1f7f0d79bfa3b8ffbc730d7fbcc5c"
+dependencies = [
+ "libc",
+]
+
+[[package]]
+name = "lazy_static"
+version = "1.4.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "e2abad23fbc42b3700f2f279844dc832adb2b2eb069b2df918f455c4e18cc646"
+
+[[package]]
+name = "lexical-core"
+version = "0.7.5"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "21f866863575d0e1d654fbeeabdc927292fdf862873dc3c96c6f753357e13374"
+dependencies = [
+ "arrayvec",
+ "bitflags",
+ "cfg-if",
+ "ryu",
+ "static_assertions",
+]
+
+[[package]]
+name = "libc"
+version = "0.2.86"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "b7282d924be3275cec7f6756ff4121987bc6481325397dde6ba3e7802b1a8b1c"
+
+[[package]]
+name = "log"
+version = "0.4.14"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "51b9bbe6c47d51fc3e1a9b945965946b4c44142ab8792c50835a980d362c2710"
+dependencies = [
+ "cfg-if",
+]
+
+[[package]]
+name = "loom"
+version = "0.4.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "d44c73b4636e497b4917eb21c33539efa3816741a2d3ff26c6316f1b529481a4"
+dependencies = [
+ "cfg-if",
+ "generator",
+ "scoped-tls",
+]
+
+[[package]]
+name = "memchr"
+version = "2.3.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "0ee1c47aaa256ecabcaea351eae4a9b01ef39ed810004e298d2511ed284b1525"
+
+[[package]]
+name = "miniz_oxide"
+version = "0.4.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "0f2d26ec3309788e423cfbf68ad1800f061638098d76a83681af979dc4eda19d"
+dependencies = [
+ "adler",
+ "autocfg",
+]
+
+[[package]]
+name = "nom"
+version = "6.1.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "e7413f999671bd4745a7b624bd370a569fb6bc574b23c83a3c5ed2e453f3d5e2"
+dependencies = [
+ "bitvec",
+ "funty",
+ "lexical-core",
+ "memchr",
+ "version_check",
+]
+
+[[package]]
+name = "num-traits"
+version = "0.2.14"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "9a64b1ec5cda2586e284722486d802acf1f7dbdc623e2bfc57e65ca1cd099290"
+dependencies = [
+ "autocfg",
+]
+
+[[package]]
+name = "proc-macro-error"
+version = "1.0.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "da25490ff9892aab3fcf7c36f08cfb902dd3e71ca0f9f9517bea02a73a5ce38c"
+dependencies = [
+ "proc-macro-error-attr",
+ "proc-macro2",
+ "quote",
+ "syn",
+ "version_check",
+]
+
+[[package]]
+name = "proc-macro-error-attr"
+version = "1.0.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "a1be40180e52ecc98ad80b184934baf3d0d29f979574e439af5a55274b35f869"
+dependencies = [
+ "proc-macro2",
+ "quote",
+ "version_check",
+]
+
+[[package]]
+name = "proc-macro2"
+version = "1.0.24"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "1e0704ee1a7e00d7bb417d0770ea303c1bccbabf0ef1667dae92b5967f5f8a71"
+dependencies = [
+ "unicode-xid",
+]
+
+[[package]]
+name = "quote"
+version = "1.0.9"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "c3d0b9745dc2debf507c8422de05d7226cc1f0644216dfdfead988f9b1ab32a7"
+dependencies = [
+ "proc-macro2",
+]
+
+[[package]]
+name = "radium"
+version = "0.5.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "941ba9d78d8e2f7ce474c015eea4d9c6d25b6a3327f9832ee29a4de27f91bbb8"
+
+[[package]]
+name = "read-pftrace"
+version = "0.1.0"
+dependencies = [
+ "clap",
+ "hdrhistogram",
+ "structopt",
+]
+
+[[package]]
+name = "rustc_version"
+version = "0.2.3"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "138e3e0acb6c9fb258b19b67cb8abd63c00679d2851805ea151465464fe9030a"
+dependencies = [
+ "semver",
+]
+
+[[package]]
+name = "ryu"
+version = "1.0.5"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "71d301d4193d031abdd79ff7e3dd721168a9572ef3fe51a1517aba235bd8f86e"
+
+[[package]]
+name = "scoped-tls"
+version = "1.0.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "ea6a9290e3c9cf0f18145ef7ffa62d68ee0bf5fcd651017e586dc7fd5da448c2"
+
+[[package]]
+name = "semver"
+version = "0.9.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "1d7eb9ef2c18661902cc47e535f9bc51b78acd254da71d375c2f6720d9a40403"
+dependencies = [
+ "semver-parser",
+]
+
+[[package]]
+name = "semver-parser"
+version = "0.7.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "388a1df253eca08550bef6c72392cfe7c30914bf41df5269b68cbd6ff8f570a3"
+
+[[package]]
+name = "static_assertions"
+version = "1.1.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "a2eb9349b6444b326872e140eb1cf5e7c522154d69e7a0ffb0fb81c06b37543f"
+
+[[package]]
+name = "strsim"
+version = "0.8.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "8ea5119cdb4c55b55d432abb513a0429384878c15dde60cc77b1c99de1a95a6a"
+
+[[package]]
+name = "structopt"
+version = "0.3.21"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "5277acd7ee46e63e5168a80734c9f6ee81b1367a7d8772a2d765df2a3705d28c"
+dependencies = [
+ "clap",
+ "lazy_static",
+ "structopt-derive",
+]
+
+[[package]]
+name = "structopt-derive"
+version = "0.4.14"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "5ba9cdfda491b814720b6b06e0cac513d922fc407582032e8706e9f137976f90"
+dependencies = [
+ "heck",
+ "proc-macro-error",
+ "proc-macro2",
+ "quote",
+ "syn",
+]
+
+[[package]]
+name = "syn"
+version = "1.0.64"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "3fd9d1e9976102a03c542daa2eff1b43f9d72306342f3f8b3ed5fb8908195d6f"
+dependencies = [
+ "proc-macro2",
+ "quote",
+ "unicode-xid",
+]
+
+[[package]]
+name = "tap"
+version = "1.0.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "55937e1799185b12863d447f42597ed69d9928686b8d88a1df17376a097d8369"
+
+[[package]]
+name = "textwrap"
+version = "0.11.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "d326610f408c7a4eb6f51c37c330e496b08506c9457c9d34287ecc38809fb060"
+dependencies = [
+ "unicode-width",
+]
+
+[[package]]
+name = "unicode-segmentation"
+version = "1.7.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "bb0d2e7be6ae3a5fa87eed5fb451aff96f2573d2694942e40543ae0bbe19c796"
+
+[[package]]
+name = "unicode-width"
+version = "0.1.8"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "9337591893a19b88d8d87f2cec1e73fad5cdfd10e5a6f349f498ad6ea2ffb1e3"
+
+[[package]]
+name = "unicode-xid"
+version = "0.2.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "f7fe0bb3479651439c9112f72b6c505038574c9fbb575ed1bf3b797fa39dd564"
+
+[[package]]
+name = "vec_map"
+version = "0.8.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "f1bddf1187be692e79c5ffeab891132dfb0f236ed36a43c7ed39f1165ee20191"
+
+[[package]]
+name = "version_check"
+version = "0.9.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "b5a972e5669d67ba988ce3dc826706fb0a8b01471c088cb0b6110b805cc36aed"
+
+[[package]]
+name = "winapi"
+version = "0.3.9"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "5c839a674fcd7a98952e593242ea400abe93992746761e38641405d28b00f419"
+dependencies = [
+ "winapi-i686-pc-windows-gnu",
+ "winapi-x86_64-pc-windows-gnu",
+]
+
+[[package]]
+name = "winapi-i686-pc-windows-gnu"
+version = "0.4.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "ac3b87c63620426dd9b991e5ce0329eff545bccbbb34f3be09ff6fb6ab51b7b6"
+
+[[package]]
+name = "winapi-x86_64-pc-windows-gnu"
+version = "0.4.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "712e227841d057c1ee1cd2fb22fa7e5a5461ae8e48fa2ca79ec42cfc1931183f"
+
+[[package]]
+name = "wyz"
+version = "0.2.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "85e60b0d1b5f99db2556934e21937020776a5d31520bf169e851ac44e6420214"
diff --git a/mm/read-pftrace/Cargo.toml b/mm/read-pftrace/Cargo.toml
new file mode 100644
index 000000000000..2983fa72d192
--- /dev/null
+++ b/mm/read-pftrace/Cargo.toml
@@ -0,0 +1,13 @@
+[package]
+name = "read-pftrace"
+version = "0.1.0"
+authors = ["mark <markm@cs.wisc.edu>"]
+edition = "2018"
+
+# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html
+
+[dependencies]
+clap = "2.33.3"
+hdrhistogram = "7.2.0"
+structopt = "0.3.21"
+
diff --git a/mm/read-pftrace/src/main.rs b/mm/read-pftrace/src/main.rs
new file mode 100644
index 000000000000..1fd90e1c419c
--- /dev/null
+++ b/mm/read-pftrace/src/main.rs
@@ -0,0 +1,595 @@
+//! Reads traces in binary form produced by the pftrace mechanism.
+
+use std::collections::{BTreeMap, HashSet};
+use std::path::PathBuf;
+
+use clap::arg_enum;
+
+use hdrhistogram::Histogram;
+
+use structopt::StructOpt;
+
+type CategorizedData = BTreeMap<MMStatsBitflags, Histogram<u64>>;
+
+/// Reads pftrace output and dumps useful numbers for plotting.
+#[derive(StructOpt, Debug, Clone)]
+#[structopt(name = "read-pftrace")]
+struct Config {
+    /// The pftrace output file.
+    pftrace_file: PathBuf,
+
+    /// The file with counts of rejected pftrace samples.
+    #[structopt(requires("rejection-threshold"))]
+    rejected_file: Option<PathBuf>,
+
+    /// The threshold below which samples are rejected.
+    #[structopt(requires("rejected-file"))]
+    rejection_threshold: Option<u64>,
+
+    /// Output PDF, rather than CDF.
+    #[structopt(long)]
+    pdf: bool,
+
+    /// Combine all categories in the output, generating the overall distribution of all page
+    /// faults, rather than individual categories.
+    #[structopt(long)]
+    combined: bool,
+
+    /// Output tail latency based on the given maximum number of 9's. That is, given a value of 5,
+    /// we output 100 points, with exponentially more density as we go from 0 to 99.999 (i.e., 5
+    /// 9's).
+    #[structopt(long, conflicts_with("percentile"), conflicts_with("pdf"))]
+    tail: Option<usize>,
+
+    /// Report the number of events at each percentile, too.
+    #[structopt(long, conflicts_with("percentile"), conflicts_with("pdf"))]
+    freq: bool,
+
+    /// Dump the trace to stdout.
+    #[structopt(
+        long,
+        conflicts_with("percentile"),
+        conflicts_with("pdf"),
+        conflicts_with("tail"),
+        conflicts_with("freq"),
+        conflicts_with("other_category")
+    )]
+    dump: bool,
+
+    /// Which data to output.
+    #[structopt(
+        long,
+        possible_values = &DataMode::variants(),
+        case_insensitive = true,
+        default_value = "duration",
+    )]
+    data_mode: DataMode,
+
+    /// Only output 1 line of data for use with a plotting script.
+    #[structopt(long)]
+    cli_only: bool,
+
+    /// If passed, creates an "Other" category and collects all bitflags whose frequency is
+    /// less than or equal to the given threshold. Otherwise, all categories are listed, even
+    /// if they only have one recorded sample. Passing this flag increases the time to process
+    /// the trace.
+    #[structopt(long, conflicts_with("combined"))]
+    other_category: Option<u64>,
+
+    /// Exclude categories containing the given string from the trace before doing any other
+    /// processing. Note that for PDFs, this will change the proportion of events.
+    #[structopt(long)]
+    exclude: Vec<String>,
+
+    /// Instead of printing a distribution, print the given percentile for each of set of bitflags.
+    #[structopt(long)]
+    percentile: Option<f64>,
+}
+
+arg_enum! {
+    #[derive(Debug, Clone, PartialEq, Eq)]
+    enum DataMode {
+        Duration,
+        AllocTotal,
+        AllocClearing,
+        PrepTotal,
+    }
+}
+
+/// ```c
+/// typedef u64 mm_stats_bitflags_t;
+/// ```
+#[derive(Debug, Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Hash)]
+#[repr(transparent)]
+struct MMStatsBitflags(u64);
+
+/// ```c
+/// struct mm_stats_pftrace {
+/// 	// A bunch of bitflags indicating things that happened during this #PF.
+/// 	// See `mm_econ_flags` for more info.
+/// 	mm_stats_bitflags_t bitflags;
+///
+/// 	// The start and end TSC of the #PF.
+/// 	u64 start_tsc;
+/// 	u64 end_tsc;
+///
+/// 	// Timestamps at which the #PF did the following:
+/// 	u64 alloc_start_tsc; // started allocating memory
+/// 	u64 alloc_end_tsc;   // finished allocating memory (or OOMed)
+///
+/// 	u64 prep_start_tsc;  // started preparing the alloced mem
+/// 	u64 prep_end_tsc;    // finished ...
+/// };
+/// ```
+#[repr(C)]
+struct MMStatsPftrace {
+    bitflags: MMStatsBitflags,
+
+    start_tsc: u64,
+    end_tsc: u64,
+
+    alloc_start_tsc: u64,
+    alloc_end_tsc: u64,
+    alloc_zeroing_duration: u64,
+
+    prep_start_tsc: u64,
+    prep_end_tsc: u64,
+}
+
+macro_rules! with_stringify {
+    (enum $name:ident { $($variant:ident),+ $(,)? }) => {
+        #[allow(non_camel_case_types, dead_code)]
+        #[repr(u8)]
+        #[derive(Debug, Clone, Copy)]
+        enum $name { $($variant),+ }
+
+        impl $name {
+            pub fn name(&self) -> &'static str {
+                match self {
+                    $(
+                        $name :: $variant => stringify!($variant)
+                    ),+
+                }
+            }
+        }
+    };
+}
+
+with_stringify! {
+    enum MMStatsPftraceFlags {
+        HUGE_PAGE,
+        VERY_HUGE_PAGE,
+        BADGER_TRAP,
+        WP,
+        EXEC,
+        NUMA,
+        SWAP,
+        NOT_ANON,
+        NOT_ANON_READ,
+        NOT_ANON_COW,
+        NOT_ANON_SHARED,
+        ZERO,
+        HUGE_ALLOC_FAILED,
+        HUGE_SPLIT,
+        HUGE_PROMOTION,
+        HUGE_PROMOTION_FAILED,
+        HUGE_COPY,
+        CLEARED_MEM,
+        ALLOC_FALLBACK,
+        ALLOC_FALLBACK_RETRY,
+        ALLOC_FALLBACK_RECLAIM,
+        ALLOC_FALLBACK_COMPACT,
+        ALLOC_PREZEROED,
+        ALLOC_NODE_RECLAIM,
+
+        OTHER, // A hack -- shouldn't actually be in use...
+        MM_STATS_NUM_FLAGS,
+    }
+}
+
+impl MMStatsPftraceFlags {
+    pub fn from_u8(n: u8) -> Self {
+        if n <= (MMStatsPftraceFlags::MM_STATS_NUM_FLAGS as u8) {
+            unsafe { std::mem::transmute(n) }
+        } else {
+            panic!("Invalid flag: {:X}", n);
+        }
+    }
+}
+
+impl MMStatsBitflags {
+    pub fn flags(self) -> Vec<MMStatsPftraceFlags> {
+        let mut vec = Vec::new();
+        for i in 0..(MMStatsPftraceFlags::MM_STATS_NUM_FLAGS as u8) {
+            if self.0 & (1 << i) != 0 {
+                vec.push(MMStatsPftraceFlags::from_u8(i));
+            }
+        }
+        vec
+    }
+
+    pub fn from_hex_str(s: &str) -> Result<Self, std::num::ParseIntError> {
+        u64::from_str_radix(s, 16).map(|f| Self(f))
+    }
+
+    pub fn name(self) -> String {
+        let name = self
+            .flags()
+            .iter()
+            .map(MMStatsPftraceFlags::name)
+            .collect::<Vec<_>>()
+            .join(",");
+
+        if name.is_empty() {
+            "none".to_string()
+        } else {
+            name
+        }
+    }
+}
+
+fn main() -> std::io::Result<()> {
+    let config = Config::from_args();
+
+    let excluded_bitmask = compute_excluded_bitmask(&config.exclude);
+
+    let rejected = config.rejected_file.as_ref().map(|rejected_fname| {
+        let rejected = std::fs::read_to_string(rejected_fname)
+            .expect("Unable to read rejection file")
+            .trim_end_matches('\u{0}')
+            .trim()
+            .split(' ')
+            .map(|part| {
+                let mut iter = part.split(':');
+                (iter.next().unwrap(), iter.next().unwrap())
+            })
+            .map(|(bits, count)| {
+                (
+                    MMStatsBitflags::from_hex_str(bits).expect("not an integer"),
+                    count.parse::<u64>().expect("not an integer"),
+                )
+            })
+            .filter(|(bits, _)| bits.0 & excluded_bitmask.0 == 0)
+            .collect::<Vec<_>>();
+
+        (rejected, config.rejection_threshold.unwrap())
+    });
+    let buf = std::fs::read(&config.pftrace_file)?;
+    let buf: &[MMStatsPftrace] = unsafe {
+        assert!(buf.len() % std::mem::size_of::<MMStatsPftrace>() == 0);
+        let (pre, aligned, post) = buf.as_slice().align_to();
+        assert_eq!(pre.len(), 0);
+        assert_eq!(post.len(), 0);
+        aligned
+    };
+
+    if config.dump {
+        dump_trace(&config, &buf, excluded_bitmask);
+    } else if config.percentile.is_some() {
+        generate_percentiles(&config, &buf, rejected.as_ref(), excluded_bitmask);
+    } else if config.pdf {
+        generate_pdfs(&config, &buf, rejected.as_ref(), excluded_bitmask);
+    } else {
+        generate_cdfs(&config, &buf, rejected.as_ref(), excluded_bitmask);
+    }
+
+    Ok(())
+}
+
+fn compute_excluded_bitmask(exclude: &[String]) -> MMStatsBitflags {
+    let mut x = 0u64;
+
+    for i in 0..(MMStatsPftraceFlags::MM_STATS_NUM_FLAGS as u8) {
+        if exclude
+            .iter()
+            .any(|ex| MMStatsPftraceFlags::from_u8(i).name().contains(ex))
+        {
+            x |= 1 << i;
+        }
+    }
+
+    MMStatsBitflags(x)
+}
+
+fn categorize(
+    config: &Config,
+    buf: &[MMStatsPftrace],
+    rejected: Option<&(Vec<(MMStatsBitflags, u64)>, u64)>,
+    excluded_bitmask: MMStatsBitflags,
+) -> CategorizedData {
+    // We categorize events by their bitflags.
+    let mut categorized: CategorizedData = BTreeMap::new();
+    for trace in buf
+        .iter()
+        .filter(|t| t.bitflags.0 & excluded_bitmask.0 == 0)
+    {
+        let data = match config.data_mode {
+            DataMode::Duration => trace.end_tsc - trace.start_tsc,
+            DataMode::AllocTotal => trace.alloc_end_tsc - trace.alloc_start_tsc,
+            DataMode::AllocClearing => trace.alloc_zeroing_duration,
+            DataMode::PrepTotal => trace.prep_end_tsc - trace.prep_start_tsc,
+        };
+
+        let bitflags = if config.combined {
+            MMStatsBitflags(0)
+        } else {
+            trace.bitflags
+        };
+
+        categorized
+            .entry(bitflags)
+            .or_insert(Histogram::new(5).unwrap())
+            .record(data)
+            .unwrap();
+    }
+
+    // Adjust for the rejected samples.
+    if let Some((rejected, threshold)) = rejected {
+        for (bitflags, rejected_count) in rejected {
+            let flags = if config.combined {
+                MMStatsBitflags(0)
+            } else {
+                *bitflags
+            };
+
+            categorized
+                .entry(flags)
+                .or_insert(Histogram::new(5).unwrap())
+                .record_n(*threshold, *rejected_count)
+                .unwrap();
+        }
+    }
+
+    // Create "Other" category if needed.
+    if let Some(threshold) = config.other_category {
+        // Figure out which categories to collect under "Other".
+        let cats = categorized
+            .iter()
+            .filter(|(_, hist)| hist.len() <= threshold)
+            .map(|(cat, _)| *cat)
+            .collect::<HashSet<_>>();
+
+        // Remove categories.
+        for cat in cats.iter() {
+            categorized.remove(&cat);
+        }
+
+        // Create the new histogram.
+        let mut other_hist: Histogram<u64> = Histogram::new(5).unwrap();
+
+        for trace in buf
+            .iter()
+            .filter(|t| t.bitflags.0 & excluded_bitmask.0 == 0)
+            .filter(|trace| cats.contains(&trace.bitflags))
+        {
+            let data = match config.data_mode {
+                DataMode::Duration => trace.end_tsc - trace.start_tsc,
+                DataMode::AllocTotal => trace.alloc_end_tsc - trace.alloc_start_tsc,
+                DataMode::AllocClearing => trace.alloc_zeroing_duration,
+                DataMode::PrepTotal => trace.prep_end_tsc - trace.prep_start_tsc,
+            };
+
+            other_hist.record(data).unwrap();
+        }
+
+        // Re-adjust
+        if let Some((rejected, threshold)) = rejected {
+            for (bitflags, rejected_count) in rejected {
+                if cats.contains(&bitflags) {
+                    other_hist.record_n(*threshold, *rejected_count).unwrap();
+                }
+            }
+        }
+
+        if !other_hist.is_empty() {
+            categorized.insert(
+                MMStatsBitflags(1u64 << (MMStatsPftraceFlags::OTHER as u64)),
+                other_hist,
+            );
+        }
+    }
+
+    categorized
+}
+
+fn print_quartiles(categorized: &CategorizedData) {
+    let mut total = 0;
+
+    let mut keys = categorized.keys().collect::<Vec<_>>();
+    keys.sort_by_key(|flags| categorized.get(flags).unwrap().len());
+    for flags in keys.iter() {
+        let hist = categorized.get(flags).unwrap();
+        println!("{:4X}: {}", flags.0, flags.name());
+
+        const QUANTILES: &[f64] = &[0.0, 0.25, 0.5, 0.75, 1.0];
+
+        print!("\t");
+        for (p, v) in QUANTILES.iter().map(|p| (*p, hist.value_at_quantile(*p))) {
+            print!(" P{:.0}={}", p * 100.0, v);
+        }
+        println!(" N={}", hist.len());
+
+        total += hist.len();
+    }
+
+    println!("Total: {}", total);
+}
+
+fn generate_percentiles(
+    config: &Config,
+    buf: &[MMStatsPftrace],
+    rejected: Option<&(Vec<(MMStatsBitflags, u64)>, u64)>,
+    excluded_bitmask: MMStatsBitflags,
+) {
+    let categorized = categorize(config, buf, rejected, excluded_bitmask);
+    let p = config.percentile.unwrap() / 100.0;
+
+    let mut keys = categorized.keys().collect::<Vec<_>>();
+    keys.sort_by_key(|flags| categorized.get(flags).unwrap().len());
+    for flags in keys.iter() {
+        let hist = categorized.get(flags).unwrap();
+        print!(
+            "{}({}):{} ",
+            flags.name(),
+            hist.len(),
+            hist.value_at_quantile(p)
+        );
+    }
+    println!();
+}
+
+/// Generate a list of 100 percentiles to compute. If we are doing a tail latency plot, we want to
+/// exponentially be more dense at the tail. Otherwise, we just use uniform density.
+fn get_points(config: &Config) -> Box<dyn Iterator<Item = f64>> {
+    if let Some(nines) = config.tail {
+        let nines = nines as f64;
+        // If we are going from 0 to `nines` 9's, then the nth "step" is determined by this
+        // function: `p = 100 - 10^(2-n)`. For example:
+        //
+        //       n     p
+        //       0     0
+        //       1     90
+        //       2     99
+        //       3     99.9
+        //       4     99.99
+        //       5     99.999
+        //      ...    ...
+        // `nines`     99.9...9  <- `nines` 9's
+        //
+        // We want to find 100 points in this function, with denser points closer to the tail. To
+        // do this, we will first generate 100 linearly spaced points from 0 to `nines`
+        // (inclusive). Then, we will map them with the above function.
+        Box::new(
+            (0..=100)
+                .map(|n| n as f64)
+                // scale down to `[0, nines]`
+                .map(move |n| n * nines / 100.)
+                // map into log space
+                .map(|n| 100. - 10f64.powf(2. - n))
+                // scale down to `[0, 1)` (i.e., not a percent anymore)
+                .map(|p| p / 100.),
+        )
+    } else {
+        Box::new((0..=100).map(|p| p as f64).map(|p| p / 100.))
+    }
+}
+
+fn generate_cdfs(
+    config: &Config,
+    buf: &[MMStatsPftrace],
+    rejected: Option<&(Vec<(MMStatsBitflags, u64)>, u64)>,
+    excluded_bitmask: MMStatsBitflags,
+) {
+    let categorized = categorize(config, buf, rejected, excluded_bitmask);
+
+    // Print output.
+    if !config.cli_only {
+        print_quartiles(&categorized);
+    }
+
+    // Print for plotting...
+    //
+    // For the sake of plotting, we sort by the number of events.
+    let mut keys = categorized.keys().collect::<Vec<_>>();
+    keys.sort_by_key(|flags| categorized.get(flags).unwrap().len());
+    for flags in keys.iter() {
+        let hist = categorized.get(flags).unwrap();
+        print!(" {}({})", flags.name(), hist.len());
+        for v in get_points(&config).map(|p| hist.value_at_quantile(p)) {
+            if config.freq {
+                print!(" {},{}", v, hist.count_between(0, v));
+            } else {
+                print!(" {}", v);
+            }
+        }
+    }
+
+    /*
+    for trace in buf {
+        println!(
+            "total={:10} bits={:4X} {}",
+            trace.end_tsc - trace.start_tsc,
+            trace.bitflags.0,
+            trace
+                .bitflags
+                .name()
+        );
+    }
+    */
+}
+
+fn generate_pdfs(
+    config: &Config,
+    buf: &[MMStatsPftrace],
+    rejected: Option<&(Vec<(MMStatsBitflags, u64)>, u64)>,
+    excluded_bitmask: MMStatsBitflags,
+) {
+    let categorized = categorize(config, buf, rejected, excluded_bitmask);
+
+    // Print output.
+    if !config.cli_only {
+        print_quartiles(&categorized);
+    }
+
+    // Print for plotting...
+    //
+    // For the sake of plotting, we sort by the number of events.
+    let minvalue = categorized
+        .values()
+        .map(|h| h.min())
+        .min()
+        .expect("No min?");
+    let mut keys = categorized.keys().collect::<Vec<_>>();
+    keys.sort_by_key(|flags| categorized.get(flags).unwrap().len());
+    for flags in keys.iter() {
+        let hist = categorized.get(flags).unwrap();
+        print!(" {}({})", flags.name(), hist.len());
+
+        const PDF_STEP_SIZE: u64 = 2;
+        let start = if let Some((_, threshold)) = rejected {
+            *threshold
+        } else {
+            minvalue
+        };
+        let mut min = hist.lowest_equivalent(start);
+        for i in 0.. {
+            let max = hist.highest_equivalent(min + PDF_STEP_SIZE.pow(i));
+            print!(" {}:{}", max, hist.count_between(min, max));
+
+            if max > hist.max() {
+                break;
+            } else {
+                min = max;
+            }
+        }
+    }
+
+    /*
+    for trace in buf {
+        println!(
+            "total={:10} bits={:4X} {}",
+            trace.end_tsc - trace.start_tsc,
+            trace.bitflags.0,
+            trace
+                .bitflags
+                .name()
+        );
+    }
+    */
+}
+
+fn dump_trace(config: &Config, buf: &[MMStatsPftrace], excluded_bitmask: MMStatsBitflags) {
+    for trace in buf
+        .iter()
+        .filter(|t| t.bitflags.0 & excluded_bitmask.0 == 0)
+    {
+        let data = match config.data_mode {
+            DataMode::Duration => trace.end_tsc - trace.start_tsc,
+            DataMode::AllocTotal => trace.alloc_end_tsc - trace.alloc_start_tsc,
+            DataMode::AllocClearing => trace.alloc_zeroing_duration,
+            DataMode::PrepTotal => trace.prep_end_tsc - trace.prep_start_tsc,
+        };
+
+        println!("{data}");
+    }
+}
diff --git a/mm/shuffle.c b/mm/shuffle.c
index b3fe97fd6654..75609d8daa10 100644
--- a/mm/shuffle.c
+++ b/mm/shuffle.c
@@ -54,11 +54,33 @@ static __meminit int shuffle_store(const char *val,
 }
 module_param_call(shuffle, shuffle_store, shuffle_show, &shuffle_param, 0400);
 
+int page_alloc_shuffle_order = DEFAULT_SHUFFLE_ORDER;
+module_param(page_alloc_shuffle_order, int, 0644);
+
+static bool shuffle_trigger;
+static int shuffle_trigger_store(const char *val,
+		const struct kernel_param *kp)
+{
+	int nid;
+
+	static_branch_enable(&page_alloc_shuffle_key);
+
+	pr_warn("page_alloc: shuffling free list order=%d\n", page_alloc_shuffle_order);
+
+	for_each_node_state(nid, N_MEMORY) {
+		shuffle_free_memory(NODE_DATA(nid));
+		pr_warn("page_alloc: shuffled nid=%d.\n", nid);
+	}
+
+	return 0;
+}
+module_param_call(shuffle_trigger, shuffle_trigger_store, shuffle_show, &shuffle_trigger, 0200);
+
 /*
  * For two pages to be swapped in the shuffle, they must be free (on a
  * 'free_area' lru), have the same order, and have the same migratetype.
  */
-static struct page * __meminit shuffle_valid_page(unsigned long pfn, int order)
+static struct page * shuffle_valid_page(unsigned long pfn, int order)
 {
 	struct page *page;
 
@@ -102,14 +124,16 @@ static struct page * __meminit shuffle_valid_page(unsigned long pfn, int order)
  * be a perfect shuffle.
  */
 #define SHUFFLE_RETRY 10
-void __meminit __shuffle_zone(struct zone *z)
+void __shuffle_zone(struct zone *z)
 {
 	unsigned long i, flags;
 	unsigned long start_pfn = z->zone_start_pfn;
 	unsigned long end_pfn = zone_end_pfn(z);
-	const int order = SHUFFLE_ORDER;
+	const int order = page_alloc_shuffle_order;
 	const int order_pages = 1 << order;
 
+	pr_warn("page_alloc: shuffling zone, start=%lu, end=%lu\n", start_pfn, end_pfn);
+
 	spin_lock_irqsave(&z->lock, flags);
 	start_pfn = ALIGN(start_pfn, order_pages);
 	for (i = start_pfn; i < end_pfn; i += order_pages) {
@@ -175,7 +199,7 @@ void __meminit __shuffle_zone(struct zone *z)
  * shuffle_free_memory - reduce the predictability of the page allocator
  * @pgdat: node page data
  */
-void __meminit __shuffle_free_memory(pg_data_t *pgdat)
+void __shuffle_free_memory(pg_data_t *pgdat)
 {
 	struct zone *z;
 
diff --git a/mm/shuffle.h b/mm/shuffle.h
index 777a257a0d2f..10c7aafec6a7 100644
--- a/mm/shuffle.h
+++ b/mm/shuffle.h
@@ -16,12 +16,13 @@ enum mm_shuffle_ctl {
 	SHUFFLE_FORCE_DISABLE,
 };
 
-#define SHUFFLE_ORDER (MAX_ORDER-1)
+#define DEFAULT_SHUFFLE_ORDER (MAX_ORDER-1)
 
 #ifdef CONFIG_SHUFFLE_PAGE_ALLOCATOR
 DECLARE_STATIC_KEY_FALSE(page_alloc_shuffle_key);
 extern void page_alloc_shuffle(enum mm_shuffle_ctl ctl);
 extern void __shuffle_free_memory(pg_data_t *pgdat);
+extern int page_alloc_shuffle_order;
 static inline void shuffle_free_memory(pg_data_t *pgdat)
 {
 	if (!static_branch_unlikely(&page_alloc_shuffle_key))
@@ -41,7 +42,7 @@ static inline bool is_shuffle_order(int order)
 {
 	if (!static_branch_unlikely(&page_alloc_shuffle_key))
 		return false;
-	return order >= SHUFFLE_ORDER;
+	return order >= page_alloc_shuffle_order;
 }
 #else
 static inline void shuffle_free_memory(pg_data_t *pgdat)
